id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/670:11,deployability,observ,observations,11,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:102,deployability,depend,depends,102,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:194,deployability,cluster,clusters,194,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,deployability,depend,depends,253,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:285,deployability,cluster,clustering,285,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:388,deployability,cluster,clusters,388,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:525,deployability,observ,observed,525,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:583,deployability,cluster,clusters,583,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:847,deployability,recov,recovers,847,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1160,deployability,cluster,clusters,1160,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:102,integrability,depend,depends,102,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,integrability,depend,depends,253,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:633,integrability,sub,subtypes,633,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:102,modifiability,depend,depends,102,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,modifiability,depend,depends,253,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:417,performance,perform,performed,417,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:847,reliability,recov,recovers,847,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:102,safety,depend,depends,102,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,safety,depend,depends,253,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:268,safety,input,input,268,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:847,safety,recov,recovers,847,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1134,safety,detect,detected,1134,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:847,security,recov,recovers,847,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1134,security,detect,detected,1134,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:11,testability,observ,observations,11,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:102,testability,depend,depends,102,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,testability,depend,depends,253,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:525,testability,observ,observed,525,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:268,usability,input,input,268,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:417,usability,perform,performed,417,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:940,usability,user,user-images,940,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:116,availability,cluster,clusters,116,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:192,availability,cluster,cluster,192,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:300,availability,cluster,cluster,300,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:116,deployability,cluster,clusters,116,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:192,deployability,cluster,cluster,192,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:300,deployability,cluster,cluster,300,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:173,safety,detect,detect,173,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:173,security,detect,detect,173,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:45,usability,efficien,efficient,45,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:335,usability,efficien,efficient,335,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:153,availability,cluster,clusters,153,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:563,availability,cluster,clusters,563,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:22,deployability,log,logical,22,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:153,deployability,cluster,clusters,153,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:263,deployability,modul,modularity,263,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,deployability,modul,modularity,308,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:425,deployability,modul,modularity,425,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:512,deployability,modul,modularity,512,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:563,deployability,cluster,clusters,563,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,energy efficiency,optim,optimize,253,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:362,energy efficiency,optim,optimizing,362,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:452,energy efficiency,optim,optimized,452,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:263,integrability,modular,modularity,263,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,integrability,modular,modularity,308,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:425,integrability,modular,modularity,425,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:512,integrability,modular,modularity,512,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:99,modifiability,paramet,parameter,99,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:263,modifiability,modul,modularity,263,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,modifiability,modul,modularity,308,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:425,modifiability,modul,modularity,425,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:512,modifiability,modul,modularity,512,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,performance,optimiz,optimize,253,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:362,performance,optimiz,optimizing,362,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:452,performance,optimiz,optimized,452,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:22,safety,log,logical,22,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:263,safety,modul,modularity,263,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,safety,modul,modularity,308,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:425,safety,modul,modularity,425,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:512,safety,modul,modularity,512,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:22,security,log,logical,22,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:22,testability,log,logical,22,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:263,testability,modula,modularity,263,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,testability,modula,modularity,308,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:425,testability,modula,modularity,425,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:512,testability,modula,modularity,512,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. . In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:118,availability,cluster,clusters,118,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:118,deployability,cluster,clusters,118,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:382,deployability,depend,dependent,382,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:382,integrability,depend,dependent,382,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:64,modifiability,paramet,parameter,64,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:354,modifiability,paramet,parameter,354,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:382,modifiability,depend,dependent,382,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:382,safety,depend,dependent,382,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:382,testability,depend,dependent,382,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:75,availability,cluster,clustering,75,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:273,availability,cluster,clusters,273,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:580,availability,cluster,clustering,580,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:725,availability,cluster,clusters,725,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:75,deployability,cluster,clustering,75,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:142,deployability,depend,depending,142,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:183,deployability,depend,depending,183,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:273,deployability,cluster,clusters,273,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:580,deployability,cluster,clustering,580,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:725,deployability,cluster,clusters,725,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1051,deployability,depend,depends,1051,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:928,energy efficiency,estimat,estimation,928,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:142,integrability,depend,depending,142,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:183,integrability,depend,depending,183,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:818,integrability,topic,topic,818,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1051,integrability,depend,depends,1051,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:824,interoperability,specif,specific,824,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:142,modifiability,depend,depending,142,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:183,modifiability,depend,depending,183,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:529,modifiability,paramet,parameters,529,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1051,modifiability,depend,depends,1051,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:142,safety,depend,depending,142,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:183,safety,depend,depending,183,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:240,safety,valid,validate,240,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1051,safety,depend,depends,1051,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:240,security,validat,validate,240,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:521,security,control,control,521,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:568,security,assess,assess,568,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:886,security,assess,assess,886,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:142,testability,depend,depending,142,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:183,testability,depend,depending,183,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:521,testability,control,control,521,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1051,testability,depend,depends,1051,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:676,usability,help,helpful,676,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set. Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:53,deployability,modul,modularity,53,It would be nice if scanpy could provide the maximum modularity obtained in the Louvain and Leiden Algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:53,integrability,modular,modularity,53,It would be nice if scanpy could provide the maximum modularity obtained in the Louvain and Leiden Algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:53,modifiability,modul,modularity,53,It would be nice if scanpy could provide the maximum modularity obtained in the Louvain and Leiden Algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:53,safety,modul,modularity,53,It would be nice if scanpy could provide the maximum modularity obtained in the Louvain and Leiden Algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:53,testability,modula,modularity,53,It would be nice if scanpy could provide the maximum modularity obtained in the Louvain and Leiden Algorithm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:38,performance,time,times,38,@Siliegia I actually needed this many times. Here is a PR: https://github.com/theislab/scanpy/pull/819,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,deployability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,integrability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,interoperability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,modifiability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,reliability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,security,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:86,testability,integr,integrates,86,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:972,availability,cluster,clusterings,972,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1052,availability,cluster,clusterings,1052,"appi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1073,availability,cluster,clusterings,1073,"hon that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour = ""black"",. # node_la",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1112,availability,cluster,clusterings,1112," I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour = ""black"",. # node_label = NULL,. # node_label_aggr = NULL,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,deployability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:972,deployability,cluster,clusterings,972,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1052,deployability,cluster,clusterings,1052,"appi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1073,deployability,cluster,clusterings,1073,"hon that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour = ""black"",. # node_la",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1112,deployability,cluster,clusterings,1112," I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour = ""black"",. # node_label = NULL,. # node_label_aggr = NULL,.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:207,energy efficiency,load,loads,207,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:376,energy efficiency,load,load,376,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,integrability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,interoperability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,modifiability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:336,modifiability,pac,package,336,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:207,performance,load,loads,207,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:376,performance,load,load,376,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,reliability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:200,safety,input,input,200,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,security,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:88,testability,integr,integrates,88,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:200,usability,input,input,200,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:464,usability,help,helps,464,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:634,usability,command,commandArgs,634,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy? I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({. library(reticulate). library(SingleCellExperiment). library(glue). library(clustree). }). sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:2521,usability,user,user-images,2521,"(""scanpy""). args <- commandArgs(trailingOnly = TRUE). H5AD_PATH = args[1]. OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")). print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {. adata <- sc$read_h5ad(h5ad_path). return(adata). }. count_clusterings = function(adata){. # Ryan suggests:. # length(grep(""leiden"",names(adata$obs))). clusterings = c(). for (x in adata$obs_keys()){. if (startsWith(x, ""leiden"")){. clusterings = append(clusterings, x). }. }. . return(length(clusterings)). }. set_fig_dimensions = function(num_clusterings){. width = 10. height = (0.6 * num_clusterings). . if (height < 8){. height = 8. }. . png(width = width, height = height). options(repr.plot.width = width, repr.plot.height = height). . return(list(width=width,height=height)). }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)). # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(. x=adata$obs,. prefix=""leiden_"",. # suffix = NULL,. # metadata = NULL,. # count_filter = 0,. # prop_filter = 0.1,. # layout = ""sugiyama"",. # layout = ""tree"",. # use_core_edges = FALSE,. # highlight_core = FALSE,. # node_colour = prefix,. # node_colour_aggr = NULL,. # node_size = ""size"",. # node_size_aggr = NULL,. # node_size_range = c(4, 15),. # node_alpha = 1,. # node_alpha_aggr = NULL,. # node_text_size = 3,. # scale_node_text = FALSE,. # node_text_colour = ""black"",. # node_label = NULL,. # node_label_aggr = NULL,. # node_label_size = 3,. # node_label_nudge = -0.2,. # edge_width = 1.5,. # edge_arrow = TRUE,. # edge_arrow_ends = c(""last"", ""first"", ""both""),. # show_axis = FALSE,. # return = c(""plot"", ""graph"", ""layout""),. ) #+ scale_color_brewer(palette = ""Set3""). g. ggsave(. OUT_PATH, . width = dims$width, . height = dims$height, . dpi = 100. ). ```. I then run it like this from the notebook: . ![image](https://user-images.githubusercontent.com/138931/109053154-6838c180-76aa-11eb-99c3-106fba81629e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:87,availability,cluster,clustering,87,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:903,availability,cluster,clustering,903,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:947,availability,cluster,clustering,947,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:87,deployability,cluster,clustering,87,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:420,deployability,releas,release,420,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:903,deployability,cluster,clustering,903,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:947,deployability,cluster,clustering,947,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:662,interoperability,plug,plug,662,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:145,modifiability,pac,package,145,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:321,modifiability,pac,packages,321,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:411,modifiability,pac,packages,411,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:506,modifiability,pac,packages,506,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:596,performance,disk,disk,596,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1266,performance,disk,disk,1266,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1215,testability,plan,plan,1215,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1194,usability,help,helpful,1194,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1321,usability,help,helps,1321,"I don't have a lot to add. As far as I know there's no native Python implementation of clustering trees. If you want to use the R **{clustree}** package you will need to transfer your data from R to Python in some way. Using straight **{reticulate}** to read a `.h5ad` file like you have here is one option but there are packages that will do it for you including [**{zellkonverter}**](https://bioconductor.org/packages/release/bioc/html/zellkonverter.html), [**{anndata}**](https://cran.r-project.org/web/packages/anndata/index.html) and [**{SeuratDisk}**](https://github.com/mojaveazure/seurat-disk). Once you have a `SingleCellExperiment` or `Seurat` you can plug that directly into **{clustree}**. It should also be possible to call **{clustree}** from Python using [**anndata2ri**](https://github.com/theislab/anndata2ri) but I'm not sure of the details of how to do that. If you only want a basic clustering tree you could just transfer the clustering assignments (by saving to CSV for example). That would probably be easier/quicker than transferring the whole dataset but you would lose the opportunity to overlay other information such as marker gene expression (which is often really helpful). Unless you plan ahead and append that to whatever you save to disk. Sorry, that was longer than I thought 😸! Hope it helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:30,performance,time,time,30,"Thank you both for taking the time to answer. I was hoping there is a python port/re-implementation of clustree, but this is clearly not the case. Nonetheless, the workarounds you suggest are helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:125,usability,clear,clearly,125,"Thank you both for taking the time to answer. I was hoping there is a python port/re-implementation of clustree, but this is clearly not the case. Nonetheless, the workarounds you suggest are helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:192,usability,help,helpful,192,"Thank you both for taking the time to answer. I was hoping there is a python port/re-implementation of clustree, but this is clearly not the case. Nonetheless, the workarounds you suggest are helpful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:436,deployability,contain,contain,436,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:475,energy efficiency,load,load,475,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:536,energy efficiency,reduc,reduction,536,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:57,modifiability,Exten,Extending,57,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:366,performance,memor,memory,366,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:475,performance,load,load,475,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:496,performance,memor,memory,496,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:506,performance,perform,perform,506,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:208,testability,simpl,simplest,208,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:208,usability,simpl,simplest,208,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:366,usability,memor,memory,366,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:496,usability,memor,memory,496,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:506,usability,perform,perform,506,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/671:145,deployability,log,logarithmized,145,You are absolutely right; it's removed: https://github.com/theislab/scanpy/commit/c319ac0699937278e2a231988e72e432c8950c0a. The function expects logarithmized data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/671
https://github.com/scverse/scanpy/issues/671:145,safety,log,logarithmized,145,You are absolutely right; it's removed: https://github.com/theislab/scanpy/commit/c319ac0699937278e2a231988e72e432c8950c0a. The function expects logarithmized data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/671
https://github.com/scverse/scanpy/issues/671:145,security,log,logarithmized,145,You are absolutely right; it's removed: https://github.com/theislab/scanpy/commit/c319ac0699937278e2a231988e72e432c8950c0a. The function expects logarithmized data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/671
https://github.com/scverse/scanpy/issues/671:145,testability,log,logarithmized,145,You are absolutely right; it's removed: https://github.com/theislab/scanpy/commit/c319ac0699937278e2a231988e72e432c8950c0a. The function expects logarithmized data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/671
https://github.com/scverse/scanpy/issues/672:194,energy efficiency,estimat,estimation,194,Hey! It looks like this might be due to the kde. You could pass `cut=0` to ensure the kde doesn't go past 0. It will however look less nice then. But that is just a limitation of kernel density estimation I guess...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/672
https://github.com/scverse/scanpy/issues/672:90,reliability,doe,doesn,90,Hey! It looks like this might be due to the kde. You could pass `cut=0` to ensure the kde doesn't go past 0. It will however look less nice then. But that is just a limitation of kernel density estimation I guess...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/672
https://github.com/scverse/scanpy/issues/673:46,deployability,log,log-transformed,46,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:135,deployability,log,log,135,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:249,deployability,log,log-transformed,249,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:50,integrability,transform,transformed,50,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:139,integrability,transform,transform,139,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:253,integrability,transform,transformed,253,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:50,interoperability,transform,transformed,50,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:139,interoperability,transform,transform,139,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:253,interoperability,transform,transformed,253,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:291,modifiability,paramet,parameter,291,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:102,reliability,doe,doesn,102,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:46,safety,log,log-transformed,46,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:135,safety,log,log,135,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:222,safety,detect,detecting,222,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:249,safety,log,log-transformed,249,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:46,security,log,log-transformed,46,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:135,security,log,log,135,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:222,security,detect,detecting,222,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:249,security,log,log-transformed,249,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:46,testability,log,log-transformed,46,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:135,testability,log,log,135,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:249,testability,log,log-transformed,249,"Yeah, so it appears that the function expects log-transformed data. Otherwise the use of `np.expm1()` doesn't make sense. If you don't log transform your data, you would get the wrong values. It might be worth either auto-detecting whether data was log-transformed or not, or adding it as a parameter.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:82,deployability,log,log-transformed,82,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:86,integrability,transform,transformed,86,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:351,integrability,transform,transformed,351,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:86,interoperability,transform,transformed,86,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:351,interoperability,transform,transformed,351,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:169,modifiability,paramet,parameter,169,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:82,safety,log,log-transformed,82,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:82,security,log,log-transformed,82,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:82,testability,log,log-transformed,82,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:221,testability,simpl,simplify,221,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:139,usability,workflow,workflow,139,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/673:221,usability,simpl,simplify,221,"Hi! Sorry for the very late reply! But yes, this function is assuming the data is log-transformed before ranking genes. That's the typical workflow. It originally had a parameter to check for this, but then we decided to simplify and remove it (#519). There was some brief discussions here about adding an attribute when pp.log1p is run to handle non-transformed data, but I don't think was ever implemented. Might be worth revisiting though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/673
https://github.com/scverse/scanpy/issues/674:0,usability,Close,Closed,0,Closed via #683,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/674:112,integrability,filter,filtering,112,@ivirshup Thanks a lot for your reply....I already have removed all genes that are not expressed at all through filtering part so I doubt if that is the problem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/674
https://github.com/scverse/scanpy/issues/675:76,testability,context,context,76,I'm not sure what you're asking about here. Could you provide a little more context?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:100,availability,error,error,100,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:78,integrability,compon,components,78,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:106,integrability,messag,message,106,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:78,interoperability,compon,components,78,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:106,interoperability,messag,message,106,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:78,modifiability,compon,components,78,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:100,performance,error,error,100,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:100,safety,error,error,100,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:390,security,auth,authored,390,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:758,security,auth,auth,758,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:336,testability,context,context,336,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:100,usability,error,error,100,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2. components), I get an error message saying that it must be greater than 2. I was wondering why? On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>. wrote:. > I'm not sure what you're asking about here. Could you provide a little. > more context? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>. > . >. -- . Harvard-MIT MD-PhD Student. G1, Biophysics. Lander Lab.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:65,integrability,compon,components,65,"I'm not sure. @flying-sheep, why do we insist that more than two components of a diffusion map are computed? I believe these are the relevant lines:. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/tools/_diffmap.py#L41-L43. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/plotting/_tools/scatterplots.py#L463. Something about the first eigenvalue? Possibly related to #170.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:65,interoperability,compon,components,65,"I'm not sure. @flying-sheep, why do we insist that more than two components of a diffusion map are computed? I believe these are the relevant lines:. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/tools/_diffmap.py#L41-L43. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/plotting/_tools/scatterplots.py#L463. Something about the first eigenvalue? Possibly related to #170.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:65,modifiability,compon,components,65,"I'm not sure. @flying-sheep, why do we insist that more than two components of a diffusion map are computed? I believe these are the relevant lines:. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/tools/_diffmap.py#L41-L43. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/plotting/_tools/scatterplots.py#L463. Something about the first eigenvalue? Possibly related to #170.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:238,usability,tool,tools,238,"I'm not sure. @flying-sheep, why do we insist that more than two components of a diffusion map are computed? I believe these are the relevant lines:. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/tools/_diffmap.py#L41-L43. https://github.com/theislab/scanpy/blob/1ac97e83705322989fb0df8528fc0de31f323202/scanpy/plotting/_tools/scatterplots.py#L463. Something about the first eigenvalue? Possibly related to #170.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:155,integrability,compon,components,155,"Yes, it's related to the first eigenvalue: ""2 corresponds to 1 for diff maps"" in that case. But, there is no restriction by computing the default 10 or 15 components anyway...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:155,interoperability,compon,components,155,"Yes, it's related to the first eigenvalue: ""2 corresponds to 1 for diff maps"" in that case. But, there is no restriction by computing the default 10 or 15 components anyway...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/issues/675:155,modifiability,compon,components,155,"Yes, it's related to the first eigenvalue: ""2 corresponds to 1 for diff maps"" in that case. But, there is no restriction by computing the default 10 or 15 components anyway...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675
https://github.com/scverse/scanpy/pull/676:49,usability,support,support,49,> 3.\ We switched to Python 3.7+ though. No more support for python 3.6? That was a quick move from 3.5 support to only 3.7+...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:104,usability,support,support,104,> 3.\ We switched to Python 3.7+ though. No more support for python 3.6? That was a quick move from 3.5 support to only 3.7+...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:156,deployability,log,logg,156,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:5,energy efficiency,cool,cool,5,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:42,modifiability,concern,concern,42,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:109,performance,time,timer,109,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:135,performance,time,timer,135,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:89,safety,prevent,prevent,89,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:156,safety,log,logg,156,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:89,security,preven,prevent,89,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:156,security,log,logg,156,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:42,testability,concern,concern,42,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:156,testability,log,logg,156,"Very cool, @flying-sheep! Really the only concern I have is whether we need an option to prevent setting the timer (or for setting the timer) when calling `logg.info`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:18,safety,test,tests,18,"I’m doing all the tests @ivirshup proposed. If my explanations to the questions I left open are sufficient, I’ll merge this! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:18,testability,test,tests,18,"I’m doing all the tests @ivirshup proposed. If my explanations to the questions I left open are sufficient, I’ll merge this! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:25,deployability,log,log,25,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:167,deployability,log,logged,167,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:191,deployability,log,log,191,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:220,deployability,log,log,220,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:321,deployability,log,log,321,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:58,energy efficiency,current,current,58,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:93,modifiability,paramet,parameter,93,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:66,performance,time,time,66,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:104,performance,time,time,104,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:154,performance,time,time,154,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:236,performance,time,time,236,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:303,performance,time,time,303,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:355,performance,time,time,355,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:25,safety,log,log,25,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:167,safety,log,logged,167,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:191,safety,log,log,191,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:220,safety,log,log,220,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:321,safety,log,log,321,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:25,security,log,log,25,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:167,security,log,logged,167,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:191,security,log,log,191,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:220,security,log,log,220,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:321,security,log,log,321,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:25,testability,log,log,25,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:167,testability,log,logged,167,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:191,testability,log,log,191,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:220,testability,log,log,220,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:321,testability,log,log,321,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:224,usability,hint,hint,224,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:283,usability,custom,customize,283,"OK, one more change. The log functions now all return the current time and have the optional parameter `time: datetime`. If you pass something there, the time will be logged:. ```py. start = log.info('foo'). # do stuff. log.hint('bar', time=start) # --> bar (00:00:02). ```. You can customize where the time ends up via `log.*('blah {time_passed}: blub', time=...)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:31,deployability,log,logging,31,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:39,deployability,modul,module,39,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:0,energy efficiency,Cool,Cool,0,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:22,energy efficiency,cool,cool,22,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:39,modifiability,modul,module,39,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:31,safety,log,logging,31,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:39,safety,modul,module,39,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:31,security,log,logging,31,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:31,testability,log,logging,31,Cool solution! Really cool new logging module! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:366,availability,error,errors,366,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:1,deployability,log,logg,1,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:20,deployability,log,logg,20,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:150,deployability,log,logging,150,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:158,deployability,modul,modules,158,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:236,deployability,log,logging,236,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:244,deployability,modul,module,244,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:299,deployability,log,logg,299,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:314,deployability,log,logg,314,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:388,deployability,log,logg,388,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:504,deployability,log,logg,504,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:133,energy efficiency,core,core,133,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:219,energy efficiency,adapt,adapted,219,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:219,integrability,adapt,adapted,219,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:219,interoperability,adapt,adapted,219,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:158,modifiability,modul,modules,158,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:205,modifiability,pac,packages,205,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:219,modifiability,adapt,adapted,219,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:244,modifiability,modul,module,244,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:366,performance,error,errors,366,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:418,performance,time,time,418,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:1,safety,log,logg,1,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:20,safety,log,logg,20,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:150,safety,log,logging,150,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:158,safety,modul,modules,158,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:236,safety,log,logging,236,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:244,safety,modul,module,244,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:299,safety,log,logg,299,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:314,safety,log,logg,314,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:366,safety,error,errors,366,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:388,safety,log,logg,388,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:504,safety,log,logg,504,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:1,security,log,logg,1,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:20,security,log,logg,20,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:150,security,log,logging,150,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:236,security,log,logging,236,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:299,security,log,logg,299,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:314,security,log,logg,314,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:388,security,log,logg,388,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:504,security,log,logg,504,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:1,testability,log,logg,1,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:20,testability,log,logg,20,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:150,testability,log,logging,150,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:236,testability,log,logging,236,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:299,testability,log,logg,299,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:314,testability,log,logg,314,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:388,testability,log,logg,388,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:504,testability,log,logg,504,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:366,usability,error,errors,366,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:10,deployability,log,logging,10,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:18,deployability,modul,module,18,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:48,deployability,log,logging,48,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:204,deployability,API,API,204,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:298,deployability,log,logging,298,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:446,deployability,log,logging,446,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:77,integrability,pub,public,77,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:204,integrability,API,API,204,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:204,interoperability,API,API,204,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:18,modifiability,modul,module,18,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:235,reliability,stabil,stabilize,235,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:420,reliability,doe,doesn,420,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:10,safety,log,logging,10,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:18,safety,modul,module,18,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:48,safety,log,logging,48,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:298,safety,log,logging,298,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:446,safety,log,logging,446,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:10,security,log,logging,10,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:48,security,log,logging,48,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:103,security,sign,signature,103,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:298,security,log,logging,298,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:446,security,log,logging,446,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:10,testability,log,logging,10,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:48,testability,log,logging,48,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:298,testability,log,logging,298,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:446,testability,log,logging,446,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:28,usability,undo,undocumented,28,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:249,usability,document,document,249,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/pull/676:428,usability,interact,interact,428,"Well, the logging module is undocumented, only `logging.print_versions()` is public. And I changed the signature, it no longer works like `print`, so they’ll have to change it anyway. But I think the new API is pretty sweet, so we can stabilize and document it now. Python has `warnings.warn` and `logging.warning`. I think we should follow suit. You can of course always do `from warnings import warn` and use that. It doesn’t interact with our logging system (yet), though!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676
https://github.com/scverse/scanpy/issues/677:73,integrability,compon,components,73,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:107,integrability,compon,components,107,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:73,interoperability,compon,components,73,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:107,interoperability,compon,components,107,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:73,modifiability,compon,components,73,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/677:107,modifiability,compon,components,107,"I made a PR (#678) to fix this. However, if you computed UMAP with three components you don't need to set `components`. I.e. this works as well:. `sc.pl.umap(adata_g, color=[key_added], cmap='viridis', projection='3d')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/677
https://github.com/scverse/scanpy/issues/680:111,availability,cluster,clusters,111,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:0,deployability,UPDAT,UPDATE,0,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:111,deployability,cluster,clusters,111,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:0,safety,UPDAT,UPDATE,0,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:0,security,UPDAT,UPDATE,0,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:256,usability,visual,visualizing,256,"UPDATE: . So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using:. `sc.tl.paga(adata, groups='seurat_clusters')`. . Would you guys say this is a legal move to make statistically speaking? . I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:177,modifiability,variab,variable,177,"sounds to me like you know what you're doing. Where do you get your PCA from? Seurat? Otherwise I would consider running `sc.pp.pca(adata, svd_solver='arpack')` with the highly variable gene set and your pre-processed data from Seurat. Also, for the future, an easier way to go between Seurat and Scanpy might be [anndata2ri](https://www.github.com/flying-sheep/anndata2ri). I have an example notebook for that [here](https://www.github.com/LuckyMD/Code_snippets).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:156,performance,perform,performed,156,"Wow well that certainly makes things a lot easier, thank you for creating that code snippet! It seems as though Scanpy was smart enough to realize I hadn't performed PCA and thus did it for me using the default settings. However as you have suggested I should be using 'arpack' to make my results reproducible, will do! . Thank you for your reply!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:156,usability,perform,performed,156,"Wow well that certainly makes things a lot easier, thank you for creating that code snippet! It seems as though Scanpy was smart enough to realize I hadn't performed PCA and thus did it for me using the default settings. However as you have suggested I should be using 'arpack' to make my results reproducible, will do! . Thank you for your reply!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:111,availability,cluster,clusters,111,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:436,availability,cluster,clustering,436,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:2,deployability,UPDAT,UPDATE,2,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:111,deployability,cluster,clusters,111,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:436,deployability,cluster,clustering,436,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:2,safety,UPDAT,UPDATE,2,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:2,security,UPDAT,UPDATE,2,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:263,usability,visual,visualizing,263,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/680:818,usability,help,help,818,"> UPDATE: So after running `sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)` I was able to run PAGA w/ Seurat clusters using: `sc.tl.paga(adata, groups='seurat_clusters')`. > . > Would you guys say this is a legal move to make statistically speaking? > . > I am visualizing the trajectory inferences using `scanpy.pl.paga(adata)`. Hi thank you very much for your information. I'm running the same purpose, using the Seurat object with clustering information for Scanpy trajectory analysis. May I ask why your adata object has so much information inside. Mine is: . ![image](https://github.com/scverse/scanpy/assets/82354685/ef2554df-a067-45f5-8e4f-8527540a7994). What I do is reading the h5ad file and running sc.tl.paga(adata, groups='seurat_clusters'). May I ask if it is correct or not. Thanks again for your kind help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680
https://github.com/scverse/scanpy/issues/681:169,availability,robust,robust,169,"Hi,. Check your `sc.pp.pca()` results. This is normally the origin of the differences. If you use `svd_solver='arpack'` it's more reproducible between systems (and more robust in general), but there will still be some differences due to the underlying numeric libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:169,reliability,robust,robust,169,"Hi,. Check your `sc.pp.pca()` results. This is normally the origin of the differences. If you use `svd_solver='arpack'` it's more reproducible between systems (and more robust in general), but there will still be some differences due to the underlying numeric libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:169,safety,robust,robust,169,"Hi,. Check your `sc.pp.pca()` results. This is normally the origin of the differences. If you use `svd_solver='arpack'` it's more reproducible between systems (and more robust in general), but there will still be some differences due to the underlying numeric libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:152,deployability,contain,contain,152,"Thanks for the fast reply @LuckyMD, . I have check this already, we are using `svd_solver = 'arpack`, the topology of the PCA look exactly the same and contain the same genes. But I think I could work with this explanation for now :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:222,availability,sli,slightly,222,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:238,availability,cluster,clustering,238,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:331,availability,sli,slight,331,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:238,deployability,cluster,clustering,238,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:128,energy efficiency,current,current,128,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:171,performance,time,times,171,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:141,reliability,pra,practices,141,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:222,reliability,sli,slightly,222,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:331,reliability,sli,slight,331,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:298,testability,trace,traceable,298,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:25,usability,help,help,25,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/681:253,usability,visual,visualization,253,"Sorry I can't be of more help. Would be curious to hear what the issue was when you find it. I've reproduced a case study for a current best practices manuscript multiple times on different systems, and the results differ slightly on the clustering and visualization side, but that has always been traceable back to the PCA. Often slight differences in PC-space turn into larger differences in the connectivities matrix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/681
https://github.com/scverse/scanpy/issues/682:177,deployability,api,api,177,"There is no convenience function for that, I fear. You can look at the code by clicking on the _Edit on GitHub_ button of its [doc page](https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.paga_path.html#scanpy.pl.paga_path).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/682
https://github.com/scverse/scanpy/issues/682:177,integrability,api,api,177,"There is no convenience function for that, I fear. You can look at the code by clicking on the _Edit on GitHub_ button of its [doc page](https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.paga_path.html#scanpy.pl.paga_path).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/682
https://github.com/scverse/scanpy/issues/682:177,interoperability,api,api,177,"There is no convenience function for that, I fear. You can look at the code by clicking on the _Edit on GitHub_ button of its [doc page](https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.paga_path.html#scanpy.pl.paga_path).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/682
https://github.com/scverse/scanpy/issues/682:72,usability,close,close,72,"Just came across this, thanks again for bringing up this issue! We will close the issue soon, as based on the provided information and the discussion so far, it seems that the question has been addressed and hopefully resolved :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/682
https://github.com/scverse/scanpy/issues/684:20,deployability,log,logging,20,"First step: removed logging of `asctime`, which we never had before: https://github.com/theislab/scanpy/commit/4650568cf61d8a654e64abd1ec0807e19423f1ff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:20,safety,log,logging,20,"First step: removed logging of `asctime`, which we never had before: https://github.com/theislab/scanpy/commit/4650568cf61d8a654e64abd1ec0807e19423f1ff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:20,security,log,logging,20,"First step: removed logging of `asctime`, which we never had before: https://github.com/theislab/scanpy/commit/4650568cf61d8a654e64abd1ec0807e19423f1ff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:20,testability,log,logging,20,"First step: removed logging of `asctime`, which we never had before: https://github.com/theislab/scanpy/commit/4650568cf61d8a654e64abd1ec0807e19423f1ff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:39,deployability,log,logged,39,"Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. a37efc71876f1cd9ace1165d7f774e390d30343d. The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:181,performance,time,time,181,"Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. a37efc71876f1cd9ace1165d7f774e390d30343d. The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:39,safety,log,logged,39,"Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. a37efc71876f1cd9ace1165d7f774e390d30343d. The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:39,security,log,logged,39,"Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. a37efc71876f1cd9ace1165d7f774e390d30343d. The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:39,testability,log,logged,39,"Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. a37efc71876f1cd9ace1165d7f774e390d30343d. The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened].",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:27,deployability,log,logging,27,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:98,deployability,log,logging,98,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:343,deployability,log,logged,343,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:421,deployability,log,log,421,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:452,deployability,version,version,452,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:452,integrability,version,version,452,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:196,interoperability,format,formatting,196,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:452,modifiability,version,version,452,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:568,performance,time,time,568,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:27,safety,log,logging,27,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:98,safety,log,logging,98,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:343,safety,log,logged,343,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:421,safety,log,log,421,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:749,safety,test,testing,749,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:792,safety,prevent,prevents,792,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:27,security,log,logging,27,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:98,security,log,logging,98,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:343,security,log,logged,343,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:421,security,log,log,421,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:792,security,preven,prevents,792,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:27,testability,log,logging,27,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:98,testability,log,logging,98,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:343,testability,log,logged,343,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:421,testability,log,log,421,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:749,testability,test,testing,749,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:249,usability,interact,interactive,249,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/issues/684:277,usability,document,document,277,"Thank you! > step: removed logging of `asctime`, which we never had before. I took that from your logging code in the data science bowl notebook, but I agree: People should add `{asctime}` to all formatting themselves if they use scanpy in some non-interactive code. We should document how to do that. > Second step: reverted things that were logged at a level equal or higher than 4 to `debug`. Good! FYI: I changed the log levels because the initial version of the overhaul only allowed timing information in `info`. > The only thing that remains is to reformat the time output, which now displays many useless digits after the seconds comma [and fix all other places in which similar things happened]. I’m on it. Seems like I missed that because testing code only uses full seconds, which prevents the milliseconds from showing up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/684
https://github.com/scverse/scanpy/pull/685:37,safety,test,test,37,"multiple are fine, I like having the test setup in the tests directory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/685
https://github.com/scverse/scanpy/pull/685:55,safety,test,tests,55,"multiple are fine, I like having the test setup in the tests directory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/685
https://github.com/scverse/scanpy/pull/685:37,testability,test,test,37,"multiple are fine, I like having the test setup in the tests directory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/685
https://github.com/scverse/scanpy/pull/685:55,testability,test,tests,55,"multiple are fine, I like having the test setup in the tests directory",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/685
https://github.com/scverse/scanpy/issues/686:376,usability,user,user-images,376,"If I call `sc.pl.paga(adata)` before `paga_compare`:. ```python. import scanpy as sc. sc.set_figure_params(dpi=70). adata = sc.datasets.paul15(). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(adata). sc.tl.paga(adata, groups='paul15_clusters'). sc.pl.paga(adata). sc.pl.paga_compare(adata, color='leiden'). ```. it produces something even crazier:. ![image](https://user-images.githubusercontent.com/1140359/59246397-3b006300-8bea-11e9-9803-f5ce162b97d0.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/686
https://github.com/scverse/scanpy/issues/687:7,availability,replic,replicated,7,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:22,availability,error,error,22,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:141,availability,error,error,141,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:40,deployability,instal,installation,40,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:64,deployability,instal,install,64,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:22,performance,error,error,22,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:141,performance,error,error,141,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:22,safety,error,error,22,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:141,safety,error,error,141,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:22,usability,error,error,22,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:141,usability,error,error,141,"I have replicated the error using local installation with 'pip3 install scanpy'. When I run the regress_out code on a jupyter notebook, same error appears.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:67,deployability,version,versions,67,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:97,deployability,instal,installing,97,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:67,integrability,version,versions,67,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:56,interoperability,compatib,compatible,56,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/687:67,modifiability,version,versions,67,That’s statsmodels/statsmodels#5759. We already require compatible versions from everything. try installing scanpy from git and it should work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/687
https://github.com/scverse/scanpy/issues/688:89,deployability,build,builds,89,@falexwolf @flying-sheep @fidelram @ivirshup Kind reminder. Due to this problem the PAGA builds as part of dyno are failing. Are you able to reproduce my problem? Do you need any more information?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:116,deployability,fail,failing,116,@falexwolf @flying-sheep @fidelram @ivirshup Kind reminder. Due to this problem the PAGA builds as part of dyno are failing. Are you able to reproduce my problem? Do you need any more information?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:116,reliability,fail,failing,116,@falexwolf @flying-sheep @fidelram @ivirshup Kind reminder. Due to this problem the PAGA builds as part of dyno are failing. Are you able to reproduce my problem? Do you need any more information?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:143,interoperability,specif,specific,143,"Makes sense, thanks for clarifying. This issue is then related to lmcinnes/umap#252. @flying-sheep @lmcinnes Is there any way I can turn these specific warnings off?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:191,availability,error,errors,191,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:191,performance,error,errors,191,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:191,safety,error,errors,191,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:162,testability,simpl,simplefilter,162,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:162,usability,simpl,simplefilter,162,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:191,usability,error,errors,191,"https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings. ```py. import numba. import warnings. with warnings.catch_warnings():. warnings.simplefilter('ignore', numba.errors.NumbaDeprecationWarning):. do_thing(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:236,deployability,contain,container,236,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:299,deployability,build,builds,299,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:312,deployability,contain,container,312,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:70,integrability,wrap,wrapper,70,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:70,interoperability,wrapper,wrapper,70,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:328,performance,time,time,328,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/688:1,reliability,rca,rcannood,1,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper. Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings? My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd? Best,. Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688
https://github.com/scverse/scanpy/issues/689:84,usability,tool,tool,84,"Hi, to run umap, you need to run pp.neighbors at first, and neighbors have use_rep. tool.umap use representation that pp.neighbors used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689
https://github.com/scverse/scanpy/issues/691:523,availability,error,error,523,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:110,deployability,version,versions,110,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:158,deployability,log,logistic,158,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:5,energy efficiency,Current,Currently,5,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:110,integrability,version,versions,110,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:247,integrability,batch,batch,247,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:457,integrability,batch,batch-corrected,457,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:110,modifiability,version,versions,110,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:247,performance,batch,batch,247,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:422,performance,perform,performing,422,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:457,performance,batch,batch-corrected,457,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:523,performance,error,error,523,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:105,safety,test,test,105,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:158,safety,log,logistic,158,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:488,safety,test,tests,488,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:523,safety,error,error,523,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:158,security,log,logistic,158,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:105,testability,test,test,105,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:158,testability,log,logistic,158,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:167,testability,regress,regression,167,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:488,testability,test,tests,488,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:422,usability,perform,performing,422,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:523,usability,error,error,523,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:655,availability,avail,available,655,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:358,integrability,batch,batch,358,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:417,integrability,batch,batch,417,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:569,integrability,batch,batch-corrected,569,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:603,integrability,batch,batch,603,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:358,performance,batch,batch,358,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:417,performance,batch,batch,417,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:569,performance,batch,batch-corrected,569,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:603,performance,batch,batch,603,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:655,reliability,availab,available,655,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:655,safety,avail,available,655,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:73,security,auth,authors,73,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:655,security,availab,available,655,"Hi @LuckyMD,. Thank you for your rapid reply! In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? . Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels. I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,. BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1534,availability,avail,available,1534,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:305,deployability,compos,compositions,305,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:596,deployability,compos,compositions,596,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:41,energy efficiency,current,current,41,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:466,energy efficiency,estimat,estimation,466,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1169,energy efficiency,optim,optimal,1169,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:182,integrability,batch,batch,182,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:228,integrability,batch,batch,228,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:333,integrability,batch,batches,333,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:422,integrability,batch,batches,422,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:453,integrability,batch,batch,453,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:644,integrability,batch,batches,644,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:706,integrability,batch,batch,706,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:883,integrability,batch,batch,883,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:305,modifiability,compos,compositions,305,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:596,modifiability,compos,compositions,596,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:723,modifiability,scenario,scenarios,723,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:182,performance,batch,batch,182,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:228,performance,batch,batch,228,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:333,performance,batch,batches,333,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:422,performance,batch,batches,422,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:453,performance,batch,batch,453,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:644,performance,batch,batches,644,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:706,performance,batch,batch,706,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:883,performance,batch,batch,883,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1679,performance,content,content,1679,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:54,reliability,pra,practices,54,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1534,reliability,availab,available,1534,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:25,safety,review,review,25,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:991,safety,detect,detection,991,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1139,safety,detect,detect,1139,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1233,safety,detect,detection,1233,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1411,safety,test,testing,1411,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1534,safety,avail,available,1534,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:991,security,detect,detection,991,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1139,security,detect,detect,1139,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1148,security,sign,signal,1148,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1233,security,detect,detection,1233,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1534,security,availab,available,1534,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:25,testability,review,review,25,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:699,testability,simpl,simple,699,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:1411,testability,test,testing,1411,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:699,usability,simpl,simple,699,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:. 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods. 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:279,availability,slo,slow,279,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:534,availability,cluster,clusters,534,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:534,deployability,cluster,clusters,534,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:84,integrability,batch,batch,84,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:195,integrability,batch,batch,195,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:567,integrability,batch,batch,567,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:212,modifiability,scenario,scenarios,212,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:84,performance,batch,batch,84,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:163,performance,perform,performance,163,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:195,performance,batch,batch,195,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:567,performance,batch,batch,567,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:279,reliability,slo,slow,279,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:188,testability,simpl,simple,188,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:163,usability,perform,performance,163,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:188,usability,simpl,simple,188,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/691:310,usability,help,help,310,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward. In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:. ```. tmp_cluster=adata.obs['leiden'].astype(int). ```. ```. %%R -i tmp_cluster -i adata -o tmp_allMarkers. tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""). tmp_allMarkers<-as.list(tmp_allMarkers). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691
https://github.com/scverse/scanpy/issues/692:194,interoperability,specif,specify,194,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:21,security,hardcod,hardcoded,21,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:160,testability,simpl,simply,160,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:160,usability,simpl,simply,160,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:0,usability,Document,Documented,0,Documented in 5ab9def681264a95acd48aeda45b46ad0b86fee8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:299,deployability,fail,fail,299,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:109,reliability,doe,does,109,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:299,reliability,fail,fail,299,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:326,safety,input,inputting,326,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:326,usability,input,inputting,326,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:647,usability,command,command,647,"Thanks for the answer and for your suggestion @flying-sheep. I can't make this work though. The color choice does not work while also using scatter plot to plot `x` and `y` values. The suggested code only works when `basis is not None`. However using the plain basis function e.g. `sc.pl.umap` will fail earlier than this for inputting a color. ### In summary,. this will not work:. ```. sc.pl.umap(adata, color=#fe57a1). ```. this will work. ```. sc.pl.scatter(adata, basis='umap', color=#fe57a1). ```. this will not work (what I actually want to do). ```. sc.pl.scatter(adata, x='dpt_pseudotime', y='SOMEGENE', color=#fe57a1). ```. For the last command the code makes a check already before passing the color argument to the plotting function. https://github.com/theislab/scanpy/blob/5ab9def681264a95acd48aeda45b46ad0b86fee8/scanpy/plotting/_anndata.py#L121. EDITED so it links to the correct code line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:0,testability,Simpl,Simply,0,Simply adding the matplotlib `is_color_like` seems to do the trick. ```. and (color is None or color in adata.obs.keys() or color in adata.var.index or is_color_like(color))):. ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:0,usability,Simpl,Simply,0,Simply adding the matplotlib `is_color_like` seems to do the trick. ```. and (color is None or color in adata.obs.keys() or color in adata.var.index or is_color_like(color))):. ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/692:38,security,modif,modification,38,"@Xparx I had this issue as well. Your modification worked for me, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692
https://github.com/scverse/scanpy/issues/693:80,availability,cluster,cluster,80,"I should have said that the figure edges are cropped (not the graph edges). See cluster 0 below. That also happens if I plot without borders. `sc.pl.paga(adata, layout='fr', threshold=0.015, node_size_scale=4, edge_width_scale=1.5, save='threshold0.015.png')`. ![pagathreshold0 015](https://user-images.githubusercontent.com/31794642/60177054-d7904b00-980f-11e9-85f8-b0a89b003f70.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/693
https://github.com/scverse/scanpy/issues/693:80,deployability,cluster,cluster,80,"I should have said that the figure edges are cropped (not the graph edges). See cluster 0 below. That also happens if I plot without borders. `sc.pl.paga(adata, layout='fr', threshold=0.015, node_size_scale=4, edge_width_scale=1.5, save='threshold0.015.png')`. ![pagathreshold0 015](https://user-images.githubusercontent.com/31794642/60177054-d7904b00-980f-11e9-85f8-b0a89b003f70.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/693
https://github.com/scverse/scanpy/issues/693:291,usability,user,user-images,291,"I should have said that the figure edges are cropped (not the graph edges). See cluster 0 below. That also happens if I plot without borders. `sc.pl.paga(adata, layout='fr', threshold=0.015, node_size_scale=4, edge_width_scale=1.5, save='threshold0.015.png')`. ![pagathreshold0 015](https://user-images.githubusercontent.com/31794642/60177054-d7904b00-980f-11e9-85f8-b0a89b003f70.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/693
https://github.com/scverse/scanpy/pull/694:0,deployability,Upgrad,Upgrading,0,"Upgrading to 1.3.0 triggers #159, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/694
https://github.com/scverse/scanpy/pull/694:0,modifiability,Upgrad,Upgrading,0,"Upgrading to 1.3.0 triggers #159, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/694
https://github.com/scverse/scanpy/pull/694:7,deployability,upgrad,upgrading,7,I mean upgrading to scipy 1.3.0 triggers theislab/anndata#159 (and now #695 #696),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/694
https://github.com/scverse/scanpy/pull/694:7,modifiability,upgrad,upgrading,7,I mean upgrading to scipy 1.3.0 triggers theislab/anndata#159 (and now #695 #696),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/694
https://github.com/scverse/scanpy/issues/695:52,availability,down,downgrade,52,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695
https://github.com/scverse/scanpy/issues/695:116,interoperability,compatib,compatibility,116,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695
https://github.com/scverse/scanpy/issues/696:52,availability,down,downgrade,52,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/696
https://github.com/scverse/scanpy/issues/696:116,interoperability,compatib,compatibility,116,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/696
https://github.com/scverse/scanpy/issues/697:112,usability,close,closest,112,I think it is! We could start using GitHub’s projects feature: https://github.com/theislab/scanpy/projects. The closest thing we have rn is #453,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/697
https://github.com/scverse/scanpy/issues/698:31,performance,time,time,31,"Hi @ivirshup,. It took us some time to come up with a reproducible example. But then we realized that this behavior only is present in scanpy==1.4 (and perhaps earlier). In 1.4.1 and 1.4.3 crazy Z-scores are gone. The results seem to be correct. Regarding ties. Quickly scanning through the thread I didn't find any mentions of tie correction (maybe I missed something). In any case, please consider the crazy Z-score issue resolved. Tie correction still could be discussed, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:107,usability,behavi,behavior,107,"Hi @ivirshup,. It took us some time to come up with a reproducible example. But then we realized that this behavior only is present in scanpy==1.4 (and perhaps earlier). In 1.4.1 and 1.4.3 crazy Z-scores are gone. The results seem to be correct. Regarding ties. Quickly scanning through the thread I didn't find any mentions of tie correction (maybe I missed something). In any case, please consider the crazy Z-score issue resolved. Tie correction still could be discussed, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:315,interoperability,standard,standard,315,"Great, just wanted to make sure that we had that out of the way first. About the tie correction, I'm not the most knowledgeable person about our differential expression testing. Maybe @falexwolf or @a-munoz-rojas would be able to comment on this? @idavydov, what do you think our results should be? Is there a gold standard in scipy.stats which we should be returning the same results as?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:169,safety,test,testing,169,"Great, just wanted to make sure that we had that out of the way first. About the tie correction, I'm not the most knowledgeable person about our differential expression testing. Maybe @falexwolf or @a-munoz-rojas would be able to comment on this? @idavydov, what do you think our results should be? Is there a gold standard in scipy.stats which we should be returning the same results as?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:169,testability,test,testing,169,"Great, just wanted to make sure that we had that out of the way first. About the tie correction, I'm not the most knowledgeable person about our differential expression testing. Maybe @falexwolf or @a-munoz-rojas would be able to comment on this? @idavydov, what do you think our results should be? Is there a gold standard in scipy.stats which we should be returning the same results as?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:128,usability,person,person,128,"Great, just wanted to make sure that we had that out of the way first. About the tie correction, I'm not the most knowledgeable person about our differential expression testing. Maybe @falexwolf or @a-munoz-rojas would be able to comment on this? @idavydov, what do you think our results should be? Is there a gold standard in scipy.stats which we should be returning the same results as?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:256,availability,slo,slower,256,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:59,deployability,version,version,59,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:286,deployability,version,version,286,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:357,deployability,version,version,357,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:278,energy efficiency,current,current,278,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:59,integrability,version,version,59,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:286,integrability,version,version,286,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:357,integrability,version,version,357,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:59,modifiability,version,version,59,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:286,modifiability,version,version,286,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:357,modifiability,version,version,357,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:336,performance,perform,performance,336,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:79,reliability,doe,doesn,79,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:256,reliability,slo,slower,256,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:242,security,sign,significantly,242,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:336,usability,perform,performance,336,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:247,deployability,version,version,247,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:247,integrability,version,version,247,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:353,interoperability,share,share,353,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:247,modifiability,version,version,247,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:154,testability,simpl,simple,154,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:154,usability,simpl,simple,154,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/698:169,integrability,sub,submit,169,"Just brining this thread back up - I think it would be useful to have tie-correction in the code. @falexwolf what do you think? If we agree, @idavydov would you be able submit a pull request to implement it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698
https://github.com/scverse/scanpy/issues/699:380,availability,operat,operations,380,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:261,deployability,scale,scale,261,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:261,energy efficiency,scale,scale,261,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:94,integrability,sub,subsetting,94,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:261,modifiability,scal,scale,261,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:261,performance,scale,scale,261,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:395,performance,perform,performed,395,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:321,usability,behavi,behavior,321,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/699:395,usability,perform,performed,395,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(). sc.pp.scale(pbmc). ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699
https://github.com/scverse/scanpy/issues/700:5,safety,reme,remember,5,"If I remember correctly, problem is caused by this https://github.com/theislab/anndata/issues/145.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700
https://github.com/scverse/scanpy/issues/701:48,availability,cluster,clusters,48,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:155,availability,cluster,cluster,155,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:226,availability,cluster,cluster,226,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:48,deployability,cluster,clusters,48,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:155,deployability,cluster,cluster,155,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:226,deployability,cluster,cluster,226,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:199,security,sign,significant,199,Ideally I could get something like a `Number of clusters x Number of Genes` array of p-values that shows the p-value of fold change for each gene for gene cluster so I can pick out the statistically significant genes for each cluster,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:50,availability,cluster,cluster,50,"I believe the array you're getting should be the `cluster x genes` array you're looking for. And the p-values should be increasing as the list descends (with a max of 1). Could you give an example of what your results looks like, in particular, showing how they differ from what you expect? Admittedly, the differential expression data structure is kinda hard to work with at the moment. If you're on the master branch of `scanpy` you should be able to use `sc.get.rank_genes_groups_df(adata, group_key)` for an easier to use structure. This also has key-word arguments for setting a p-value cutoff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:50,deployability,cluster,cluster,50,"I believe the array you're getting should be the `cluster x genes` array you're looking for. And the p-values should be increasing as the list descends (with a max of 1). Could you give an example of what your results looks like, in particular, showing how they differ from what you expect? Admittedly, the differential expression data structure is kinda hard to work with at the moment. If you're on the master branch of `scanpy` you should be able to use `sc.get.rank_genes_groups_df(adata, group_key)` for an easier to use structure. This also has key-word arguments for setting a p-value cutoff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:923,availability,cluster,cluster-,923,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:1023,availability,down,down,1023,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:1169,availability,Cluster,ClusterOneVsRest,1169,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:1240,availability,Cluster,ClusterOneVsRest,1240,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:923,deployability,cluster,cluster-,923,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:1169,deployability,Cluster,ClusterOneVsRest,1169,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:1240,deployability,Cluster,ClusterOneVsRest,1240,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:376,interoperability,distribut,distribution,376,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/701:295,usability,document,documentations,295,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right? ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701
https://github.com/scverse/scanpy/issues/702:180,reliability,pra,practices,180,"Hi @Seandelao,. You can find tutorials and the description of the commands for scanpy in the documentation [here](https://scanpy.readthedocs.io/). If you're interested in our best-practices workflow, which is based on scanpy (but not only), you can find that [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:66,usability,command,commands,66,"Hi @Seandelao,. You can find tutorials and the description of the commands for scanpy in the documentation [here](https://scanpy.readthedocs.io/). If you're interested in our best-practices workflow, which is based on scanpy (but not only), you can find that [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:93,usability,document,documentation,93,"Hi @Seandelao,. You can find tutorials and the description of the commands for scanpy in the documentation [here](https://scanpy.readthedocs.io/). If you're interested in our best-practices workflow, which is based on scanpy (but not only), you can find that [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:190,usability,workflow,workflow,190,"Hi @Seandelao,. You can find tutorials and the description of the commands for scanpy in the documentation [here](https://scanpy.readthedocs.io/). If you're interested in our best-practices workflow, which is based on scanpy (but not only), you can find that [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,deployability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,integrability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,interoperability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,modifiability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,reliability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,security,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:53,testability,integr,integrate,53,"> Great, thank you! hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,deployability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,integrability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:180,integrability,batch,batch-effect-correction,180,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:269,integrability,batch,batch,269,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,interoperability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,modifiability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:180,performance,batch,batch-effect-correction,180,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:269,performance,batch,batch,269,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,reliability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,security,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:23,testability,integr,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:253,testability,simpl,simpler,253,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:253,usability,simpl,simpler,253,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,deployability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,integrability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:188,integrability,batch,batch-effect-correction,188,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:277,integrability,batch,batch,277,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,interoperability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,modifiability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:188,performance,batch,batch-effect-correction,188,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:277,performance,batch,batch,277,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,reliability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,security,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,testability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:261,testability,simpl,simpler,261,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:261,usability,simpl,simpler,261,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,deployability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,deployability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,integrability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:188,integrability,batch,batch-effect-correction,188,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:277,integrability,batch,batch,277,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,integrability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,interoperability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,interoperability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,modifiability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,modifiability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:188,performance,batch,batch-effect-correction,188,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:277,performance,batch,batch,277,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,reliability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,reliability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,security,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,security,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:31,testability,integr,integration,31,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:261,testability,simpl,simpler,261,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:817,testability,Integr,IntegrateData,817,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:261,usability,simpl,simpler,261,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:508,usability,help,help-information,508,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:529,usability,help,help,529,"> Hi @grimwoo,. > . > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. . I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: . adata001$Sample <- ""001"". adata002$Sample <- ""002"". adata002$Sample <- ""003"". adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11). adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:181,integrability,batch,batch,181,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:321,integrability,batch,batch,321,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:386,integrability,batch,batch,386,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:534,integrability,batch,batches,534,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:275,modifiability,paramet,parameters,275,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:506,modifiability,scenario,scenarios,506,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:181,performance,batch,batch,181,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:321,performance,batch,batch,321,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:350,performance,perform,performs,350,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:386,performance,batch,batch,386,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:534,performance,batch,batches,534,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:439,security,ident,identities,439,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:314,testability,simpl,simple,314,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:225,usability,document,documentation,225,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:314,usability,simpl,simple,314,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:350,usability,perform,performs,350,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:619,usability,usab,usable,619,"Just concatenate the datasets first and then use Combat. Something like:. ```. adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). sc.pp.combat(adata_merge, batch='sample'). ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:193,integrability,batch,batch,193,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:341,integrability,batch,batch,341,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:406,integrability,batch,batch,406,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:554,integrability,batch,batches,554,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:295,modifiability,paramet,parameters,295,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:526,modifiability,scenario,scenarios,526,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:193,performance,batch,batch,193,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:341,performance,batch,batch,341,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:370,performance,perform,performs,370,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:406,performance,batch,batch,406,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:554,performance,batch,batches,554,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:459,security,ident,identities,459,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:334,testability,simpl,simple,334,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:245,usability,document,documentation,245,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:334,usability,simpl,simple,334,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:370,usability,perform,performs,370,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:639,usability,usab,usable,639,"> Just concatenate the datasets first and then use Combat. Something like:. > . > ```. > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'). > sc.pp.combat(adata_merge, batch='sample'). > ```. > . > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:57,availability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:587,availability,error,error,587,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:57,performance,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:587,performance,error,error,587,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:57,safety,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:587,safety,error,error,587,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:57,usability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:107,usability,support,supported,107,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:587,usability,error,error,587,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/issues/702:605,usability,help,help,605,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702
https://github.com/scverse/scanpy/pull/703:59,deployability,api,api,59,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:159,energy efficiency,load,loaded,159,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:59,integrability,api,api,59,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:59,interoperability,api,api,59,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:159,performance,load,loaded,159,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:223,security,session,sessions,223,"Instant imports would definitely be nice, but I'll settle for less noticeable ones. Potentially we could even use the [LazyLoader](https://docs.python.org/3/library/importlib.html#importlib.util.LazyLoader) for interactive sessions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/703:211,usability,interact,interactive,211,"Instant imports would definitely be nice, but I'll settle for less noticeable ones. Potentially we could even use the [LazyLoader](https://docs.python.org/3/library/importlib.html#importlib.util.LazyLoader) for interactive sessions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703
https://github.com/scverse/scanpy/pull/704:252,deployability,modul,module,252,Please make the import conditional using. 1. a `python_version < 3.8` [environment marker](https://www.python.org/dev/peps/pep-0345/#environment-markers) and. 2. Try importing the 3.8 location (`importlib.metadata`) first before importing the backport module.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:252,modifiability,modul,module,252,Please make the import conditional using. 1. a `python_version < 3.8` [environment marker](https://www.python.org/dev/peps/pep-0345/#environment-markers) and. 2. Try importing the 3.8 location (`importlib.metadata`) first before importing the backport module.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:252,safety,modul,module,252,Please make the import conditional using. 1. a `python_version < 3.8` [environment marker](https://www.python.org/dev/peps/pep-0345/#environment-markers) and. 2. Try importing the 3.8 location (`importlib.metadata`) first before importing the backport module.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:75,energy efficiency,profil,profiling,75,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:188,energy efficiency,current,current,188,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:279,energy efficiency,profil,profile,279,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:315,energy efficiency,profil,profile,315,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:75,performance,profil,profiling,75,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:92,performance,time,times,92,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:279,performance,profil,profile,279,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:315,performance,profil,profile,315,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:225,security,session,session,225,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:62,usability,command,commands,62,"Done. Just so I can figure out how I did it later, here's the commands for profiling import times:. ```python. import cProfile. from pstats import SortKey, Stats. # This imports scanpy to current env, can't be run twice in a session. cProfile.run(""import scanpy"", ""import-scanpy.profile""). p = Stats(""import-scanpy.profile""). p.sort_stats(SortKey.CUMULATIVE). p.print_stats(30). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:13,safety,test,test,13,"I started to test also with 3.8, but sadly there’s a bug in scipy: scipy/scipy#10354",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:13,testability,test,test,13,"I started to test also with 3.8, but sadly there’s a bug in scipy: scipy/scipy#10354",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:190,deployability,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:224,deployability,build,builds,224,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:236,deployability,fail,fail,236,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:190,integrability,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:190,modifiability,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:236,reliability,fail,fail,236,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:190,safety,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:190,testability,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:230,availability,error,error,230,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:45,deployability,build,build,45,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:62,deployability,fail,fail,62,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:147,deployability,fail,fails,147,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:230,performance,error,error,230,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:62,reliability,fail,fail,62,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:147,reliability,fail,fails,147,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:230,safety,error,error,230,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:230,usability,error,error,230,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:57,deployability,releas,release,57,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:25,safety,test,test,25,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:25,testability,test,test,25,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:15,availability,error,error,15,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:21,energy efficiency,load,loading,21,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:97,energy efficiency,load,loading,97,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:206,integrability,coupl,couple,206,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:206,modifiability,coupl,couple,206,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:15,performance,error,error,15,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:21,performance,load,loading,21,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:97,performance,load,loading,97,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:15,safety,error,error,15,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:206,testability,coupl,couple,206,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:15,usability,error,error,15,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:113,usability,learn,learn,113,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/704:0,reliability,Doe,Doesn,0,Doesn’t seem like a problem with this. Further discussion in #739.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704
https://github.com/scverse/scanpy/pull/705:28,reliability,doe,does,28,"Thanks for the PR! @Koncopd does this look right to you? I'm not too familiar with this code, but this looks like [what cellranger does](https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L36-L67) to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:131,reliability,doe,does,131,"Thanks for the PR! @Koncopd does this look right to you? I'm not too familiar with this code, but this looks like [what cellranger does](https://github.com/10XGenomics/cellranger/blob/5f5a6293bbc067e1965e50f0277286914b96c908/lib/python/cellranger/analysis/stats.py#L36-L67) to me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:65,deployability,version,version,65,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:131,deployability,version,version,131,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:65,integrability,version,version,65,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:131,integrability,version,version,131,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:65,modifiability,version,version,65,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:81,modifiability,refact,refactored,81,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:131,modifiability,version,version,131,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:81,performance,refactor,refactored,81,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:449,deployability,releas,release,449,"@Koncopd, ah, this was as far back as I could trace the code initially: https://github.com/theislab/scanpy/commit/af4a82a2540eee65c732cb5e401d2145846e6d97. Now I've found an earlier commit from @falexwolf at https://github.com/theislab/scanpy/commit/43e71fe8577a8b3a51dc2117bd431911001d9869. @falexwolf, does this change look right to you? @LuckyMD, I'm not sure if this would count as backward breaking if it's a bug. Should definitely go into the release notes, maybe as warning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:304,reliability,doe,does,304,"@Koncopd, ah, this was as far back as I could trace the code initially: https://github.com/theislab/scanpy/commit/af4a82a2540eee65c732cb5e401d2145846e6d97. Now I've found an earlier commit from @falexwolf at https://github.com/theislab/scanpy/commit/43e71fe8577a8b3a51dc2117bd431911001d9869. @falexwolf, does this change look right to you? @LuckyMD, I'm not sure if this would count as backward breaking if it's a bug. Should definitely go into the release notes, maybe as warning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:46,testability,trace,trace,46,"@Koncopd, ah, this was as far back as I could trace the code initially: https://github.com/theislab/scanpy/commit/af4a82a2540eee65c732cb5e401d2145846e6d97. Now I've found an earlier commit from @falexwolf at https://github.com/theislab/scanpy/commit/43e71fe8577a8b3a51dc2117bd431911001d9869. @falexwolf, does this change look right to you? @LuckyMD, I'm not sure if this would count as backward breaking if it's a bug. Should definitely go into the release notes, maybe as warning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:41,usability,progress,progress,41,Hi! Thank you for looking into this. Any progress with the issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:119,performance,time,time,119,"@falexwolf @ivirshup it would be good to clear this up soon. If this is correct, then we choose HVGs incorrectly every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:41,usability,clear,clear,41,"@falexwolf @ivirshup it would be good to clear this up soon. If this is correct, then we choose HVGs incorrectly every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:139,safety,reme,remedied,139,"I wonder if the absolute value was initially used as the dispersions can be negative due to negative mean expression values? That could be remedied by taking the absolute value of only the dispersions though, and not the dispersion offset by the median dispersion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:69,usability,user,user-images,69,These figures might be clarifying:. Original code:. ![image](https://user-images.githubusercontent.com/32548783/62890385-63532d80-bd11-11e9-8c49-9912fa51ee40.png). Corrected code:. ![image](https://user-images.githubusercontent.com/32548783/62890408-72d27680-bd11-11e9-913a-63f625e3491d.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:198,usability,user,user-images,198,These figures might be clarifying:. Original code:. ![image](https://user-images.githubusercontent.com/32548783/62890385-63532d80-bd11-11e9-8c49-9912fa51ee40.png). Corrected code:. ![image](https://user-images.githubusercontent.com/32548783/62890408-72d27680-bd11-11e9-913a-63f625e3491d.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:105,safety,reme,remember,105,"We could ask the authors of the paper, they even mention taking the absolute value in their methods if I remember correctly",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/pull/705:17,security,auth,authors,17,"We could ask the authors of the paper, they even mention taking the absolute value in their methods if I remember correctly",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705
https://github.com/scverse/scanpy/issues/706:702,safety,test,test,702,"Hi,. I've tried to reproduce this with scanpy 1.4.3+80.g740c557 on the pbmc68k_reduced dataset and it works for me. I did the following:. ```. adata = sc.datasets.pbmc68k_reduced() . sc.pp.filter_cells(adata, min_counts=10) . sc.pp.filter_genes(adata, min_cells=5) . sc.pp.normalize_per_cell(adata) . sc.pp.log1p(adata). sc.tl.rank_genes_groups(adata, groupby='bulk_labels', groups=['CD56+ NK', 'Dendritic', 'CD34+'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True). ```. The `sc.tl.rank_genes_groups()` call is taken more or less from your example. Could you check that this works for you, and otherwise provide a minimal reproducible example that I could test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:702,testability,test,test,702,"Hi,. I've tried to reproduce this with scanpy 1.4.3+80.g740c557 on the pbmc68k_reduced dataset and it works for me. I did the following:. ```. adata = sc.datasets.pbmc68k_reduced() . sc.pp.filter_cells(adata, min_counts=10) . sc.pp.filter_genes(adata, min_cells=5) . sc.pp.normalize_per_cell(adata) . sc.pp.log1p(adata). sc.tl.rank_genes_groups(adata, groupby='bulk_labels', groups=['CD56+ NK', 'Dendritic', 'CD34+'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True). ```. The `sc.tl.rank_genes_groups()` call is taken more or less from your example. Could you check that this works for you, and otherwise provide a minimal reproducible example that I could test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:660,usability,minim,minimal,660,"Hi,. I've tried to reproduce this with scanpy 1.4.3+80.g740c557 on the pbmc68k_reduced dataset and it works for me. I did the following:. ```. adata = sc.datasets.pbmc68k_reduced() . sc.pp.filter_cells(adata, min_counts=10) . sc.pp.filter_genes(adata, min_cells=5) . sc.pp.normalize_per_cell(adata) . sc.pp.log1p(adata). sc.tl.rank_genes_groups(adata, groupby='bulk_labels', groups=['CD56+ NK', 'Dendritic', 'CD34+'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True). ```. The `sc.tl.rank_genes_groups()` call is taken more or less from your example. Could you check that this works for you, and otherwise provide a minimal reproducible example that I could test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:237,usability,user,user-images,237,"Hi LuckyMD,. Thanks for your response. I've been able to reproduce the code that you posted above in a fresh notebook with no issue. I'm not sure why it is working now and wasn't working before for the pbmc68k data set. ![image](https://user-images.githubusercontent.com/52190118/60123779-3beed400-9756-11e9-9b91-95e96044750e.png). However I am still having trouble with my own data set, so perhaps it is related how I processed the data instead? I'm not sure. I've sent you an email link to the h5ad file and ipynb file for MRE. Thanks a mil!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/706:254,integrability,filter,filtering,254,I'm afraid I have a limited bandwidth at the moment and won't be able to get around to this this week. I would suggest you check whether there are negative values in your dataset and whether there are some genes that are not expressed in your data (gene filtering) in the meantime.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706
https://github.com/scverse/scanpy/issues/707:206,energy efficiency,current,current,206,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:75,modifiability,variab,variable,75,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:51,performance,perform,performed,51,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:219,reliability,pra,practices,219,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:19,testability,Regress,Regressing,19,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:51,usability,perform,performed,51,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:277,usability,tool,tools,277,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:91,integrability,sub,subsetting,91,"Hi everyone! I was wondering about the same issue. Would you then suggest to regress after subsetting HVGs (for speed reasons) and then re-searching and re-subsetting HVGs after the unwanted source of variation is corrected for? In such a way, one would inevitably loose some interesting genes (due to the first HVGs subsetting), but a cleaner signal would be obtained at the end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:156,integrability,sub,subsetting,156,"Hi everyone! I was wondering about the same issue. Would you then suggest to regress after subsetting HVGs (for speed reasons) and then re-searching and re-subsetting HVGs after the unwanted source of variation is corrected for? In such a way, one would inevitably loose some interesting genes (due to the first HVGs subsetting), but a cleaner signal would be obtained at the end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:317,integrability,sub,subsetting,317,"Hi everyone! I was wondering about the same issue. Would you then suggest to regress after subsetting HVGs (for speed reasons) and then re-searching and re-subsetting HVGs after the unwanted source of variation is corrected for? In such a way, one would inevitably loose some interesting genes (due to the first HVGs subsetting), but a cleaner signal would be obtained at the end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:344,security,sign,signal,344,"Hi everyone! I was wondering about the same issue. Would you then suggest to regress after subsetting HVGs (for speed reasons) and then re-searching and re-subsetting HVGs after the unwanted source of variation is corrected for? In such a way, one would inevitably loose some interesting genes (due to the first HVGs subsetting), but a cleaner signal would be obtained at the end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:77,testability,regress,regress,77,"Hi everyone! I was wondering about the same issue. Would you then suggest to regress after subsetting HVGs (for speed reasons) and then re-searching and re-subsetting HVGs after the unwanted source of variation is corrected for? In such a way, one would inevitably loose some interesting genes (due to the first HVGs subsetting), but a cleaner signal would be obtained at the end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:203,energy efficiency,current,current,203,"I don't think I would do HVG selection twice. I reckon it's often not such a time-taking process to regress out across all genes. Just do it once, and then do HVG selection afterwards. However, with the current setup this is difficult as `sc.pp.regress_out` also removes the offset, and thus has 0 mean per gene. This prevents `sc.pp.highly_variable` from binning the genes by mean expression (see #722).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:77,performance,time,time-taking,77,"I don't think I would do HVG selection twice. I reckon it's often not such a time-taking process to regress out across all genes. Just do it once, and then do HVG selection afterwards. However, with the current setup this is difficult as `sc.pp.regress_out` also removes the offset, and thus has 0 mean per gene. This prevents `sc.pp.highly_variable` from binning the genes by mean expression (see #722).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:318,safety,prevent,prevents,318,"I don't think I would do HVG selection twice. I reckon it's often not such a time-taking process to regress out across all genes. Just do it once, and then do HVG selection afterwards. However, with the current setup this is difficult as `sc.pp.regress_out` also removes the offset, and thus has 0 mean per gene. This prevents `sc.pp.highly_variable` from binning the genes by mean expression (see #722).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:318,security,preven,prevents,318,"I don't think I would do HVG selection twice. I reckon it's often not such a time-taking process to regress out across all genes. Just do it once, and then do HVG selection afterwards. However, with the current setup this is difficult as `sc.pp.regress_out` also removes the offset, and thus has 0 mean per gene. This prevents `sc.pp.highly_variable` from binning the genes by mean expression (see #722).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:100,testability,regress,regress,100,"I don't think I would do HVG selection twice. I reckon it's often not such a time-taking process to regress out across all genes. Just do it once, and then do HVG selection afterwards. However, with the current setup this is difficult as `sc.pp.regress_out` also removes the offset, and thus has 0 mean per gene. This prevents `sc.pp.highly_variable` from binning the genes by mean expression (see #722).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:696,deployability,scale,scale,696,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:696,energy efficiency,scale,scale,696,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:430,interoperability,format,format,430,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:408,modifiability,variab,variable,408,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:696,modifiability,scal,scale,696,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:696,performance,scale,scale,696,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:42,testability,regress,regress,42,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:62,testability,Regress,Regress,62,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:592,usability,user,user-images,592,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:731,usability,user,user-images,731,"Is HVG function designed to be done after regress out? ```. # Regress out process of interest from expression data. sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') . # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio). sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', . batch_key=None, n_top_genes =2000). print('\n','Number of highly variable genes: {:d}'.format(. np.sum(adata_b_rn_sub2.var['highly_variable']))). rcParams['figure.figsize']=(10,5). sc.pl.highly_variable_genes(adata_b_rn_sub2). ```. ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png). If I do scale before HVG. ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:110,energy efficiency,current,current,110,"As I wrote above, our standard hvg selection function bins genes by mean, and thus doesn't work well with the current implementation of `regress_out`. You would have to add the offset again to do this properly, and not just use the residuals.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:22,interoperability,standard,standard,22,"As I wrote above, our standard hvg selection function bins genes by mean, and thus doesn't work well with the current implementation of `regress_out`. You would have to add the offset again to do this properly, and not just use the residuals.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:83,reliability,doe,doesn,83,"As I wrote above, our standard hvg selection function bins genes by mean, and thus doesn't work well with the current implementation of `regress_out`. You would have to add the offset again to do this properly, and not just use the residuals.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:271,availability,cluster,cluster,271,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:203,deployability,scale,scale,203,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:271,deployability,cluster,cluster,271,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:203,energy efficiency,scale,scale,203,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:351,energy efficiency,current,current,351,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:267,integrability,sub,sub-cluster,267,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:203,modifiability,scal,scale,203,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:203,performance,scale,scale,203,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:124,testability,context,context,124,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/707:138,testability,regress,regressing,138,"I have the same problem and I do not know how to 'add the offset again'. Could you maybe tell me how this is done? Just for context: I am regressing out cell cycle and mitochondrial genes, after which I scale and do PCA. I then analyse my dataset, but when I want to sub-cluster, I want to run hvg selection again. Since that is not possible with the current implementation of regress_out, I am trying to find a solution for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707
https://github.com/scverse/scanpy/issues/709:45,integrability,batch,batches,45,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:64,integrability,sub,subset,64,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:78,integrability,batch,batch,78,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:157,integrability,batch,batch,157,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:45,performance,batch,batches,45,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:78,performance,batch,batch,78,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:157,performance,batch,batch,157,"If you're trying to just look at one of your batches, you could subset to the batch, then plot that? Something like:. ```python. sc.pl.umap(adata[adata.obs[""batch""] == ""batch1""], ... ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:49,integrability,batch,batch,49,"another option is. ```. sc.pl.umap(adata, color='batch', groups=['batch1']. ```. ![image](https://user-images.githubusercontent.com/4964309/60266476-67a1c380-98e8-11e9-9a33-d18d986bb0e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:49,performance,batch,batch,49,"another option is. ```. sc.pl.umap(adata, color='batch', groups=['batch1']. ```. ![image](https://user-images.githubusercontent.com/4964309/60266476-67a1c380-98e8-11e9-9a33-d18d986bb0e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:98,usability,user,user-images,98,"another option is. ```. sc.pl.umap(adata, color='batch', groups=['batch1']. ```. ![image](https://user-images.githubusercontent.com/4964309/60266476-67a1c380-98e8-11e9-9a33-d18d986bb0e3.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/709:238,modifiability,paramet,parameters,238,"> @fidelram Hi fidelram, in your example, is it possible to set different colors for ""CD56"" and ""NK""? Thanks Dan. You could be able to set different colors, if you have more than one group, by using the ""colormap""/""cmap"" and/or ""palette"" parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/709
https://github.com/scverse/scanpy/issues/710:275,deployability,Version,Version,275,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:275,integrability,Version,Version,275,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:275,modifiability,Version,Version,275,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:235,performance,memor,memory,235,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:235,usability,memor,memory,235,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:309,usability,help,helpful,309,"That's a little surprising to me. I think I've been able to do a 2d umap on similar sized data on a smaller server. Could you provide some more information? In particular, I'm wondering what step in the process you're at, and how much memory was being used before that step. Version information would also be helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:259,deployability,build,build,259,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:157,energy efficiency,Current,Current,157,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:369,energy efficiency,cpu,cpu,369,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:225,integrability,compon,components,225,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:225,interoperability,compon,components,225,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:225,modifiability,compon,components,225,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:131,performance,overhead,overhead,131,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:336,performance,memor,memory,336,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:369,performance,cpu,cpu,369,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:709,performance,memor,memory,709,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:336,usability,memor,memory,336,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:709,usability,memor,memory,709,"Hi Isaac,. Thank for reply! Yeh, 2d is no problem - I can run it on my laptop, however projecting data on 3d space results in huge overhead even on 1 epoch. Current step of my analysis is 1M cells with 40 features (principal components). At this point, I can build neighbors graph (with 10 neighbors) using UMAP. Here are some stats of memory usage until this point:. `cpu=00:14:31, mem=7958.77575 GB s, io=9.16637 GB, vmem=2.988G, maxvmem=17.595G`. After that - I use . `sc.tl.umap(adata, n_components=3, maxiter=epoch)` where `epoch` is `int` in [0:600]. The process is getting killed at `epoch=1`. I have tried this on machines with up to 256G RAM, but the process has always been killed at the point when memory usage was overshooting the maximum. Hope it makes sense! Thanks,. g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:51,integrability,transform,transformer,51,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:51,interoperability,transform,transformer,51,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:161,performance,memor,memory,161,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:78,usability,learn,learn,78,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:98,usability,learn,learn,98,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:161,usability,memor,memory,161,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:33,deployability,fail,fails,33,"Hm, this is strange, that scanpy fails and umap works. Need to investigate...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/710:33,reliability,fail,fails,33,"Hm, this is strange, that scanpy fails and umap works. Need to investigate...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710
https://github.com/scverse/scanpy/issues/712:189,deployability,fail,fails,189,"This has been an issue with loompy, resolved [here](https://github.com/linnarsson-lab/loompy/commit/2e620d2db8f203462dbd0ee8df08ed9fc6b8b0d7). However, one would need to check why it still fails with anndata's `read_loom` while running well with loompy directly. For me, it works after the latest loompy commit. Could you provide a minimal example?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:189,reliability,fail,fails,189,"This has been an issue with loompy, resolved [here](https://github.com/linnarsson-lab/loompy/commit/2e620d2db8f203462dbd0ee8df08ed9fc6b8b0d7). However, one would need to check why it still fails with anndata's `read_loom` while running well with loompy directly. For me, it works after the latest loompy commit. Could you provide a minimal example?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:332,usability,minim,minimal,332,"This has been an issue with loompy, resolved [here](https://github.com/linnarsson-lab/loompy/commit/2e620d2db8f203462dbd0ee8df08ed9fc6b8b0d7). However, one would need to check why it still fails with anndata's `read_loom` while running well with loompy directly. For me, it works after the latest loompy commit. Could you provide a minimal example?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:101,safety,test,test,101,"Just checked on this example. ```. adata = sc.AnnData(np.array([[1, 2],[-1, 2]])). adata.write_loom('test.loom'). adata = sc.read('test.loom', sparse=True). ```. and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:131,safety,test,test,131,"Just checked on this example. ```. adata = sc.AnnData(np.array([[1, 2],[-1, 2]])). adata.write_loom('test.loom'). adata = sc.read('test.loom', sparse=True). ```. and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:101,testability,test,test,101,"Just checked on this example. ```. adata = sc.AnnData(np.array([[1, 2],[-1, 2]])). adata.write_loom('test.loom'). adata = sc.read('test.loom', sparse=True). ```. and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:131,testability,test,test,131,"Just checked on this example. ```. adata = sc.AnnData(np.array([[1, 2],[-1, 2]])). adata.write_loom('test.loom'). adata = sc.read('test.loom', sparse=True). ```. and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:60,deployability,version,version,60,"Thanks, I haven't try the latest loompy yet. With the older version, I could still read the negative values using loompy directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:60,integrability,version,version,60,"Thanks, I haven't try the latest loompy yet. With the older version, I could still read the negative values using loompy directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/issues/712:60,modifiability,version,version,60,"Thanks, I haven't try the latest loompy yet. With the older version, I could still read the negative values using loompy directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712
https://github.com/scverse/scanpy/pull/713:59,safety,review,review,59,@ivirshup I didn't notice until now that you requested the review. The code looks ok but I will do some tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713
https://github.com/scverse/scanpy/pull/713:104,safety,test,tests,104,@ivirshup I didn't notice until now that you requested the review. The code looks ok but I will do some tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713
https://github.com/scverse/scanpy/pull/713:59,testability,review,review,59,@ivirshup I didn't notice until now that you requested the review. The code looks ok but I will do some tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713
https://github.com/scverse/scanpy/pull/713:104,testability,test,tests,104,@ivirshup I didn't notice until now that you requested the review. The code looks ok but I will do some tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713
https://github.com/scverse/scanpy/issues/714:7,energy efficiency,heat,heatmap,7,`sc.pl.heatmap` allows to pass kwargs to matplotlib `imshow`. This can include smoothing (`interpolation`). Is this what you have in mind? . For example:. ![image](https://user-images.githubusercontent.com/4964309/60266093-9d927800-98e7-11e9-9ba4-77b12ff52ecd.png). This is the same case without smoothing (interpolation):. ![image](https://user-images.githubusercontent.com/4964309/60266142-bef36400-98e7-11e9-96d2-3911b6477539.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/714:172,usability,user,user-images,172,`sc.pl.heatmap` allows to pass kwargs to matplotlib `imshow`. This can include smoothing (`interpolation`). Is this what you have in mind? . For example:. ![image](https://user-images.githubusercontent.com/4964309/60266093-9d927800-98e7-11e9-9ba4-77b12ff52ecd.png). This is the same case without smoothing (interpolation):. ![image](https://user-images.githubusercontent.com/4964309/60266142-bef36400-98e7-11e9-96d2-3911b6477539.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/714:341,usability,user,user-images,341,`sc.pl.heatmap` allows to pass kwargs to matplotlib `imshow`. This can include smoothing (`interpolation`). Is this what you have in mind? . For example:. ![image](https://user-images.githubusercontent.com/4964309/60266093-9d927800-98e7-11e9-9ba4-77b12ff52ecd.png). This is the same case without smoothing (interpolation):. ![image](https://user-images.githubusercontent.com/4964309/60266142-bef36400-98e7-11e9-96d2-3911b6477539.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/714:28,availability,cluster,cluster,28,"yea, except the thing about cluster borders. smoothing across those isn’t a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/714:28,deployability,cluster,cluster,28,"yea, except the thing about cluster borders. smoothing across those isn’t a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/714:5,safety,except,except,5,"yea, except the thing about cluster borders. smoothing across those isn’t a good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/714
https://github.com/scverse/scanpy/issues/718:481,energy efficiency,model,model,481,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:446,integrability,filter,filtering,446,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:45,interoperability,distribut,distributions,45,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:29,performance,time,time,29,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:198,reliability,pra,practice,198,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:268,reliability,doe,does,268,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:481,security,model,model,481,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:0,usability,Person,Personally,0,"Personally, I spend a bit of time looking at distributions of various qc metrics using `sns.distplot` and `sns.jointplot`. It would be useful if we could suggest cutoffs, but I'm not sure what best practice for this would be. Just for reference, [here's what `scPipe` does](https://github.com/LuyiTian/scPipe/blob/master/R/qc.R#L72). I think it's hard to suggest a good all-round cutoff, since the biology can vary so much. I'd like to think any filtering should be based off some model of what a ""poor quality cell"" looks like for the dataset at hand.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:159,availability,echo,echo,159,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:119,deployability,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:119,integrability,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:136,interoperability,distribut,distributions,136,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:201,interoperability,distribut,distributions,201,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:222,interoperability,distribut,distributions,222,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:119,modifiability,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:340,reliability,pra,practices,340,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:119,safety,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:119,testability,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:12,usability,experien,experience,12,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:418,availability,echo,echo,418,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:47,deployability,pipelin,pipeline-for-everything,47,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:143,deployability,pipelin,pipelines,143,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:375,deployability,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:47,integrability,pipelin,pipeline-for-everything,47,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:143,integrability,pipelin,pipelines,143,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:375,integrability,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:395,interoperability,distribut,distributions,395,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:493,interoperability,distribut,distributions,493,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:514,interoperability,distribut,distributions,514,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:375,modifiability,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:637,reliability,pra,practices,637,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:375,safety,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:763,security,auth,authored,763,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:1131,security,auth,auth,1131,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:375,testability,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:265,usability,experien,experience,265,"Thanks everyone! I wonder how this affects one-pipeline-for-everything. portals, like the EBI single cell expression atlas... and standarized. pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>. wrote:. > Based on my experience setting a single cutoff for all datasets will not. > work, as I've used a lot of different cutoffs depending on the. > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s. > suggestion of looking at distributions. Joint distributions being a lot. > more important than individual histograms. There's a small discussion about. > it in our best practices paper. > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:19,reliability,Doe,Does,19,"Ah. I see. Thanks! Does it requires a minimum of 4000 UMIs or a. maximum of 4000 UMIs ? And do we know the other thresholds, like. %-mito, number of cells, number genes ? On Mon, Jul 1, 2019 at 3:40 PM MalteDLuecken <notifications@github.com> wrote:. >. > As far as I'm aware Cell Ranger works with global cutoffs. I recall seeing 4k umis being used. That seems quite excessive to me. I've seen small cells fall out of the analysis with that threshold. I guess if you just want to work with sth, then a conservative cutoff might be fine, but you can always do better with your data. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:628,security,auth,authored,628,"Ah. I see. Thanks! Does it requires a minimum of 4000 UMIs or a. maximum of 4000 UMIs ? And do we know the other thresholds, like. %-mito, number of cells, number genes ? On Mon, Jul 1, 2019 at 3:40 PM MalteDLuecken <notifications@github.com> wrote:. >. > As far as I'm aware Cell Ranger works with global cutoffs. I recall seeing 4k umis being used. That seems quite excessive to me. I've seen small cells fall out of the analysis with that threshold. I guess if you just want to work with sth, then a conservative cutoff might be fine, but you can always do better with your data. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:38,usability,minim,minimum,38,"Ah. I see. Thanks! Does it requires a minimum of 4000 UMIs or a. maximum of 4000 UMIs ? And do we know the other thresholds, like. %-mito, number of cells, number genes ? On Mon, Jul 1, 2019 at 3:40 PM MalteDLuecken <notifications@github.com> wrote:. >. > As far as I'm aware Cell Ranger works with global cutoffs. I recall seeing 4k umis being used. That seems quite excessive to me. I've seen small cells fall out of the analysis with that threshold. I guess if you just want to work with sth, then a conservative cutoff might be fine, but you can always do better with your data. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:85,deployability,version,versions,85,"I recall a minimum of 4000 UMIs, although I'm not sure if that's still true in newer versions. I don't know about other thresholds. I think you should be able to find this on their website though (somewhere hidden).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:85,integrability,version,versions,85,"I recall a minimum of 4000 UMIs, although I'm not sure if that's still true in newer versions. I don't know about other thresholds. I think you should be able to find this on their website though (somewhere hidden).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:85,modifiability,version,versions,85,"I recall a minimum of 4000 UMIs, although I'm not sure if that's still true in newer versions. I don't know about other thresholds. I think you should be able to find this on their website though (somewhere hidden).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:11,usability,minim,minimum,11,"I recall a minimum of 4000 UMIs, although I'm not sure if that's still true in newer versions. I don't know about other thresholds. I think you should be able to find this on their website though (somewhere hidden).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:80,safety,safe,safe,80,Google can't find 4000 and UMI on their website:. https://www.google.com/search?safe=off&ei=PC4aXYaLBcWAad_4vdAG&q=%224000%22++inurl%3A10xgenomics+umi&oq=%224000%22++inurl%3A10xgenomics+umi&gs_l=psy-ab.3...3637.3826..3984...0.0..0.76.218.3......0....1..gws-wiz.......0i71.oYAb2OqUy7I. >.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:108,performance,time,time,108,I remember it being quite difficult to find out how CellRanger does things... I found it in some pdf at the time. The 4k UMI threshold might be old by now as well though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:63,reliability,doe,does,63,I remember it being quite difficult to find out how CellRanger does things... I found it in some pdf at the time. The 4k UMI threshold might be old by now as well though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/718:2,safety,reme,remember,2,I remember it being quite difficult to find out how CellRanger does things... I found it in some pdf at the time. The 4k UMI threshold might be old by now as well though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718
https://github.com/scverse/scanpy/issues/719:221,deployability,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:221,integrability,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:221,modifiability,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:20,reliability,pra,practicality,20,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:221,safety,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:221,testability,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:70,usability,visual,visualize,70,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:236,availability,cluster,clusters,236,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:255,availability,cluster,clustering,255,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:334,availability,cluster,clusters,334,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:236,deployability,cluster,clusters,236,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:255,deployability,cluster,clustering,255,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:334,deployability,cluster,clusters,334,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:38,energy efficiency,measur,measure,38,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:401,reliability,doe,doesn,401,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:498,security,trust,trust,498,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:459,usability,support,support,459,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:508,usability,user,user,508,"On the cells per group, is there some measure we could use to have a sense of whether the KDE is representative, potentially throwing a warning if it's below a threshold? I came across this when I wanted to look at the densities of two clusters (out of a clustering solution with ~15) without changing the relative positions of those clusters in the embedding. If the computational expense of the KDE doesn't become prohibitive, I think it'd be reasonable to support this case. I also think we can trust the user to choose how many plots they want to generate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:252,security,trust,trust,252,"@LuckyMD, just got reminded of this by a potential solution to the many groups issues. Topological KDE plots:. ![image](https://user-images.githubusercontent.com/8238804/69897510-29319880-13a1-11ea-8086-ad5f4ca2d137.png). Unfortunately, I'm not sure I trust how plotly is generating them, but they do look nice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:128,usability,user,user-images,128,"@LuckyMD, just got reminded of this by a potential solution to the many groups issues. Topological KDE plots:. ![image](https://user-images.githubusercontent.com/8238804/69897510-29319880-13a1-11ea-8086-ad5f4ca2d137.png). Unfortunately, I'm not sure I trust how plotly is generating them, but they do look nice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:258,availability,reliab,reliable,258,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:18,energy efficiency,cool,cool,18,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:258,reliability,reliab,reliable,258,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:232,security,assess,assess,232,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:337,usability,user,user,337,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:15,usability,help,help,15,"I think it can help for when groups overlap. When two solid colors overlap, you lose visual information. Here you may be able to plot multiple groupings on the same plot, since there are no fills so it's more likely you can disambiguate the boundaries when overlaps occur.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:85,usability,visual,visual,85,"I think it can help for when groups overlap. When two solid colors overlap, you lose visual information. Here you may be able to plot multiple groupings on the same plot, since there are no fills so it's more likely you can disambiguate the boundaries when overlaps occur.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:370,energy efficiency,estimat,estimate,370,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:122,integrability,wrap,wrapper,122,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:122,interoperability,wrapper,wrapper,122,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:253,modifiability,concern,concerns,253,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:80,performance,time,time,80,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:253,testability,concern,concerns,253,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/719:229,usability,user,user,229,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide! To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719
https://github.com/scverse/scanpy/issues/720:176,deployability,api,api,176,What is that you want to plot? do you have some example in mind that we can take a look? Have you tried [embedding_density](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.pl.embedding_density.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/720
https://github.com/scverse/scanpy/issues/720:176,integrability,api,api,176,What is that you want to plot? do you have some example in mind that we can take a look? Have you tried [embedding_density](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.pl.embedding_density.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/720
https://github.com/scverse/scanpy/issues/720:176,interoperability,api,api,176,What is that you want to plot? do you have some example in mind that we can take a look? Have you tried [embedding_density](https://icb-scanpy.readthedocs-hosted.com/en/stable/api/scanpy.pl.embedding_density.html),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/720
https://github.com/scverse/scanpy/issues/720:173,interoperability,standard,standard,173,"I haven't tried embedding_density yet; let me give that a try. The example I have in mind is reminiscent of contour plots from flow cytometry data, as an alternative to the standard dot plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/720
https://github.com/scverse/scanpy/pull/721:61,deployability,api,api,61,"> I'm not sure how to deal with references to external under api, so there are two copies of external at the moment. can you elaborate please? I don’t understand this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/721
https://github.com/scverse/scanpy/pull/721:61,integrability,api,api,61,"> I'm not sure how to deal with references to external under api, so there are two copies of external at the moment. can you elaborate please? I don’t understand this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/721
https://github.com/scverse/scanpy/pull/721:61,interoperability,api,api,61,"> I'm not sure how to deal with references to external under api, so there are two copies of external at the moment. can you elaborate please? I don’t understand this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/721
https://github.com/scverse/scanpy/pull/721:151,testability,understand,understand,151,"> I'm not sure how to deal with references to external under api, so there are two copies of external at the moment. can you elaborate please? I don’t understand this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/721
https://github.com/scverse/scanpy/issues/722:121,deployability,scale,scale,121,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:44,energy efficiency,measur,measure,44,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:121,energy efficiency,scale,scale,121,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:226,integrability,transform,transformed,226,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:255,integrability,batch,batched,255,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:226,interoperability,transform,transformed,226,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:121,modifiability,scal,scale,121,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:190,modifiability,variab,variable,190,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:121,performance,scale,scale,121,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:255,performance,batch,batched,255,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:41,deployability,scale,scale,41,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:101,deployability,scale,scale,101,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:413,deployability,continu,continuous,413,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,deployability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:41,energy efficiency,scale,scale,41,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:101,energy efficiency,scale,scale,101,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:205,integrability,batch,batches,205,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:372,integrability,batch,batch,372,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:459,integrability,Batch,Batch,459,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:565,integrability,batch,batch,565,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,integrability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,interoperability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:41,modifiability,scal,scale,41,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:101,modifiability,scal,scale,101,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:277,modifiability,scenario,scenarios,277,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,modifiability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:41,performance,scale,scale,41,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:101,performance,scale,scale,101,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:205,performance,batch,batches,205,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:372,performance,batch,batch,372,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:459,performance,Batch,Batch,459,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:565,performance,batch,batch,565,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,reliability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:613,safety,compl,complex,613,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:613,security,compl,complex,613,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,security,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:401,testability,regress,regress,401,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:626,testability,integr,integration,626,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then? As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:19,deployability,updat,updates,19,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:221,deployability,scale,scale,221,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:265,deployability,updat,updates,265,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:221,energy efficiency,scale,scale,221,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:221,modifiability,scal,scale,221,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:221,performance,scale,scale,221,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:19,safety,updat,updates,19,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:265,safety,updat,updates,265,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:19,security,updat,updates,19,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:265,security,updat,updates,265,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:205,testability,regress,regress,205,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/722:260,usability,tip,tips,260,"Hi @LuckyMD ,. Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722
https://github.com/scverse/scanpy/issues/723:55,integrability,topic,topics,55,I think there was some discussion of this (among other topics) here: https://github.com/theislab/scanpy/issues/562,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:306,deployability,updat,update,306,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:346,deployability,log,logreg,346,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:306,safety,updat,update,306,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:317,safety,test,test,317,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:346,safety,log,logreg,346,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:306,security,updat,update,306,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:346,security,log,logreg,346,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:209,testability,simpl,simplifying,209,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:317,testability,test,test,317,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:346,testability,log,logreg,346,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:56,usability,help,helper,56,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:209,usability,simpl,simplifying,209,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased. 2. replace lists `rankings_gene_...` by DataFrame. 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test. 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:61,availability,avail,available,61,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:411,availability,consist,consistent,411,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:344,deployability,log,logfoldchanges,344,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:446,deployability,log,logreg,446,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:470,deployability,log,logfoldchanges,470,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:61,reliability,availab,available,61,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:454,reliability,doe,does,454,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:61,safety,avail,available,61,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:344,safety,log,logfoldchanges,344,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:446,safety,log,logreg,446,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:470,safety,log,logfoldchanges,470,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:61,security,availab,available,61,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:344,security,log,logfoldchanges,344,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:446,security,log,logreg,446,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:470,security,log,logfoldchanges,470,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:344,testability,log,logfoldchanges,344,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:446,testability,log,logreg,446,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:470,testability,log,logfoldchanges,470,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:411,usability,consist,consistent,411,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/issues/723:1126,usability,user,user-images,1126,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON. def rank_genes_groups_df(adata, key='rank_genes_groups'):. # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, . # logfoldchanges, pvals). . # Ideally, the list of columns should be consistent between methods. # but 'logreg' does not return logfoldchanges for example. dd = []. groupby = adata.uns['rank_genes_groups']['params']['groupby']. for group in adata.obs[groupby].cat.categories:. cols = []. # inner loop to make data frame by concatenating the columns per group. for col in adata.uns[key].keys():. if col != 'params':. cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])). . df = pd.concat(cols,axis=1). df['group'] = group. dd.append(df). # concatenate the individual group data frames into one long data frame. rgg = pd.concat(dd). rgg['group'] = rgg['group'].astype('category'). return rgg.set_index('group'). ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723
https://github.com/scverse/scanpy/pull/724:4,safety,test,test,4,"The test takes a while to run. I'm not sure how much of that is this being the first to import UMAP, but I'll look into it before merging.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:4,testability,test,test,4,"The test takes a while to run. I'm not sure how much of that is this being the first to import UMAP, but I'll look into it before merging.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:0,energy efficiency,Reduc,Reducing,0,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:59,energy efficiency,reduc,reduced,59,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:73,performance,time,time,73,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:24,availability,slo,slow,24,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:97,availability,sla,slaying,97,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:59,performance,time,times,59,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:24,reliability,slo,slow,24,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:97,reliability,sla,slaying,97,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:54,safety,test,test,54,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:54,testability,test,test,54,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:129,energy efficiency,model,model,129,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:119,performance,execution model,execution model,119,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:148,reliability,doe,does,148,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:195,safety,test,tests,195,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:129,security,model,model,129,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:195,testability,test,tests,195,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:48,energy efficiency,model,model,48,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:144,interoperability,plug,plugin,144,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:38,performance,execution model,execution model,38,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:184,performance,parallel,parallel,184,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:67,reliability,doe,does,67,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:114,safety,test,tests,114,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:48,security,model,model,48,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/pull/724:114,testability,test,tests,114,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests? Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724
https://github.com/scverse/scanpy/issues/727:269,deployability,version,version,269,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:269,integrability,version,version,269,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:269,modifiability,version,version,269,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:84,usability,guid,guides,84,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:291,usability,help,helpful,291,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:263,availability,error,error,263,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:673,deployability,version,version,673,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:673,integrability,version,version,673,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:673,modifiability,version,version,673,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:263,performance,error,error,263,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:253,safety,test,test,253,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:263,safety,error,error,263,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:28,testability,Assert,AssertionError,28,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:253,testability,test,test,253,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:263,usability,error,error,263,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```. Group,Group1,Group1,Group3,Group6,Group5. Gene1,11,0,0,14,0. Gene2,12,17,9,34,11. Gene3,0,0,0,0,2. ```. so, u can test this error locally by:. ```python. df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:704,availability,error,error,704,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python. from io import StringIO. import pandas as pd. import numpy as np. import scanpy as sc. csv = """""". Group,Group1,Group1,Group3,Group6,Group5 . Gene1,11,0,0,14,0 . Gene2,12,17,9,34,11 . Gene3,0,0,0,0,2. """""". df = pd.read_csv(StringIO(csv), index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:704,performance,error,error,704,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python. from io import StringIO. import pandas as pd. import numpy as np. import scanpy as sc. csv = """""". Group,Group1,Group1,Group3,Group6,Group5 . Gene1,11,0,0,14,0 . Gene2,12,17,9,34,11 . Gene3,0,0,0,0,2. """""". df = pd.read_csv(StringIO(csv), index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:704,safety,error,error,704,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python. from io import StringIO. import pandas as pd. import numpy as np. import scanpy as sc. csv = """""". Group,Group1,Group1,Group3,Group6,Group5 . Gene1,11,0,0,14,0 . Gene2,12,17,9,34,11 . Gene3,0,0,0,0,2. """""". df = pd.read_csv(StringIO(csv), index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:743,testability,trace,traceback,743,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python. from io import StringIO. import pandas as pd. import numpy as np. import scanpy as sc. csv = """""". Group,Group1,Group1,Group3,Group6,Group5 . Gene1,11,0,0,14,0 . Gene2,12,17,9,34,11 . Gene3,0,0,0,0,2. """""". df = pd.read_csv(StringIO(csv), index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:704,usability,error,error,704,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python. from io import StringIO. import pandas as pd. import numpy as np. import scanpy as sc. csv = """""". Group,Group1,Group1,Group3,Group6,Group5 . Gene1,11,0,0,14,0 . Gene2,12,17,9,34,11 . Gene3,0,0,0,0,2. """""". df = pd.read_csv(StringIO(csv), index_col=0). genes = df.index.values. barcodes = df.columns. adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)). adata.var_names_make_unique(). sc.pp.filter_genes(adata, min_cells=1). adata.raw = adata. sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4). ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:21,deployability,updat,update,21,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:21,safety,updat,update,21,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:21,security,updat,update,21,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:20,availability,error,error,20,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:154,availability,error,error,154,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:217,availability,error,error,217,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:60,deployability,version,versions,60,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:60,integrability,version,versions,60,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:60,modifiability,version,versions,60,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:20,performance,error,error,20,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:154,performance,error,error,154,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:217,performance,error,error,217,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:20,safety,error,error,20,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:154,safety,error,error,154,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:217,safety,error,error,217,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:20,usability,error,error,20,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:154,usability,error,error,154,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/727:217,usability,error,error,217,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727
https://github.com/scverse/scanpy/issues/728:227,deployability,version,version,227,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:103,energy efficiency,load,load,103,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:227,integrability,version,version,227,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:227,modifiability,version,version,227,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:103,performance,load,load,103,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:139,usability,command,command,139,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:177,usability,indicat,indicates,177,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:178,availability,cluster,cluster,178,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:291,availability,cluster,cluster,291,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:498,availability,cluster,cluster,498,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:669,availability,cluster,cluster,669,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:828,availability,cluster,cluster,828,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1012,availability,cluster,cluster,1012,"imple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1168,availability,cluster,cluster,1168,"ile ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1327,availability,cluster,cluster,1327,"r/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1496,availability,cluster,cluster,1496,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1679,availability,cluster,cluster,1679,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1838,availability,cluster,cluster,1838,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1988,availability,cluster,cluster,1988,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2138,availability,cluster,cluster,2138,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:178,deployability,cluster,cluster,178,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:248,deployability,modul,module,248,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:291,deployability,cluster,cluster,291,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:480,deployability,log,logFname,480,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:498,deployability,cluster,cluster,498,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:669,deployability,cluster,cluster,669,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:828,deployability,cluster,cluster,828,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1012,deployability,cluster,cluster,1012,"imple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1168,deployability,cluster,cluster,1168,"ile ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1327,deployability,cluster,cluster,1327,"r/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1496,deployability,cluster,cluster,1496,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1679,deployability,cluster,cluster,1679,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1838,deployability,cluster,cluster,1838,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1988,deployability,cluster,cluster,1988,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2138,deployability,cluster,cluster,2138,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:742,energy efficiency,core,core,742,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:901,energy efficiency,core,core,901,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1085,energy efficiency,core,core,1085,"data[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1241,energy efficiency,core,core,1241," in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1400,energy efficiency,core,core,1400,"data, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1569,energy efficiency,core,core,1569,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1752,energy efficiency,core,core,1752,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1910,energy efficiency,core,core,1910,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2060,energy efficiency,core,core,2060,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2210,energy efficiency,core,core,2210,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:248,modifiability,modul,module,248,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:725,modifiability,pac,packages,725,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:884,modifiability,pac,packages,884,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1068,modifiability,pac,packages,1068," ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1224,modifiability,pac,packages,1224,"anpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1383,modifiability,pac,packages,1383," cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but correspond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1552,modifiability,pac,packages,1552,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1735,modifiability,pac,packages,1735,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1894,modifiability,pac,packages,1894,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2044,modifiability,pac,packages,2044,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2194,modifiability,pac,packages,2194,"anpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 290. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:22,reliability,doe,doesn,22,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:248,safety,modul,module,248,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:480,safety,log,logFname,480,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:480,security,log,logFname,480,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:15,testability,simpl,simple,15,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:135,testability,Trace,Traceback,135,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:480,testability,log,logFname,480,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:15,usability,simpl,simple,15,"Even something simple doesn't work anymore, without going through h5ad:. ```. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. Traceback (most recent call last):. File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>. cellbrowser.cbScanpyCli(). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli. adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname). File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy. adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:102,availability,operat,operations,102,"I wonder if this has to do with the view discussed in #699. . The weird thing is these are very basic operations and I imagine this has come up before for someone else... Anyhow, I'm closing this, #699 gave me the idea that this is just a very recent problem, it works fine with scanpy 1.4.1, I guess this is already on your radar via #699",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:85,usability,behavi,behaviour,85,"This is separate from that, what's happening is that `_get_obs_array` had a change a behaviour during a bug fix. What we should do is . 1. Allow use_raw to be passed while referring to a column of obs in the deprecated method. 2. Finish removing all usage of the deprecated method from scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:158,availability,operat,operation,158,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:251,availability,down,downgraded,251,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:297,availability,error,error,297,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:330,availability,down,downgrade,330,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:415,deployability,pipelin,pipeline,415,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:438,deployability,releas,release,438,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:415,integrability,pipelin,pipeline,415,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:406,interoperability,standard,standard,406,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:297,performance,error,error,297,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:73,reliability,doe,doesn,73,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:297,safety,error,error,297,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:390,safety,test,testing,390,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:31,testability,simpl,simple,31,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:390,testability,test,testing,390,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:31,usability,simpl,simple,31,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:297,usability,error,error,297,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script. doesn't work. I'm getting the well-known ""TypeError: Categorical is not. ordered for operation max. you can use .as_ordered() to change the Categorical to an ordered one"". So. I downgraded anndata, which lead to another new error. I guess I'd also. have to downgrade pandas now. This makes me wonder if there is some testing. with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,availability,error,error,20,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,performance,error,error,20,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,safety,error,error,20,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:238,safety,test,tests,238,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:233,testability,unit,unit,233,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:238,testability,test,tests,238,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:271,testability,coverag,coverage,271,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:323,testability,coverag,coverage,323,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,usability,error,error,20,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:251,usability,clear,clearly,251,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:156,deployability,releas,releases,156,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:68,safety,test,tests,68,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:68,testability,test,tests,68,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,availability,error,error,20,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:329,availability,error,error,329,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:144,deployability,updat,update,144,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:295,deployability,observ,observation,295,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:65,interoperability,coordinat,coordinates,65,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:307,modifiability,variab,variable,307,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,performance,error,error,20,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:329,performance,error,error,329,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,safety,error,error,20,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:144,safety,updat,update,144,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:289,safety,valid,valid,289,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:329,safety,error,error,329,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:144,security,updat,update,144,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:295,testability,observ,observation,295,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,usability,error,error,20,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:329,usability,error,error,329,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`. Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:. `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:63,modifiability,layer,layer,63,"Oh, I also get `DeprecationWarning` and `FutureWarning` about `layer='X'` being removed in future and `obs_vector` being used, while I assume these are just used in `sc.pl.scatter` in the background. I guess this is in the process of being fixed though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1369,availability,cluster,cluster,1369,"200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/clust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1548,availability,cluster,cluster,1548,"st1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1715,availability,slo,slobj,1715,"not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/ho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1731,availability,cluster,cluster,1731,"ke them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1870,availability,sli,slicer,1870," names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluste",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2209,availability,cluster,cluster,2209,"_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2368,availability,cluster,cluster,2368,"ster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2552,availability,cluster,cluster,2552,"home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2708,availability,cluster,cluster,2708,"ce(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2867,availability,cluster,cluster,2867,"licer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3036,availability,cluster,cluster,3036,"e exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any othe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3219,availability,cluster,cluster,3219,"ax/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3378,availability,cluster,cluster,3378,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3528,availability,cluster,cluster,3528,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3678,availability,cluster,cluster,3678,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:15,deployability,updat,updated,15,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:142,deployability,log,logging,142,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:696,deployability,Observ,Observation,696,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1369,deployability,cluster,cluster,1369,"200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/clust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1548,deployability,cluster,cluster,1548,"st1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1635,deployability,manag,managers,1635,"learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1731,deployability,cluster,cluster,1731,"ke them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2147,deployability,modul,module,2147,"th n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2209,deployability,cluster,cluster,2209,"_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2368,deployability,cluster,cluster,2368,"ster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2552,deployability,cluster,cluster,2552,"home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2708,deployability,cluster,cluster,2708,"ce(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2867,deployability,cluster,cluster,2867,"licer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3036,deployability,cluster,cluster,3036,"e exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any othe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3219,deployability,cluster,cluster,3219,"ax/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3378,deployability,cluster,cluster,3378,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3528,deployability,cluster,cluster,3528,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3678,deployability,cluster,cluster,3678,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1441,energy efficiency,core,core,1441,"nt(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1620,energy efficiency,core,core,1620,"s==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/annda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1635,energy efficiency,manag,managers,1635,"learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1803,energy efficiency,core,core,1803,"unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2282,energy efficiency,core,core,2282," × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2441,energy efficiency,core,core,2441,"re/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2625,energy efficiency,core,core,2625,"ternals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2781,energy efficiency,core,core,2781,"ite-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2940,energy efficiency,core,core,2940,"ension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dim",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3109,energy efficiency,core,core,3109,"ast):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3292,energy efficiency,core,core,3292,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3450,energy efficiency,core,core,3450,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3600,energy efficiency,core,core,3600,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3750,energy efficiency,core,core,3750,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:4086,interoperability,format,format,4086,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:782,modifiability,Variab,Variable,782,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:865,modifiability,Variab,Variable,865,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:948,modifiability,Variab,Variable,948,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1425,modifiability,pac,packages,1425," 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1604,modifiability,pac,packages,1604,"y==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1787,modifiability,pac,packages,1787,"names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/annda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2147,modifiability,modul,module,2147,"th n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2265,modifiability,pac,packages,2265,"ject with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2424,modifiability,pac,packages,2424,"kages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2608,modifiability,pac,packages,2608,"/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2764,modifiability,pac,packages,2764,"lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2923,modifiability,pac,packages,2923,"array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but correspond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3092,modifiability,pac,packages,3092,"t recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3275,modifiability,pac,packages,3275,"ta/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3434,modifiability,pac,packages,3434,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3584,modifiability,pac,packages,3584,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3734,modifiability,pac,packages,3734,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:4180,performance,content,content,4180,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1715,reliability,slo,slobj,1715,"not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/ho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1870,reliability,sli,slicer,1870," names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluste",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:3974,reliability,doe,does,3974,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:4015,reliability,doe,does,4015,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:4154,reliability,doe,does,4154,"/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize_indices. obs = _normalize_index(obs, self._adata.obs_names). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index. positions = positions[index]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__. return self._get_with(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 946, in _get_with. return self._get_values(key). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 980, in _get_values. return self._values[indexer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. ```. The problem does not appear with the pbmc3k data. It does appear with any other expression matrix, as long as it is in text format. I noted that the adata object when reading from a text file does not have a real .var content, the .var is a dataframe with just an index. But I have no idea if this is related to the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:15,safety,updat,updated,15,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:101,safety,test,test,101,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:142,safety,log,logging,142,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1635,safety,manag,managers,1635,"learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2042,safety,except,exception,2042,"ith n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/hom",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2061,safety,except,exception,2061,"= 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/en",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2124,safety,test,test,2124,"0498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"",",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2147,safety,modul,module,2147,"th n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 363, in _normalize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:15,security,updat,updated,15,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:142,security,log,logging,142,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:101,testability,test,test,101,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:142,testability,log,logging,142,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:696,testability,Observ,Observation,696,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1326,testability,Trace,Traceback,1326,"ata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. retu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2082,testability,Trace,Traceback,2082,"ta object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2124,testability,test,test,2124,"0498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""test.py"", line 14, in <module>. adata = adata[adata.obs['n_genes'] > 100, :]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. return self._getitem_view(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. return AnnData(self, oidx=oidx, vidx=vidx, asview=True). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__. self._init_as_view(X, oidx, vidx). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. self._raw = adata_ref.raw[oidx]. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__. oidx, vidx = self._normalize_indices(index). File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"",",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:93,usability,minim,minimal,93,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:639,usability,learn,learn,639,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```. import scanpy as sc. sc.logging.print_versions(). #adata = sc.datasets.pbmc3k(). adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""). print(adata). adata = adata.T. print(adata). adata.raw = adata. print(adata). sc.pp.filter_cells(adata, min_genes=200). print(adata). adata = adata[adata.obs['n_genes'] < 5000, :]. print(adata). adata = adata[adata.obs['n_genes'] > 100, :]. print(adata). ```. output is:. ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 . Observation names are not unique. To make them unique, call `.obs_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs × n_vars = 60498 × 466 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . AnnData object with n_obs × n_vars = 466 × 60498 . obs: 'n_genes'. View of AnnData object with n_obs × n_vars = 311 × 60498 . obs: 'n_genes'. Traceback (most recent call last):. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values. return self._constructor(self._data.get_slice(indexer),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice. return self.__class__(self._block._slice(slobj),. File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice. return self.values[slicer]. IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:30,reliability,doe,does,30,One more thing: the exception does not happen if I comment out the line:. adata.raw = adata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:20,safety,except,exception,20,One more thing: the exception does not happen if I comment out the line:. adata.raw = adata.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:158,availability,slo,slow,158,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:163,availability,down,down,163,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:413,availability,sli,slicing,413,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:316,deployability,pipelin,pipeline,316,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:316,integrability,pipelin,pipeline,316,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:158,reliability,slo,slow,158,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:413,reliability,sli,slicing,413,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:112,safety,compl,complex,112,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:112,security,compl,complex,112,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:27,testability,understand,understand,27,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:194,usability,user,users,194,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:34,availability,replic,replicate,34,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:238,availability,sla,slated,238,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:637,availability,state,statement,637,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:251,deployability,releas,release,251,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:544,integrability,sub,subset,544,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:626,integrability,sub,subsetting,626,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:637,integrability,state,statement,637,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:238,reliability,sla,slated,238,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:370,usability,interact,interactively,370,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:125,availability,error,error,125,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:461,availability,operat,operations,461,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:505,availability,slo,slow,505,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1216,availability,replic,replicate,1216,"The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1452,availability,sla,slated,1452,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1871,availability,state,statement,1871,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1951,availability,state,state,1951,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:293,deployability,pipelin,pipeline,293,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1468,deployability,releas,release,1468,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:131,integrability,messag,message,131,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:293,integrability,pipelin,pipeline,293,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1774,integrability,sub,subset,1774,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1860,integrability,sub,subsetting,1860,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1871,integrability,state,statement,1871,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1951,integrability,state,state,1951,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:131,interoperability,messag,message,131,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:125,performance,error,error,125,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:505,reliability,slo,slow,505,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:522,reliability,pra,practical,522,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1452,reliability,sla,slated,1452,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:125,safety,error,error,125,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:731,safety,compl,complicated,731,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:731,security,compl,complicated,731,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1927,security,modif,modified,1927,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:2305,security,auth,auth,2305,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:254,testability,simpl,simply,254,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:125,usability,error,error,125,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:254,usability,simpl,simply,254,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:762,usability,user,user,762,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:986,usability,interact,interactively,986,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was. opened after I opened this one. I did search for the error message before I. opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a. pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1590,usability,interact,interactively,1590,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/issues/728:1945,usability,close,close,1945,"rtant for file backed data, I just cannot see a. use case for file backed mode either. Any useful operations on file backed. data will be too slow anyways for practical use, and anyone can get a. high-RAM machine these days on Amazon for a few hours, so I've always. wondered file backed mode exists. (sidenote: File backed data is again a. feature that sounds rather complicated to implement. As a user I love. libraries that are small, stable and don't change a lot, especially for. very foundational things like anndata. I guess it's a matter of development. philosophy here). Also, yes, it's because I don't use scanpy interactively. that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy! On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>. wrote:. > I've just spent a while trying to replicate, before realizing I've seen. > this issue before over on AnnData (theislab/anndata#182. > <https://github.com/theislab/anndata/issues/182>). I've got some good and. > bad news about this. It's fixed on master, but that fix is slated to be. > release in v0.7, which has intentionally breaking changes. >. > I find views very useful when dealing with large datasets interactively. > They're also important for file backed data, since copies are extremely. > expensive in that case. >. > Unlike numpy, AnnData objects should always return a view when subset. If. > you'd like to get copies, you could add a .copy() to the end of your. > subsetting statement. >. > —. > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728
https://github.com/scverse/scanpy/pull/730:74,deployability,version,version,74,"Great! Don't we want to add `anndata v0.6.22` to the requirements and the version check at startup in `scanpy/__init__.py`? At least we should make sure anndata is `0.6.21` as otherwise `.obs_vector` doesn't exist (https://anndata.readthedocs.io/en/stable/#post-v0-6-june-6-2019). Otherwise, happy to make Scanpy `v1.4.4`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:74,integrability,version,version,74,"Great! Don't we want to add `anndata v0.6.22` to the requirements and the version check at startup in `scanpy/__init__.py`? At least we should make sure anndata is `0.6.21` as otherwise `.obs_vector` doesn't exist (https://anndata.readthedocs.io/en/stable/#post-v0-6-june-6-2019). Otherwise, happy to make Scanpy `v1.4.4`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:74,modifiability,version,version,74,"Great! Don't we want to add `anndata v0.6.22` to the requirements and the version check at startup in `scanpy/__init__.py`? At least we should make sure anndata is `0.6.21` as otherwise `.obs_vector` doesn't exist (https://anndata.readthedocs.io/en/stable/#post-v0-6-june-6-2019). Otherwise, happy to make Scanpy `v1.4.4`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:200,reliability,doe,doesn,200,"Great! Don't we want to add `anndata v0.6.22` to the requirements and the version check at startup in `scanpy/__init__.py`? At least we should make sure anndata is `0.6.21` as otherwise `.obs_vector` doesn't exist (https://anndata.readthedocs.io/en/stable/#post-v0-6-june-6-2019). Otherwise, happy to make Scanpy `v1.4.4`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:96,deployability,releas,release,96,"Sure! I wansn't sure if there were other bugs to fix or PRs to merge before we wanted to make a release. I'd also like to get @fidelram to give this a look over. I think I didn't break anything, but he'd be in a better position to tell if that were the case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:83,modifiability,layer,layer,83,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:129,modifiability,layer,layer,129,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:171,modifiability,layer,layer,171,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:232,modifiability,layer,layer,232,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:225,safety,test,test,225,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:242,security,ident,identical,242,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:225,testability,test,test,225,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:270,usability,user,user-images,270,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:60,modifiability,layer,layers,60,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test? https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:249,safety,test,test,249,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test? https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:343,safety,test,tests,343,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test? https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:249,testability,test,test,249,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test? https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:343,testability,test,tests,343,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test? https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:20,integrability,coupl,couple,20,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:20,modifiability,coupl,couple,20,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:27,safety,test,tests,27,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:54,safety,test,test,54,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:20,testability,coupl,couple,20,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:27,testability,test,tests,27,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:54,testability,test,test,54,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:143,availability,error,error,143,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:162,availability,toler,tolerance,162,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:784,deployability,log,logic,784,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:776,energy efficiency,current,current,776,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:1081,integrability,messag,message,1081,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:1081,interoperability,messag,message,1081,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:295,modifiability,layer,layer,295,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:444,modifiability,layer,layer,444,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:534,modifiability,layer,layer,534,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:660,modifiability,layer,layer,660,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:685,modifiability,layer,layer,685,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:706,modifiability,layer,layer,706,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:725,modifiability,layer,layer,725,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:143,performance,error,error,143,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:162,reliability,toleran,tolerance,162,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:66,safety,test,test,66,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:143,safety,error,error,143,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:232,safety,test,test,232,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:258,safety,avoid,avoid,258,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:679,safety,valid,valid,679,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:784,safety,log,logic,784,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:784,security,log,logic,784,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:66,testability,test,test,66,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:232,testability,test,test,232,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:784,testability,log,logic,784,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:143,usability,error,error,143,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:423,usability,document,documentation,423,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:1017,usability,user,users,1017,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```. layer : typing.Union[str, NoneType], optional (default: None). Name of the AnnData object layer that wants to be plotted. By default. adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name, then the layer is plotted. `layer`. takes precedence over `use_raw`. ``` . The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:12,safety,test,tested,12,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:77,safety,test,tests,77,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:12,testability,test,tested,12,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:77,testability,test,tests,77,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:528,deployability,releas,release,528,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:65,interoperability,specif,specified,65,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,modifiability,layer,layer,49,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:137,modifiability,layer,layer,137,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:316,usability,behavi,behavior,316,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:5,deployability,updat,updated,5,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:5,safety,updat,updated,5,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:5,security,updat,updated,5,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:87,usability,feedback,feedback,87,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,deployability,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,integrability,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,modifiability,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,safety,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/pull/730:49,testability,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730
https://github.com/scverse/scanpy/issues/731:208,availability,error,error,208,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:272,availability,error,error,272,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:139,deployability,updat,updating,139,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:162,deployability,releas,release,162,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:208,performance,error,error,208,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:272,performance,error,error,272,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:139,safety,updat,updating,139,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:208,safety,error,error,208,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:272,safety,error,error,272,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:139,security,updat,updating,139,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:208,usability,error,error,208,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:272,usability,error,error,272,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs? Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:31,deployability,upgrad,upgraded,31,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:256,deployability,upgrad,upgraded,256,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:265,deployability,version,version,265,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:265,integrability,version,version,265,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:31,modifiability,upgrad,upgraded,31,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:256,modifiability,upgrad,upgraded,256,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:265,modifiability,version,version,265,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:154,usability,learn,learn,154,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/731:28,deployability,releas,released,28,"Glad to hear it! We've just released `v1.4.4` which has this fix in it, so you can use that if you don't want to be on the development branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731
https://github.com/scverse/scanpy/issues/732:51,availability,cluster,clusters,51,"From your example it seems that you want to rename clusters 1,2 and 3, and maybe only plot those samples. If that is the case you can do:. ```PYTHON. sel = adata[adata.obs.louvain.isin(['1', '2', '3']).copy(). sel.obs.louvain.rename(columns={'1': 'SampleA', '2': 'SampleB', '3': 'SampleC'}). sc.pl.heatmap(sel, .......). ```. btw, your image may improve if you add `standard_scale='var'` to `sc.pl.dotplot`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:51,deployability,cluster,clusters,51,"From your example it seems that you want to rename clusters 1,2 and 3, and maybe only plot those samples. If that is the case you can do:. ```PYTHON. sel = adata[adata.obs.louvain.isin(['1', '2', '3']).copy(). sel.obs.louvain.rename(columns={'1': 'SampleA', '2': 'SampleB', '3': 'SampleC'}). sc.pl.heatmap(sel, .......). ```. btw, your image may improve if you add `standard_scale='var'` to `sc.pl.dotplot`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:298,energy efficiency,heat,heatmap,298,"From your example it seems that you want to rename clusters 1,2 and 3, and maybe only plot those samples. If that is the case you can do:. ```PYTHON. sel = adata[adata.obs.louvain.isin(['1', '2', '3']).copy(). sel.obs.louvain.rename(columns={'1': 'SampleA', '2': 'SampleB', '3': 'SampleC'}). sc.pl.heatmap(sel, .......). ```. btw, your image may improve if you add `standard_scale='var'` to `sc.pl.dotplot`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:146,availability,cluster,cluster,146,"Hi @fidelram,. Thanks for your prompt reply! Sorry I didn't make my question clear. . I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:146,deployability,cluster,cluster,146,"Hi @fidelram,. Thanks for your prompt reply! Sorry I didn't make my question clear. . I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:362,energy efficiency,heat,heatmap,362,"Hi @fidelram,. Thanks for your prompt reply! Sorry I didn't make my question clear. . I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:77,usability,clear,clear,77,"Hi @fidelram,. Thanks for your prompt reply! Sorry I didn't make my question clear. . I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:223,usability,user,user-images,223,"Hi @fidelram,. Thanks for your prompt reply! Sorry I didn't make my question clear. . I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:37,availability,cluster,clusters,37,"for that you just need to rename the clusters using pandas rename:. ```PYTHON. adata.obs.louvain.rename(columns={'1': 'SampleA', '2': 'SampleB', '3': 'SampleC' ......}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/issues/732:37,deployability,cluster,clusters,37,"for that you just need to rename the clusters using pandas rename:. ```PYTHON. adata.obs.louvain.rename(columns={'1': 'SampleA', '2': 'SampleB', '3': 'SampleC' ......}). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732
https://github.com/scverse/scanpy/pull/733:306,integrability,protocol,protocol,306,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:582,integrability,transform,transformation,582,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:306,interoperability,protocol,protocol,306,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:582,interoperability,transform,transformation,582,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:72,reliability,doe,doesn,72,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:547,security,immut,immutable,547,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are? Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:322,energy efficiency,reduc,reduced,322,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:495,integrability,transform,transformations,495,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:664,integrability,topic,topic,664,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:784,integrability,sub,subsetting,784,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:495,interoperability,transform,transformations,495,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:604,interoperability,format,formats,604,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:39,reliability,doe,doesn,39,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:407,safety,test,test,407,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:407,testability,test,test,407,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:274,usability,support,support,274,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:646,usability,efficien,efficient,646,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:143,integrability,sub,subclass,143,"Looking at the view code again, I see a key pain point for working with generic arrays is how our views work. Right now, we have to make a new subclass for every array type we accept, so that we can take views of arbitrary indices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:44,availability,sli,slightly,44,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:5,deployability,manag,managed,5,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:193,deployability,fail,fails,193,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:5,energy efficiency,manag,managed,5,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:44,reliability,sli,slightly,44,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:193,reliability,fail,fails,193,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:5,safety,manag,managed,5,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:182,safety,test,test,182,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:53,security,hack,hacky,53,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:249,security,modif,modified,249,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:182,testability,test,test,182,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:501,usability,behavi,behaviour,501,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:78,interoperability,distribut,distributed,78,@aopisco This might be relevant to your questions about zarr arrays in backed/distributed mode.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:215,deployability,updat,updated,215,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:231,deployability,build,build,231,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:104,reliability,doe,doesn,104,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:215,safety,updat,updated,215,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/pull/733:215,security,updat,updated,215,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733
https://github.com/scverse/scanpy/issues/734:135,deployability,version,version,135,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning? Could you also try running this snippet with the same interpreter? ```python. from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):. parent: ""AnnData"". attrname: str. keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:135,integrability,version,version,135,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning? Could you also try running this snippet with the same interpreter? ```python. from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):. parent: ""AnnData"". attrname: str. keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:135,modifiability,version,version,135,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning? Could you also try running this snippet with the same interpreter? ```python. from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):. parent: ""AnnData"". attrname: str. keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:110,safety,test,tests,110,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning? Could you also try running this snippet with the same interpreter? ```python. from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):. parent: ""AnnData"". attrname: str. keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:110,testability,test,tests,110,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning? Could you also try running this snippet with the same interpreter? ```python. from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):. parent: ""AnnData"". attrname: str. keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:122,deployability,modul,module,122,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>. print(ViewArgs(None, ""a"")). TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:122,modifiability,modul,module,122,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>. print(ViewArgs(None, ""a"")). TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:99,safety,test,test,99,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>. print(ViewArgs(None, ""a"")). TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:122,safety,modul,module,122,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>. print(ViewArgs(None, ""a"")). TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:99,testability,test,test,99,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>. print(ViewArgs(None, ""a"")). TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:36,deployability,version,version,36,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:174,deployability,build,builds,174,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:207,deployability,upgrad,upgrade,207,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:36,integrability,version,version,36,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:36,modifiability,version,version,36,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:207,modifiability,upgrad,upgrade,207,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:51,availability,error,error,51,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:2,deployability,updat,updated,2,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:51,performance,error,error,51,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:2,safety,updat,updated,2,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:51,safety,error,error,51,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:2,security,updat,updated,2,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:51,usability,error,error,51,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/734:99,usability,help,help,99,I updated it to python 3.6.8 and it gets past that error point. . Thank you very much for all your help. Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734
https://github.com/scverse/scanpy/issues/735:120,deployability,version,version,120,The use of a dict for genes was recently introduced. If is not working for your this is most likely because of an older version of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/735
https://github.com/scverse/scanpy/issues/735:120,integrability,version,version,120,The use of a dict for genes was recently introduced. If is not working for your this is most likely because of an older version of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/735
https://github.com/scverse/scanpy/issues/735:120,modifiability,version,version,120,The use of a dict for genes was recently introduced. If is not working for your this is most likely because of an older version of scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/735
https://github.com/scverse/scanpy/issues/735:93,modifiability,paramet,parameter,93,"scanpy 1.4.6 also have this problem when using sc.pl.rank_genes_groups_heatmap plot with the parameter ""use_raw=False"" !!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/735
https://github.com/scverse/scanpy/issues/736:113,security,control,control,113,This should be related to `matplotlib`. What is happening is that text is being saved as paths. You can probably control this using `rcParams`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/736
https://github.com/scverse/scanpy/issues/736:113,testability,control,control,113,This should be related to `matplotlib`. What is happening is that text is being saved as paths. You can probably control this using `rcParams`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/736
https://github.com/scverse/scanpy/issues/737:0,deployability,Updat,Update,0,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:61,deployability,build,build,61,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:132,deployability,build,build,132,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:90,performance,cach,cacheing,90,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:0,safety,Updat,Update,0,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:0,security,Updat,Update,0,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:44,deployability,api,api,44,Should we provide documentation for `scanpy.api`? I think that could probably just go away,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:44,integrability,api,api,44,Should we provide documentation for `scanpy.api`? I think that could probably just go away,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:44,interoperability,api,api,44,Should we provide documentation for `scanpy.api`? I think that could probably just go away,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/737:18,usability,document,documentation,18,Should we provide documentation for `scanpy.api`? I think that could probably just go away,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737
https://github.com/scverse/scanpy/issues/738:450,availability,error,error,450,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:482,availability,error,error,482,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:570,availability,error,error,570,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:412,deployability,scale,scale,412,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:589,deployability,version,version,589,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:412,energy efficiency,scale,scale,412,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:344,integrability,sub,subset,344,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:589,integrability,version,version,589,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:733,integrability,sub,subset,733,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:412,modifiability,scal,scale,412,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:589,modifiability,version,version,589,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:412,performance,scale,scale,412,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:450,performance,error,error,450,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:482,performance,error,error,482,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:570,performance,error,error,570,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:450,safety,error,error,450,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:482,safety,error,error,482,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:570,safety,error,error,570,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:450,usability,error,error,450,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:482,usability,error,error,482,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/738:570,usability,error,error,570,"Hey! Using the latest verisons of scanpy and anndata, I have tried reproducing this via:. ```. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts = 10). sc.pp.filter_cells(adata, min_counts = 10). sc.pp.normalize_per_cell(adata). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True). adata = adata[:, adata.var[""highly_variable""]]. sc.pp.scale(adata). ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on? Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738
https://github.com/scverse/scanpy/issues/739:117,availability,error,error,117,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:84,deployability,version,version,84,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,deployability,version,version,218,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:233,deployability,version,version,233,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:491,deployability,instal,installed,491,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:686,deployability,version,version,686,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:823,deployability,version,version,823,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:937,deployability,Version,Version,937,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:84,integrability,version,version,84,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,integrability,version,version,218,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:233,integrability,version,version,233,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:686,integrability,version,version,686,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:823,integrability,version,version,823,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:937,integrability,Version,Version,937,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:805,interoperability,Distribut,Distribution,805,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:855,interoperability,Distribut,Distribution,855,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:84,modifiability,version,version,84,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,modifiability,version,version,218,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:233,modifiability,version,version,233,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:656,modifiability,pac,package,656,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:686,modifiability,version,version,686,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:823,modifiability,version,version,823,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:937,modifiability,Version,Version,937,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:954,modifiability,Pac,PackageNotFoundError,954,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:117,performance,error,error,117,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:526,reliability,doe,doesn,526,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:755,reliability,doe,does,755,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:117,safety,error,error,117,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:98,usability,learn,learn,98,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:117,usability,error,error,117,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:247,usability,learn,learn,247,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:476,usability,learn,learn,476,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.0. ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package. 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py. from importlib_metadata import Distribution. def version(name):. for resolver in Distribution._discover_resolvers():. for d in resolver(name):. return d.metadata['Version']. raise PackageNotFoundError(name). ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:230,deployability,observ,observations,230,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:294,deployability,version,version,294,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:309,deployability,version,version,309,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:399,deployability,modul,module,399,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:512,deployability,api,api,512,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:534,deployability,version,version,534,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:572,deployability,version,version,572,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:685,deployability,api,api,685,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:864,deployability,api,api,864,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:949,deployability,api,api,949,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,integrability,coupl,couple,218,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:294,integrability,version,version,294,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:309,integrability,version,version,309,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:512,integrability,api,api,512,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:534,integrability,version,version,534,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:572,integrability,version,version,572,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:685,integrability,api,api,685,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:864,integrability,api,api,864,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:949,integrability,api,api,949,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1006,integrability,coupl,couple,1006,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:512,interoperability,api,api,512,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:550,interoperability,distribut,distribution,550,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:685,interoperability,api,api,685,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:706,interoperability,distribut,distribution,706,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:727,interoperability,Distribut,Distribution,727,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:864,interoperability,api,api,864,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:949,interoperability,api,api,949,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1063,interoperability,Distribut,Distribution,1063,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,modifiability,coupl,couple,218,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:294,modifiability,version,version,294,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:309,modifiability,version,version,309,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:399,modifiability,modul,module,399,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:484,modifiability,pac,packages,484,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:534,modifiability,version,version,534,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:563,modifiability,pac,package,563,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:572,modifiability,version,version,572,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:657,modifiability,pac,packages,657,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:750,modifiability,pac,package,750,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:836,modifiability,pac,packages,836,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:902,modifiability,Pac,PackageNotFoundError,902,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:953,modifiability,Pac,PackageNotFoundError,953,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1006,modifiability,coupl,couple,1006,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:399,safety,modul,module,399,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:218,testability,coupl,couple,218,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:230,testability,observ,observations,230,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:334,testability,Trace,Traceback,334,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:1006,testability,coupl,couple,1006,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:323,usability,learn,learn,323,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:980,usability,learn,learn,980,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:. ```. $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version. return distribution(package).version. File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution. return Distribution.from_name(package). File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name. raise PackageNotFoundError(name). importlib_metadata.api.PackageNotFoundError: umap-learn. ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,deployability,version,version,31,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:89,deployability,version,version,89,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:260,deployability,version,version,260,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:281,deployability,version,version,281,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:338,deployability,instal,installed,338,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,integrability,version,version,31,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:89,integrability,version,version,89,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:260,integrability,version,version,260,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:281,integrability,version,version,281,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,modifiability,version,version,31,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:89,modifiability,version,version,89,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:260,modifiability,version,version,260,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:281,modifiability,version,version,281,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:425,modifiability,pac,package,425,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:304,reliability,doe,doesn,304,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:399,reliability,Doe,Does,399,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:481,reliability,doe,doesn,481,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:152,usability,learn,learn,152,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:295,usability,learn,learn,295,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do. ```. In [3]: import umap . In [4]: umap.__version__ . Out[4]: '0.3.9'. ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,deployability,version,version,31,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:68,deployability,version,version,68,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:165,deployability,version,version,165,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,integrability,version,version,31,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:68,integrability,version,version,68,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:165,integrability,version,version,165,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:31,modifiability,version,version,31,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:68,modifiability,version,version,68,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:165,modifiability,version,version,165,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:82,usability,learn,learn,82,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:121,deployability,version,version,121,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:144,deployability,releas,released,144,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:187,deployability,version,version,187,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:270,deployability,version,version,270,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:285,deployability,version,version,285,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:535,deployability,version,version,535,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:550,deployability,version,version,550,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:121,integrability,version,version,121,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:187,integrability,version,version,187,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:270,integrability,version,version,270,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:285,integrability,version,version,285,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:535,integrability,version,version,535,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:550,integrability,version,version,550,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:121,modifiability,version,version,121,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:187,modifiability,version,version,187,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:270,modifiability,version,version,270,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:285,modifiability,version,version,285,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:361,modifiability,pac,packages,361,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:405,modifiability,pac,packages,405,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:448,modifiability,pac,packages,448,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:535,modifiability,version,version,535,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:550,modifiability,version,version,550,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:564,usability,learn,learn,564,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console. $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'. 0.18. $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*. ~/.local/lib/python3.6/site-packages/umap. ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info. $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'. 0.3.9. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:28,deployability,version,version,28,I have `importlib_metadata` version 0.6. I would put that into the requirements then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:28,integrability,version,version,28,I have `importlib_metadata` version 0.6. I would put that into the requirements then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:28,modifiability,version,version,28,I have `importlib_metadata` version 0.6. I would put that into the requirements then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:62,deployability,updat,update,62,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:62,safety,updat,update,62,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:62,security,updat,update,62,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:88,deployability,version,version,88,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:164,deployability,updat,updating,164,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:88,integrability,version,version,88,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:88,modifiability,version,version,88,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:175,modifiability,pac,package,175,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:232,performance,time,time,232,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:164,safety,updat,updating,164,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:164,security,updat,updating,164,"By the way: I suggested right at the beginning that it’s possible…. > You have an older version of `importlib_metadata` with a bug or so. In the future, please try updating a package that might have a problem, then we save a lot of time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:41,deployability,version,version,41,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:92,deployability,updat,update,92,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:41,integrability,version,version,41,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:41,modifiability,version,version,41,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:112,modifiability,pac,packages,112,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:126,performance,time,time,126,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:92,safety,updat,update,92,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/739:92,security,updat,update,92,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739
https://github.com/scverse/scanpy/issues/740:335,safety,safe,safe,335,"> best palette out there. That’s quite the generalization. | Pro | Con |. | --- | --- |. | Many colors | Ugly colors, no rhyme or reason |. | Acceptably distinguishable | Dark colors hard to distinguish on white bg, and light ones hard to distinguish on black bg |. | | There’s always colors that are hard to see on any kind of bg (no safe bg color) |. | | Not colorblind friendly |. So I’d recommend against it whenever you can avoid it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:429,safety,avoid,avoid,429,"> best palette out there. That’s quite the generalization. | Pro | Con |. | --- | --- |. | Many colors | Ugly colors, no rhyme or reason |. | Acceptably distinguishable | Dark colors hard to distinguish on white bg, and light ones hard to distinguish on black bg |. | | There’s always colors that are hard to see on any kind of bg (no safe bg color) |. | | Not colorblind friendly |. So I’d recommend against it whenever you can avoid it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:179,usability,user,user-images,179,"I was looking at this and thought ""wow, that a way better color palette than our default"", since I always end up with legends like this:. <img width=""58"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/61576234-cb4f9300-ab1a-11e9-9fd5-5ec094e894a9.png"">. I agree the white is a problem, but this seems fixable. @flying-sheep are there colorblind friendly palettes with a large number (at least 20) of distinguishable colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,availability,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:140,availability,cluster,clustered,140,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,deployability,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:140,deployability,cluster,clustered,140,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,energy efficiency,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:14,performance,time,time,14,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,reliability,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,safety,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:48,testability,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,availability,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,deployability,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,energy efficiency,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:290,energy efficiency,cloud,clouds,290,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:16,performance,time,time,16,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,reliability,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,safety,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:50,testability,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:292,availability,sli,slightly,292,"> I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. That should be easy. Only if alpha=1 we plot all points in black and slightly bigger before we plot them in their colors. > The `default_20` palette was the great idea! Thank you! I think that was a collaborative effort between Alex and me, since we needed a bigger palette and didn’t like vega20 to alternate between saturated and pale colors, so we put the saturated (i.e. vega10) first. I then made the problematic colors more colorblind friendly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:205,energy efficiency,cloud,clouds,205,"> I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. That should be easy. Only if alpha=1 we plot all points in black and slightly bigger before we plot them in their colors. > The `default_20` palette was the great idea! Thank you! I think that was a collaborative effort between Alex and me, since we needed a bigger palette and didn’t like vega20 to alternate between saturated and pale colors, so we put the saturated (i.e. vega10) first. I then made the problematic colors more colorblind friendly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:292,reliability,sli,slightly,292,"> I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. That should be easy. Only if alpha=1 we plot all points in black and slightly bigger before we plot them in their colors. > The `default_20` palette was the great idea! Thank you! I think that was a collaborative effort between Alex and me, since we needed a bigger palette and didn’t like vega20 to alternate between saturated and pale colors, so we put the saturated (i.e. vega10) first. I then made the problematic colors more colorblind friendly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:47,integrability,interfac,interface,47,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:47,interoperability,interfac,interface,47,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:47,modifiability,interfac,interface,47,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/740:152,performance,time,time,152,I think we can remove the color as a quick solution and keep discussing other options here like dot borders etc. This color is annoying me too for long time now :D,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740
https://github.com/scverse/scanpy/issues/746:33,deployability,log,logging,33,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:88,deployability,log,logging,88,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:155,deployability,updat,updating,155,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:113,performance,time,times,113,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:249,performance,time,time,249,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:96,reliability,doe,does,96,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:33,safety,log,logging,33,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:88,safety,log,logging,88,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:155,safety,updat,updating,155,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:33,security,log,logging,33,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:88,security,log,logging,88,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:155,security,updat,updating,155,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:33,testability,log,logging,33,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:88,testability,log,logging,88,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:7,deployability,log,logging,7,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:343,deployability,updat,updating,343,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:472,deployability,modul,module,472,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:490,deployability,updat,update,490,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:508,deployability,updat,updating,508,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:472,modifiability,modul,module,472,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:161,performance,time,timedelta,161,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:435,performance,time,time,435,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:7,safety,log,logging,7,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:343,safety,updat,updating,343,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:472,safety,modul,module,472,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:490,safety,updat,update,490,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:508,safety,updat,updating,508,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:7,security,log,logging,7,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:343,security,updat,updating,343,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:490,security,updat,update,490,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:508,security,updat,updating,508,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:7,testability,log,logging,7,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:245,deployability,version,version-wise,245,"Check `In[20]` [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), it still shows the fractions. This may no longer be accurate, but the public-facing tutorial still shows it. EDIT: apparently the tutorial's quite antiquated, version-wise, and was generated on 1.3.7 scanpy. Disregard this sentiment then. So yeah, stdlib or not, is there some way to overcome this and have the deep come online that you know of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:156,integrability,pub,public-facing,156,"Check `In[20]` [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), it still shows the fractions. This may no longer be accurate, but the public-facing tutorial still shows it. EDIT: apparently the tutorial's quite antiquated, version-wise, and was generated on 1.3.7 scanpy. Disregard this sentiment then. So yeah, stdlib or not, is there some way to overcome this and have the deep come online that you know of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:245,integrability,version,version-wise,245,"Check `In[20]` [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), it still shows the fractions. This may no longer be accurate, but the public-facing tutorial still shows it. EDIT: apparently the tutorial's quite antiquated, version-wise, and was generated on 1.3.7 scanpy. Disregard this sentiment then. So yeah, stdlib or not, is there some way to overcome this and have the deep come online that you know of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:245,modifiability,version,version-wise,245,"Check `In[20]` [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), it still shows the fractions. This may no longer be accurate, but the public-facing tutorial still shows it. EDIT: apparently the tutorial's quite antiquated, version-wise, and was generated on 1.3.7 scanpy. Disregard this sentiment then. So yeah, stdlib or not, is there some way to overcome this and have the deep come online that you know of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:67,deployability,releas,released,67,"Hi, sorry! deep was nonfunctional, I fixed it in 1.4.4.post1 (just released)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:25,availability,operat,operational,25,"Can confirm everything's operational now, so all's well that ends well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/746:4,usability,confirm,confirm,4,"Can confirm everything's operational now, so all's well that ends well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746
https://github.com/scverse/scanpy/issues/747:65,availability,error,error,65,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:393,deployability,updat,update,393,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:414,deployability,releas,releases,414,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:65,performance,error,error,65,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:65,safety,error,error,65,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:393,safety,updat,update,393,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:393,security,updat,update,393,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:65,usability,error,error,65,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.filter_genes(adata, min_counts=1). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes(adata). adata = adata[:, adata.var['highly_variable']]. ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:216,availability,error,error,216,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:555,deployability,updat,update,555,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:576,deployability,releas,releases,576,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:216,performance,error,error,216,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:216,safety,error,error,216,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:555,safety,updat,update,555,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:555,security,updat,update,555,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:678,security,auth,authored,678,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:1046,security,auth,auth,1046,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:216,usability,error,error,216,"Hi,. I converted the obs and values in string and it worked. Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>. wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error. > Here's what I ran:. >. > import scanpy as sc. >. > adata = sc.datasets.pbmc3k(). > sc.pp.filter_genes(adata, min_counts=1). > sc.pp.log1p(adata). > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5). > sc.pl.highly_variable_genes(adata). > adata = adata[:, adata.var['highly_variable']]. >. > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22). > and try that? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:100,safety,prevent,prevent,100,"That'll do it 😄. Do you know how you ended up with non-string indices? Ideally, we would be able to prevent that from happening or at least warn the user about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:100,security,preven,prevent,100,"That'll do it 😄. Do you know how you ended up with non-string indices? Ideally, we would be able to prevent that from happening or at least warn the user about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:149,usability,user,user,149,"That'll do it 😄. Do you know how you ended up with non-string indices? Ideally, we would be able to prevent that from happening or at least warn the user about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:85,energy efficiency,load,load,85,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:121,interoperability,convers,conversion,121,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:85,performance,load,load,85,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:362,safety,prevent,prevent,362,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:362,security,preven,prevent,362,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:471,security,auth,authored,471,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:839,security,auth,auth,839,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:411,usability,user,user,411,"I created an adata without using the functions provided by scanpy that. allow you to load single cell data. This kind of conversion is done is done. in that functions, right? On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄. >. > Do you know how you ended up with non-string indices? Ideally, we would be. > able to prevent that from happening or at least warn the user about it. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:30,energy efficiency,load,loaded,30,I had the same problem when I loaded sample data from a csv as a data frame. and assigned it to adata.obs = df. >.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:30,performance,load,loaded,30,I had the same problem when I loaded sample data from a csv as a data frame. and assigned it to adata.obs = df. >.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:32,energy efficiency,load,loaded,32,> I had the same problem when I loaded sample data from a csv as a data frame and assigned it to adata.obs = df. > […](#). I meet the same problem when I try replace the adata.obs with annother pandas dataframe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:32,performance,load,loaded,32,> I had the same problem when I loaded sample data from a csv as a data frame and assigned it to adata.obs = df. > […](#). I meet the same problem when I try replace the adata.obs with annother pandas dataframe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:102,testability,assert,assertionerror,102,Solution here is `adata.obs.index = adata.obs.index.astype(str)`. Should be called by default if this assertionerror is raised.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:138,deployability,version,version,138,"This just happened to me, only when the matrix was in .mtx format. I fixed it with `adata.var.index = adata.var.index.astype(str)` Scanpy version was 1.4.5.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:138,integrability,version,version,138,"This just happened to me, only when the matrix was in .mtx format. I fixed it with `adata.var.index = adata.var.index.astype(str)` Scanpy version was 1.4.5.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:59,interoperability,format,format,59,"This just happened to me, only when the matrix was in .mtx format. I fixed it with `adata.var.index = adata.var.index.astype(str)` Scanpy version was 1.4.5.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:138,modifiability,version,version,138,"This just happened to me, only when the matrix was in .mtx format. I fixed it with `adata.var.index = adata.var.index.astype(str)` Scanpy version was 1.4.5.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/747:46,usability,close,close,46,"Thanks everyone for the discussion here! Will close the issue for now, as based on the provided information and the discussion so far, it seems that the issues have been addressed and hopefully resolved :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747
https://github.com/scverse/scanpy/issues/748:1374,availability,cluster,clustering,1374,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1443,availability,cluster,cluster,1443,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1374,deployability,cluster,clustering,1374,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1443,deployability,cluster,cluster,1443,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:131,integrability,sub,subset,131,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1018,integrability,sub,subsetting,1018,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1601,integrability,sub,subclustering,1601,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:68,modifiability,variab,variable,68,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:911,performance,perform,perform,911,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1317,reliability,doe,doesn,1317,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:84,safety,test,test,84,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:175,safety,test,testing,175,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:339,safety,test,testing,339,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:431,safety,test,test,431,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:540,safety,test,test,540,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:928,safety,test,testing,928,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1048,safety,test,testing,1048,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:84,testability,test,test,84,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:175,testability,test,testing,175,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:339,testability,test,testing,339,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:431,testability,test,test,431,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:540,testability,test,test,540,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:928,testability,test,testing,928,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:1048,testability,test,testing,1048,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:911,usability,perform,perform,911,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:. ```. CLUST_ID = 0. gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']. gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]. results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]. ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:326,availability,cluster,clustering,326,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:544,availability,cluster,clustering,544,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:143,deployability,Log,Logistic,143,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:326,deployability,cluster,clustering,326,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,deployability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:544,deployability,cluster,clustering,544,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:495,energy efficiency,predict,predict,495,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:127,integrability,sub,subsetting,127,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,integrability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:528,integrability,sub,subset,528,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,interoperability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,modifiability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:274,reliability,doe,doesn,274,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,reliability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:143,safety,Log,Logistic,143,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:182,safety,test,testing,182,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:484,safety,test,testing,484,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:495,safety,predict,predict,495,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:143,security,Log,Logistic,143,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,security,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:143,testability,Log,Logistic,143,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:152,testability,regress,regression,152,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:182,testability,test,testing,182,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:366,testability,integr,integrated,366,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:484,testability,test,testing,484,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:69,usability,help,helps,69,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:348,usability,tool,tools,348,"Hi, . Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:479,availability,cluster,cluster,479,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:569,availability,cluster,clusters,569,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:794,availability,cluster,clusters,794,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:5,deployability,Log,Logistic,5,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:479,deployability,cluster,cluster,479,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:569,deployability,cluster,clusters,569,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:794,deployability,cluster,clusters,794,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:25,energy efficiency,current,currently,25,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:336,energy efficiency,model,models,336,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:782,energy efficiency,predict,predict,782,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:144,integrability,sub,subsetting,144,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:118,modifiability,variab,variables,118,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:35,reliability,doe,doesn,35,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:962,reliability,doe,does,962,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:5,safety,Log,Logistic,5,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:328,safety,compl,complex,328,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:782,safety,predict,predict,782,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:5,security,Log,Logistic,5,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:328,security,compl,complex,328,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:336,security,model,models,336,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:5,testability,Log,Logistic,5,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:14,testability,regress,regression,14,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:934,usability,tool,tool,934,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:988,usability,clear,clearer,988,"Hey! Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:78,performance,time,time,78,"Hi,. I would like to use scanpy for differential gene expression between four-time points. So for each time point, I have 250 cells and 20000 genes and the gene expression matrix. I do not know how should I use annData to use scanpy to find differential gene expression between each time point. . Could you please help me with this problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:103,performance,time,time,103,"Hi,. I would like to use scanpy for differential gene expression between four-time points. So for each time point, I have 250 cells and 20000 genes and the gene expression matrix. I do not know how should I use annData to use scanpy to find differential gene expression between each time point. . Could you please help me with this problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:283,performance,time,time,283,"Hi,. I would like to use scanpy for differential gene expression between four-time points. So for each time point, I have 250 cells and 20000 genes and the gene expression matrix. I do not know how should I use annData to use scanpy to find differential gene expression between each time point. . Could you please help me with this problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/748:314,usability,help,help,314,"Hi,. I would like to use scanpy for differential gene expression between four-time points. So for each time point, I have 250 cells and 20000 genes and the gene expression matrix. I do not know how should I use annData to use scanpy to find differential gene expression between each time point. . Could you please help me with this problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748
https://github.com/scverse/scanpy/issues/749:82,availability,error,error,82,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:24,integrability,messag,message,24,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:88,integrability,messag,message,88,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:24,interoperability,messag,message,24,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:88,interoperability,messag,message,88,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:82,performance,error,error,82,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:14,safety,except,exception,14,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:82,safety,error,error,82,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:82,usability,error,error,82,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:84,availability,error,error,84,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:665,availability,error,error,665,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3207,availability,error,errors,3207,"size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:5661,availability,error,errors,5661,"n3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:1210,deployability,fail,failed,1210,"e is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_siz",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4523,deployability,fail,failed,4523,"p://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:6679,deployability,modul,module,6679,"onWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8213,deployability,log,logg,8213,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:1925,energy efficiency,CPU,CPUDispatcher,1925,"his, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8516,energy efficiency,core,core,8516,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8691,energy efficiency,core,core,8691,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:26,integrability,messag,message,26,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:90,integrability,messag,message,90,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:671,integrability,messag,message,671,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3413,integrability,transform,transformation,3413,"make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4009,integrability,transform,transformation,4009,"-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:26,interoperability,messag,message,26,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:90,interoperability,messag,message,90,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:671,interoperability,messag,message,671,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3396,interoperability,specif,specified,3396,"Warning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: Nu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3413,interoperability,transform,transformation,3413,"make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3992,interoperability,specif,specified,3992,"ject-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in obje",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4009,interoperability,transform,transformation,4009,"-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:5948,interoperability,format,format,5948,"on ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:6092,interoperability,format,format,6092,"umba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:1057,modifiability,pac,packages,1057,"an also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:1615,modifiability,pac,packages,1615,"olor='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_sta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:1676,modifiability,pac,packages,1676,"a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:2056,modifiability,pac,packages,2056,"kages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:2117,modifiability,pac,packages,2117,"ling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:2364,modifiability,pac,packages,2364,", array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:2521,modifiability,pac,packages,2521,"bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see ht",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:2696,modifiability,pac,packages,2696,""", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). de",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3072,modifiability,pac,packages,3072,"e.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3297,modifiability,pac,packages,3297,"leaf_size). ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3634,modifiability,pac,packages,3634," ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'funct",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:3892,modifiability,pac,packages,3892,"formation visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:. @numba.jit(). def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4230,modifiability,pac,packages,4230,"ing(msg, self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4371,modifiability,pac,packages,4371," 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:. @numba.njit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behavi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4674,modifiability,pac,packages,4674,"jit(parallel=True). def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):. ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state. /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDepre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:4899,modifiability,pac,packages,4899,"mba/compiler.py:588: NumbaPerformanceWarning: . The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:5057,modifiability,pac,packages,5057,"To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored ele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:5191,modifiability,pac,packages,5191,"help. File ""env/lib/python3.6/site-packages/umap/nndescent.py"", line 47:. @numba.njit(parallel=True). def nn_descent(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:5567,modifiability,pac,packages,5567,"e 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:. def fuzzy_simplicial_set(. <source elided>. if knn_indices is None or knn_dists is None:. knn_indices, knn_dists, _ = nearest_neighbors(. ^. @numba.jit(). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. self.func_ir.loc)). /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:6229,modifiability,paramet,parameters,6229,"ationWarning: . Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:. @numba.jit(). def fuzzy_simplicial_set(. ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:6679,modifiability,modul,module,6679,"onWarning(msg, self.func_ir.loc)). OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:6869,modifiability,pac,packages,6869," 'numpy.float64'>'. 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'. 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]). WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by split",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:7217,modifiability,pac,packages,7217,"default parameters. WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.war",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:7427,modifiability,pac,packages,7427,"tau correlation (suppress this with `allow_kendall_tau_shift=False`). WARNING: detected group with only [] cells. ```. </details>. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:7641,modifiability,pac,packages,7641,"all last). ~/diffusion_map.py in <module>. 57 adata.uns['iroot'] = 0. 58 print(adata.uns). ---> 59 sc.tl.dpt(adata, n_branchings=2). 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy). 128 # detect branchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8082,modifiability,pac,packages,8082,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8501,modifiability,pac,packages,8501,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:8676,modifiability,pac,packages,8676,"ranchings and partition the data into segments. 129 if n_branchings > 0:. --> 130 dpt.branchings_segments(). 131 adata.obs['dpt_groups'] = pd.Categorical(. 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self). 187 for each segment. 188 """""". --> 189 self.detect_branchings(). 190 self.postprocess_segments(). 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self). 262 segs_connects,. 263 segs_undecided,. --> 264 segs_adjacency, iseg, tips3). 265 # store as class members. 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3). 476 # branching on the segment, return the list ssegs of segments that. 477 # are defined by splitting this segment. --> 478 result = self._detect_branching(Dseg, tips3, seg). 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result. 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference). 646 if len(np.flatnonzero(newseg)) <= 1:. 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'). --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]. 649 ssegs_tips.append([tips[inewseg], secondtip]). 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out). 1101 . 1102 """""". -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out). 1104 . 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds). 54 def _wrapfunc(obj, method, *args, **kwds):. 55 try:. ---> 56 return getattr(obj, method)(*args, **kwds). 57 . 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:84,performance,error,error,84,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
https://github.com/scverse/scanpy/issues/749:665,performance,error,error,665,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply! Unfortunately , no visible exception... My code is as follows:. ```py. import velocyto as vcy. import numpy as np. import scanpy as sc. import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""). S = vlm.S. S=S.transpose(). adata = anndata.AnnData(S). print(adata.X). print(adata.obs). print(adata.var). sc.pp.neighbors(adata, n_neighbors=100). adata.uns['iroot'] = 0. print(adata.uns). sc.tl.dpt(adata, n_branchings=2). sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb. WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`. Falling back to preprocessing with `sc.pp.pca` and default params. /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: . Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:. def make_euclidean_tree(data, indices, rng_state, leaf_size=30):. <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size). ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))). [2] Duri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749
