id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1653:691,usability,tool,tools,691,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:910,usability,tool,tools,910,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:1066,usability,tool,tools,1066,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/pull/1654:13,safety,test,test,13,I've added a test in https://github.com/theislab/scanpy/pull/1654/commits/189354eb0074140e3f2204d09b02aaa912d13934,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654
https://github.com/scverse/scanpy/pull/1654:13,testability,test,test,13,I've added a test in https://github.com/theislab/scanpy/pull/1654/commits/189354eb0074140e3f2204d09b02aaa912d13934,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654
https://github.com/scverse/scanpy/pull/1656:46,safety,test,test,46,"That is a good question, I'm just porting our test suite that we used in 1.4.4.post1, so I assume this is an old default ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:46,testability,test,test,46,"That is a good question, I'm just porting our test suite that we used in 1.4.4.post1, so I assume this is an old default ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:83,deployability,version,version,83,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:144,deployability,version,version,144,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:83,integrability,version,version,83,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:144,integrability,version,version,144,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:83,modifiability,version,version,83,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:102,modifiability,scal,scalar,102,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:144,modifiability,version,version,144,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:2,safety,reme,remember,2,"I remember there being a bug at some point with anndata where files written by one version would have scalar values read into an array in a new version, but I'd thought that was fixed. Do you know how this anndata object was generated/ when?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:273,safety,test,test,273,Unfortunately I just know that the files were added in https://github.com/galaxyproject/tools-iuc/commit/8dc1fcd2a14f1548249fbc6570bcf399de83703d. If this shouldn't be happening anymore I suppose I could just change `adata.uns[key]['params']['groupby']` to a string in our test data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:273,testability,test,test,273,Unfortunately I just know that the files were added in https://github.com/galaxyproject/tools-iuc/commit/8dc1fcd2a14f1548249fbc6570bcf399de83703d. If this shouldn't be happening anymore I suppose I could just change `adata.uns[key]['params']['groupby']` to a string in our test data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:88,usability,tool,tools-iuc,88,Unfortunately I just know that the files were added in https://github.com/galaxyproject/tools-iuc/commit/8dc1fcd2a14f1548249fbc6570bcf399de83703d. If this shouldn't be happening anymore I suppose I could just change `adata.uns[key]['params']['groupby']` to a string in our test data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:6,deployability,updat,updating,6,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:6,safety,updat,updating,6,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:19,safety,test,test,19,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:6,security,updat,updating,6,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1656:19,testability,test,test,19,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656
https://github.com/scverse/scanpy/pull/1659:248,usability,behavi,behaviour,248,"@ivirshup - looks like something cryptic going on here with the Scrublet code, it's also affecting https://github.com/theislab/scanpy/pull/1657 so it's nothing to do with the changes in this PR. Seems to be something new, to do with annoy and its [behaviour on certain processors](https://www.bountysource.com/issues/91392758-initializing-an-annoyindex-crashes-on-amd-processors) in the CI. . I'm looking into it, but any bright ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:70,safety,test,tests,70,"Never mind, just turned off the problem option for the purpose of the tests :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:70,testability,test,tests,70,"Never mind, just turned off the problem option for the purpose of the tests :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:95,deployability,version,version,95,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:95,integrability,version,version,95,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:95,modifiability,version,version,95,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:6,safety,test,test,6,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:27,safety,test,test,27,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:6,testability,test,test,6,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:27,testability,test,test,27,"Okay, test added! Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:17,deployability,updat,updates,17,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:17,safety,updat,updates,17,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:17,security,updat,updates,17,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/pull/1659:73,usability,help,help,73,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659
https://github.com/scverse/scanpy/issues/1661:95,modifiability,paramet,parameters,95,"After several rounds of debugging, i found that this is related to `sns.set()`. No matter what parameters I put there, it will lead to this issue. . Solution:. ```. sns.reset_orig(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1661
https://github.com/scverse/scanpy/pull/1663:76,reliability,doe,doesn,76,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:23,safety,test,tests,23,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1013,safety,input,inputs,1013,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:23,testability,test,tests,23,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1013,usability,input,inputs,1013,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1054,usability,support,supported,1054,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here? ```python. from anndata import AnnData. import numpy as np. import dask.array as da. from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'). print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'). print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'). print(type(adata.X) # is a numpy array NOT a dask array, not what I expected. ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported? * numpjy `X`, numpy `counts`. * dask `X`, dask `counts` . * csr_matrix `X`, csr_matrix `counts` . Combinations? * numpjy `X`, dask `counts`. * dask `X`, numpy `counts`. * numpjy `X`, csr_matrix `counts`. * csr_matrix `X`, numpy `counts`. * dask `X`, csr_matrix `counts`. * csr_matrix `X`, dask `counts`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:599,deployability,API,API,599,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:367,energy efficiency,core,core,367,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:599,integrability,API,API,599,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1010,integrability,wrap,wrapper,1010,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:71,interoperability,distribut,distributed,71,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:599,interoperability,API,API,599,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1010,interoperability,wrapper,wrapper,1010,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:611,reliability,pra,practice,611,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:98,safety,test,testing,98,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1127,safety,input,inputs,1127,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:98,testability,test,testing,98,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:391,usability,support,support,391,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1127,usability,input,inputs,1127,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:1166,usability,support,supported,1166,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python. a = ad.AnnData(np.ones((1000, 100))). a.X = da.from_array(a.X). type(a.X). # dask.array.core.Array. ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:140,interoperability,convers,conversation,140,> Better support for dask arrays would be a great feature request and series of additions to anndata. I'd be great to have a bigger picture conversation about this with you. I've just sent you a twitter DM with my contact details.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:9,usability,support,support,9,> Better support for dask arrays would be a great feature request and series of additions to anndata. I'd be great to have a bigger picture conversation about this with you. I've just sent you a twitter DM with my contact details.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:652,safety,tmr,tmrw,652,"Apologies for jumping in, but wanted to mention that @sakoht and I have been working on Dask+Anndata a bit over in [celsiustx/anndata](https://github.com/celsiustx/anndata/blob/ctx/anndata_dask.py), and would love to discuss some of these issues or listen in. To date, a lot of the work has been upstream in Dask (e.g. https://github.com/dask/dask/pull/6661; also not quite a PR yet but [celsiustx/dask@sum2](https://github.com/celsiustx/dask/tree/sum2) improves support for Dask Arrays with `scipy.sparse.spmatrix` blocks), but focus should be moving more into Anndata soon. I'll try to put some more cogent thoughts into an issue (likely on Anndata) tmrw, but just wanted to mention here since I've been following this thread with interest! Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:463,usability,support,support,463,"Apologies for jumping in, but wanted to mention that @sakoht and I have been working on Dask+Anndata a bit over in [celsiustx/anndata](https://github.com/celsiustx/anndata/blob/ctx/anndata_dask.py), and would love to discuss some of these issues or listen in. To date, a lot of the work has been upstream in Dask (e.g. https://github.com/dask/dask/pull/6661; also not quite a PR yet but [celsiustx/dask@sum2](https://github.com/celsiustx/dask/tree/sum2) improves support for Dask Arrays with `scipy.sparse.spmatrix` blocks), but focus should be moving more into Anndata soon. I'll try to put some more cogent thoughts into an issue (likely on Anndata) tmrw, but just wanted to mention here since I've been following this thread with interest! Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,deployability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,integrability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,interoperability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,modifiability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,reliability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,security,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:68,testability,integr,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:121,usability,progress,progress,121,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress! @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:104,modifiability,variab,variable,104,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:311,safety,input,input,311,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:311,usability,input,input,311,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:521,energy efficiency,core,core,521,"@GenevieveBuckley, could you provide an example? I think I'm not sure what you mean, as I see dask arrays being passed to `_normalize_total`. I just checked out this PR, added:. ```python. print(type(X), type(counts)). ```. as the first line of `_normalize_data` and ran this code:. ```python. import scanpy as sc. import dask.array as da. adata = sc.datasets.blobs(n_observations=100, n_variables=50). adata.X = da.from_array(adata.X). # WARNING: Some cells have total count of genes equal to zero. # <class 'dask.array.core.Array'>. # <class 'dask.array.core.Array'>. display(adata.X). # dask.array<true_divide, shape=(100, 50), dtype=float32, chunksize=(100, 50), chunktype=numpy.ndarray>. ```. -----------------. Oh, and sorry about the conflicts! I've just removed all the ignored patterns from the `pyproject.toml`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:556,energy efficiency,core,core,556,"@GenevieveBuckley, could you provide an example? I think I'm not sure what you mean, as I see dask arrays being passed to `_normalize_total`. I just checked out this PR, added:. ```python. print(type(X), type(counts)). ```. as the first line of `_normalize_data` and ran this code:. ```python. import scanpy as sc. import dask.array as da. adata = sc.datasets.blobs(n_observations=100, n_variables=50). adata.X = da.from_array(adata.X). # WARNING: Some cells have total count of genes equal to zero. # <class 'dask.array.core.Array'>. # <class 'dask.array.core.Array'>. display(adata.X). # dask.array<true_divide, shape=(100, 50), dtype=float32, chunksize=(100, 50), chunktype=numpy.ndarray>. ```. -----------------. Oh, and sorry about the conflicts! I've just removed all the ignored patterns from the `pyproject.toml`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:741,interoperability,conflict,conflicts,741,"@GenevieveBuckley, could you provide an example? I think I'm not sure what you mean, as I see dask arrays being passed to `_normalize_total`. I just checked out this PR, added:. ```python. print(type(X), type(counts)). ```. as the first line of `_normalize_data` and ran this code:. ```python. import scanpy as sc. import dask.array as da. adata = sc.datasets.blobs(n_observations=100, n_variables=50). adata.X = da.from_array(adata.X). # WARNING: Some cells have total count of genes equal to zero. # <class 'dask.array.core.Array'>. # <class 'dask.array.core.Array'>. display(adata.X). # dask.array<true_divide, shape=(100, 50), dtype=float32, chunksize=(100, 50), chunktype=numpy.ndarray>. ```. -----------------. Oh, and sorry about the conflicts! I've just removed all the ignored patterns from the `pyproject.toml`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:10,safety,reme,remember,10,"Yes, if i remember correctly `np.ravel` converts dask to numpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:54,energy efficiency,current,currently,54,"@Koncopd, I thought that too, but this isn't the case currently:. ```python. from dask.array import da; import numpy as np. np.ravel(da.ones(10)). # dask.array<ones, shape=(10,), dtype=float64, chunksize=(10,), chunktype=numpy.ndarray>. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:43,interoperability,conflict,conflicts,43,"Mind if I merge #1667? It would cause more conflicts, but should clean up the flow control in `normalize_total`. I'd also be happy to add a commit fixing up the conflicts here. Edit: I added that commit. Feel free to remove it's troublesome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:161,interoperability,conflict,conflicts,161,"Mind if I merge #1667? It would cause more conflicts, but should clean up the flow control in `normalize_total`. I'd also be happy to add a commit fixing up the conflicts here. Edit: I added that commit. Feel free to remove it's troublesome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:83,security,control,control,83,"Mind if I merge #1667? It would cause more conflicts, but should clean up the flow control in `normalize_total`. I'd also be happy to add a commit fixing up the conflicts here. Edit: I added that commit. Feel free to remove it's troublesome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1663:83,testability,control,control,83,"Mind if I merge #1667? It would cause more conflicts, but should clean up the flow control in `normalize_total`. I'd also be happy to add a commit fixing up the conflicts here. Edit: I added that commit. Feel free to remove it's troublesome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663
https://github.com/scverse/scanpy/pull/1669:173,safety,test,test,173,"Thanks for all these fixes @mvdbeek! It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:356,safety,test,test,356,"Thanks for all these fixes @mvdbeek! It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:173,testability,test,test,173,"Thanks for all these fixes @mvdbeek! It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:356,testability,test,test,356,"Thanks for all these fixes @mvdbeek! It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:29,usability,help,help,29,"Hi, anyway in which we could help on this? We need this fix for a training that we are conducting second half of April. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:74,deployability,updat,update,74,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:74,safety,updat,update,74,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:85,safety,test,test,85,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:74,security,updat,update,74,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:85,testability,test,test,85,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:93,safety,test,tests,93,"Nope, I don't think we can work around this. If the fix is not right, could someone take the tests that are included here and fix them ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:93,testability,test,tests,93,"Nope, I don't think we can work around this. If the fix is not right, could someone take the tests that are included here and fix them ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:58,availability,sli,slicing,58,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:752,availability,mainten,maintenance,752,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:58,reliability,sli,slicing,58,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:752,reliability,mainten,maintenance,752,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:703,safety,test,tested,703,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:703,testability,test,tested,703,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python. for g in _gene_names:. if adata.raw is not None and use_raw:. X_col = adata.raw[:, g].X. if gene_symbols:. g = adata.raw.var[gene_symbols][g]. else:. X_col = adata[:, g].X. if gene_symbols:. g = adata.var[gene_symbols][g]. if issparse(X_col):. X_col = X_col.toarray().flatten(). X_col = X_col.toarray().flatten(). new_gene_names.append(g). df[g] = X_col. ```. by . ```python. df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols. new_gene_names = df.columns. ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:94,deployability,updat,updates,94,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:9,reliability,doe,does,9,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:94,safety,updat,updates,94,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:94,security,updat,updates,94,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:129,safety,test,tests,129,"Ok this should be good to go @ivirshup , I've incorporated the suggestion from @fidelram , thanks @mvdbeek for first attempt and tests!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:129,testability,test,tests,129,"Ok this should be good to go @ivirshup , I've incorporated the suggestion from @fidelram , thanks @mvdbeek for first attempt and tests!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:295,deployability,releas,release,295,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out. I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:140,reliability,doe,does,140,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out. I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:238,safety,test,test,238,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out. I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:238,testability,test,test,238,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out. I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:24,deployability,releas,release,24,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:70,safety,test,tests,70,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/pull/1669:70,testability,test,tests,70,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669
https://github.com/scverse/scanpy/issues/1670:155,availability,error,error,155,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:155,performance,error,error,155,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:60,safety,compl,complete,60,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:155,safety,error,error,155,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:60,security,compl,complete,60,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:155,usability,error,error,155,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:363,deployability,version,versions,363,"```import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```. Here is the output for the example code you provided. Could it be package versions? ![CBC5B0B6-CB0D-47E1-BD84-59A329762A9D](https://user-images.githubusercontent.com/16257776/108631788-ab402e00-7439-11eb-8fdc-27452a7a5a1f.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:363,integrability,version,versions,363,"```import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```. Here is the output for the example code you provided. Could it be package versions? ![CBC5B0B6-CB0D-47E1-BD84-59A329762A9D](https://user-images.githubusercontent.com/16257776/108631788-ab402e00-7439-11eb-8fdc-27452a7a5a1f.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:355,modifiability,pac,package,355,"```import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```. Here is the output for the example code you provided. Could it be package versions? ![CBC5B0B6-CB0D-47E1-BD84-59A329762A9D](https://user-images.githubusercontent.com/16257776/108631788-ab402e00-7439-11eb-8fdc-27452a7a5a1f.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:363,modifiability,version,versions,363,"```import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```. Here is the output for the example code you provided. Could it be package versions? ![CBC5B0B6-CB0D-47E1-BD84-59A329762A9D](https://user-images.githubusercontent.com/16257776/108631788-ab402e00-7439-11eb-8fdc-27452a7a5a1f.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:421,usability,user,user-images,421,"```import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""). sc.tl.filter_rank_genes_groups(. pbmc,. min_fold_change=1,. min_in_group_fraction=0.25,. max_out_group_fraction=0.5,. use_raw=False,. ). ```. Here is the output for the example code you provided. Could it be package versions? ![CBC5B0B6-CB0D-47E1-BD84-59A329762A9D](https://user-images.githubusercontent.com/16257776/108631788-ab402e00-7439-11eb-8fdc-27452a7a5a1f.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:203,availability,error,error,203,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:129,deployability,version,versions,129,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:161,deployability,upgrad,upgrade,161,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:230,deployability,version,version,230,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:129,integrability,version,versions,129,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:230,integrability,version,version,230,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:129,modifiability,version,versions,129,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:161,modifiability,upgrad,upgrade,161,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:230,modifiability,version,version,230,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:203,performance,error,error,203,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:203,safety,error,error,203,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:203,usability,error,error,203,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:310,usability,command,command,310,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error? * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:211,availability,error,error,211,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:131,deployability,version,versions,131,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:169,deployability,upgrad,upgrade,169,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:240,deployability,version,version,240,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:343,deployability,updat,update,343,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:437,deployability,updat,updated,437,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1594,deployability,log,logical,1594,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1648,deployability,updat,updated,1648,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1602,energy efficiency,CPU,CPU,1602,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1606,energy efficiency,core,cores,1606,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:131,integrability,version,versions,131,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:240,integrability,version,version,240,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:131,modifiability,version,versions,131,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:169,modifiability,upgrad,upgrade,169,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:240,modifiability,version,version,240,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:651,modifiability,deco,decorator,651,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:993,modifiability,pac,packaging,993,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:211,performance,error,error,211,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1602,performance,CPU,CPU,1602,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:211,safety,error,error,211,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:343,safety,updat,update,343,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:437,safety,updat,updated,437,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1594,safety,log,logical,1594,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1648,safety,updat,updated,1648,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:343,security,updat,update,343,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:437,security,updat,updated,437,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1594,security,log,logical,1594,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1628,security,Session,Session,1628,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1648,security,updat,updated,1648,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1594,testability,log,logical,1594,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:211,usability,error,error,211,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:320,usability,command,command,320,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. > . > * If you upgrade scipy, do you still run into this error? > * Could you get the version info from an environment where you've only imported scanpy and run this command? I will try to update scipy. Here is the output from only import scanpy:. BTW, everything works fine until I updated scanpy to 1.7.0. ```. anndata 0.7.4. scanpy 1.7.0. sinfo 0.3.1. -----. PIL 7.2.0. anndata 0.7.4. backcall 0.2.0. cairo 1.19.1. cffi 1.14.4. colorama 0.4.3. cycler 0.10.0. cython_runtime NA. dateutil 2.8.1. decorator 4.4.2. future_fstrings NA. get_version 2.1. h5py 2.10.0. igraph 0.8.2. ipykernel 5.3.4. ipython_genutils 0.2.0. jedi 0.17.2. joblib 0.16.0. kiwisolver 1.2.0. legacy_api_wrap 1.2. leidenalg 0.8.1. llvmlite 0.34.0. louvain 0.7.0. matplotlib 3.3.1. mkl 2.3.0. mpl_toolkits NA. natsort 7.1.1. numba 0.51.2. numexpr 2.7.1. numpy 1.19.1. packaging 20.8. pandas 1.2.1. parso 0.7.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.6. psutil 5.7.2. ptyprocess 0.6.0. pygments 2.6.1. pyparsing 2.4.7. pytz 2020.1. scanpy 1.7.0. scipy 1.4.1. setuptools_scm NA. sinfo 0.3.1. six 1.15.0. sklearn 0.23.2. storemagic NA. tables 3.6.1. texttable 1.6.2. tornado 6.0.4. traitlets 4.3.3. wcwidth 0.2.5. zmq 19.0.2. zope NA. -----. IPython 7.17.0. jupyter_client 6.1.6. jupyter_core 4.6.3. notebook 6.1.3. -----. Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]. Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10. 64 logical CPU cores, x86_64. -----. Session information updated at 2021-02-21 23:42. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:100,deployability,version,version,100,"I can now reproduce this, it looks like scipy 1.4.0 has the relevant bug. We will bump the required version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:100,integrability,version,version,100,"I can now reproduce this, it looks like scipy 1.4.0 has the relevant bug. We will bump the required version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:100,modifiability,version,version,100,"I can now reproduce this, it looks like scipy 1.4.0 has the relevant bug. We will bump the required version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:0,deployability,upgrad,upgrading,0,upgrading the scipy to 1.6.0 resolves the issue. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:0,modifiability,upgrad,upgrading,0,upgrading the scipy to 1.6.0 resolves the issue. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2496,availability,error,error,2496,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2701,availability,error,error,2701,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:642,deployability,Fail,Failed,642,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:699,deployability,Fail,Failed,699,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:982,deployability,modul,module,982,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2574,deployability,Fail,Failed,2574,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1828,integrability,wrap,wrapper,1828,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1828,interoperability,wrapper,wrapper,1828,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:238,modifiability,pac,packages,238,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:453,modifiability,pac,packages,453,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:982,modifiability,modul,module,982,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1112,modifiability,pac,packages,1112,"----------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:1368,modifiability,pac,packages,1368,"*kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2061,modifiability,pac,packages,2061,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2304,modifiability,pac,packages,2304,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2496,performance,error,error,2496,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2701,performance,error,error,2701,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:642,reliability,Fail,Failed,642,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:699,reliability,Fail,Failed,699,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2574,reliability,Fail,Failed,2574,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:386,safety,except,except,386,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:393,safety,Except,Exception,393,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:830,safety,except,exception,830,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:878,safety,except,exception,878,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:954,safety,input,input-,954,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:982,safety,modul,module,982,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2388,safety,except,except,2388,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2395,safety,Except,Exception,2395,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2496,safety,error,error,2496,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2701,safety,error,error,2701,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:160,testability,Trace,Traceback,160,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:910,testability,Trace,Traceback,910,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:954,usability,input,input-,954,"actually something worse happened. Now I cant save adata. ```. ---------------------------------------------------------------------------. NotImplementedError Traceback (most recent call last). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 187 try:. --> 188 return func(elem, key, val, *args, **kwargs). 189 except Exception as e:. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_not_implemented(f, key, value, dataset_kwargs). 143 # and have explicit implementations for everything else. --> 144 raise NotImplementedError(. 145 f""Failed to write value for {key}, "". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2496,usability,error,error,2496,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:2701,usability,error,error,2701,"_matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last). <ipython-input-102-4378df4ffefd> in <module>. ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense). 1844 filename = self.filename. 1845 . -> 1846 _write_h5ad(. 1847 Path(filename),. 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs). 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):. 91 # If adata.isbacked, X should already be up to date. ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs). 93 if ""raw/X"" in as_dense and isinstance(. 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw). 872 '1 positional argument'). 873 . --> 874 return dispatch(args[0].__class__)(*args, **kw). 875 . 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs). 124 if key in f:. 125 del f[key]. --> 126 _write_method(type(value))(f, key, value, *args, **kwargs). 127 . 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs). 189 except Exception as e:. 190 parent = _get_parent(elem). --> 191 raise type(e)(. 192 f""{e}\n\n"". 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:52,deployability,version,versions,52,"@YubinXie, that is also super weird. * Can you show versions from this environment? * Can you also create a completely fresh environment and try this again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:52,integrability,version,versions,52,"@YubinXie, that is also super weird. * Can you show versions from this environment? * Can you also create a completely fresh environment and try this again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:52,modifiability,version,versions,52,"@YubinXie, that is also super weird. * Can you show versions from this environment? * Can you also create a completely fresh environment and try this again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:108,safety,compl,completely,108,"@YubinXie, that is also super weird. * Can you show versions from this environment? * Can you also create a completely fresh environment and try this again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:108,security,compl,completely,108,"@YubinXie, that is also super weird. * Can you show versions from this environment? * Can you also create a completely fresh environment and try this again?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:16,deployability,updat,updating,16,"Oh, yeah, after updating it works. The previous issue is due to 'un-re-fresh' of the notebook kernel!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:16,safety,updat,updating,16,"Oh, yeah, after updating it works. The previous issue is due to 'un-re-fresh' of the notebook kernel!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/issues/1670:16,security,updat,updating,16,"Oh, yeah, after updating it works. The previous issue is due to 'un-re-fresh' of the notebook kernel!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670
https://github.com/scverse/scanpy/pull/1672:281,availability,avail,available,281,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:296,availability,down,downside,296,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:184,deployability,configurat,configuration,184,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:409,deployability,upgrad,upgrade,409,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:184,integrability,configur,configuration,184,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:433,integrability,sub,subscription,433,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:184,modifiability,configur,configuration,184,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:409,modifiability,upgrad,upgrade,409,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:210,reliability,doe,doesn,210,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:281,reliability,availab,available,281,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:281,safety,avail,available,281,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:184,security,configur,configuration,184,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:281,security,availab,available,281,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:454,security,access,access,454,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:24,usability,user,user-images,24,"Sorta! ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:51,deployability,build,build,51,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:178,deployability,api,api,178,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:272,deployability,version,version,272,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:417,deployability,build,build,417,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:178,integrability,api,api,178,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:272,integrability,version,version,272,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:178,interoperability,api,api,178,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:272,modifiability,version,version,272,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:627,safety,input,input,627,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:442,usability,user,user-images,442,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:553,usability,confirm,confirm,553,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:627,usability,input,input,627,"I think it's not showing results because it's a PR build. If I make the same request to `latest`. ```python. In [1]: import requests. ...: URL = 'https://scanpy.readthedocs.io/_/api/v2/search/'. ...: params = {. ...: 'q': 'plotting',. ...: 'project': 'icb-scanpy',. ...: 'version': 'latest',. ...: }. ...: response = requests.get(URL, params=params). ...: print(response.json()). ```. There are results. Using the PR build:. ![image](https://user-images.githubusercontent.com/8238804/108616218-91194680-745f-11eb-9de8-838f71a39aa0.png). Not sure how to confirm this besides merging and seeing how it goes. Would be good to get input before that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/pull/1672:76,reliability,doe,doesn,76,"I'm going to merge this to see if it works, and so we can try it out. If it doesn't work, we can revert.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672
https://github.com/scverse/scanpy/issues/1675:98,deployability,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675
https://github.com/scverse/scanpy/issues/1675:98,integrability,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675
https://github.com/scverse/scanpy/issues/1675:98,modifiability,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675
https://github.com/scverse/scanpy/issues/1675:98,safety,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675
https://github.com/scverse/scanpy/issues/1675:98,testability,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675
https://github.com/scverse/scanpy/pull/1679:119,performance,time,time,119,"> we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. I thought at one point you guys were checking flake8 with CI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1679:93,usability,clear,clearly,93,"> we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. I thought at one point you guys were checking flake8 with CI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1679:220,availability,reliab,reliable,220,"> I thought at one point you guys were checking flake8 with CI. We were, sorta. The CI tool we were using had pretty stochastic reporting (which isn't really what we want in a CI tool). Hopefully it'll be back in a more reliable form soon: https://github.com/theislab/scanpy/issues/1563#issuecomment-784922713",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1679:220,reliability,reliab,reliable,220,"> I thought at one point you guys were checking flake8 with CI. We were, sorta. The CI tool we were using had pretty stochastic reporting (which isn't really what we want in a CI tool). Hopefully it'll be back in a more reliable form soon: https://github.com/theislab/scanpy/issues/1563#issuecomment-784922713",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1679:87,usability,tool,tool,87,"> I thought at one point you guys were checking flake8 with CI. We were, sorta. The CI tool we were using had pretty stochastic reporting (which isn't really what we want in a CI tool). Hopefully it'll be back in a more reliable form soon: https://github.com/theislab/scanpy/issues/1563#issuecomment-784922713",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1679:179,usability,tool,tool,179,"> I thought at one point you guys were checking flake8 with CI. We were, sorta. The CI tool we were using had pretty stochastic reporting (which isn't really what we want in a CI tool). Hopefully it'll be back in a more reliable form soon: https://github.com/theislab/scanpy/issues/1563#issuecomment-784922713",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679
https://github.com/scverse/scanpy/pull/1680:24,interoperability,conflict,conflicts,24,LGTM. . Sorry about the conflicts! Might be worth a rebase.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:26,deployability,releas,release,26,"Oops, forgot to ask for a release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:2,deployability,Releas,Release,2,"* Release note, yeah, a new PR. * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:152,deployability,version,versioning,152,"* Release note, yeah, a new PR. * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:152,integrability,version,versioning,152,"* Release note, yeah, a new PR. * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:232,interoperability,format,format,232,"* Release note, yeah, a new PR. * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:152,modifiability,version,versioning,152,"* Release note, yeah, a new PR. * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/pull/1680:7,deployability,releas,release,7,adding release note in #1740,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680
https://github.com/scverse/scanpy/issues/1688:55,safety,test,test,55,"Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:55,testability,test,test,55,"Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:305,usability,indicat,indicates,305,"Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:57,safety,test,test,57,"> Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression. Thank you very much !!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:57,testability,test,test,57,"> Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression. Thank you very much !!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:307,usability,indicat,indicates,307,"> Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression. Thank you very much !!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/issues/1688:575,reliability,doe,doesn,575,"Hi, I had a quick question. So, when I run sc.tl.rank_genes_groups(), and then I save the names, scores, pvals, and pvals_adj: I do see that the names are ordered as per decreasing scores. However, the genes with the lowest pvalues are not the ones with the highest scores. In fact, quite the opposite--the ones with the highest scores are the most negative p-valued ones. I'm quite confused about this. Are the pvals and pvals_adjusted matrices not ordered as per the scores? Also, the description says that the scores are just the zscores of the pvalues, however that also doesn't quite seem to be the case. Can someone please explain how we go from pvalues to scores?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688
https://github.com/scverse/scanpy/pull/1689:202,availability,down,down,202,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py). `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:73,deployability,modul,module,73,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py). `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:73,modifiability,modul,module,73,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py). `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:253,modifiability,variab,variable,253,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py). `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:73,safety,modul,module,73,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py). `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:209,availability,error,errors,209,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:262,availability,error,errors,262,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:161,deployability,build,build,161,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:339,deployability,modul,module,339,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:270,interoperability,format,format,270,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:99,modifiability,pac,packages,99,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:339,modifiability,modul,module,339,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:209,performance,error,errors,209,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:262,performance,error,errors,262,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:475,reliability,Doe,Does,475,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:24,safety,test,tests,24,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:209,safety,error,errors,209,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:262,safety,error,errors,262,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:339,safety,modul,module,339,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:359,safety,test,tests,359,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:24,testability,test,tests,24,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:359,testability,test,tests,359,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:134,usability,User,UserWarning,134,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:209,usability,error,errors,209,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:262,usability,error,errors,262,"```. $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff. ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:254,availability,error,error,254,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:236,deployability,contain,container,236,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:813,integrability,complian,compliant,813,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1222,integrability,event,eventually,1222,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1082,interoperability,specif,specific,1082,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:216,modifiability,variab,variable,216,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:452,modifiability,variab,variables,452,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:536,modifiability,variab,variables,536,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:101,performance,time,time,101,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:254,performance,error,error,254,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:254,safety,error,error,254,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:352,safety,test,tests,352,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:813,safety,compl,compliant,813,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:813,security,compl,compliant,813,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:352,testability,test,tests,352,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:975,testability,plan,plan,975,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:254,usability,error,error,254,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1529,availability,error,error,1529,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:679,deployability,automat,automated,679,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:790,deployability,automat,automatically,790,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1511,deployability,contain,container,1511,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:262,integrability,sub,subset,262,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:709,integrability,complian,compliance,709,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1328,integrability,complian,compliant,1328,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:775,interoperability,format,formatter,775,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:835,interoperability,format,formatted,835,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1061,interoperability,format,formatter,1061,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1491,modifiability,variab,variable,1491,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1757,modifiability,concern,concerned,1757,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:103,performance,time,time,103,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:580,performance,time,time,580,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1529,performance,error,error,1529,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:599,safety,review,reviewer,599,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:709,safety,compl,compliance,709,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1328,safety,compl,compliant,1328,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1529,safety,error,error,1529,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1631,safety,test,tests,1631,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:709,security,compl,compliance,709,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1211,security,team,team,1211,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1328,security,compl,compliant,1328,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:599,testability,review,reviewer,599,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:679,testability,automat,automated,679,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:790,testability,automat,automatically,790,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1127,testability,plan,plan,1127,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1631,testability,test,tests,1631,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1757,testability,concern,concerned,1757,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:689,usability,tool,tools,689,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:758,usability,prefer,prefer,758,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:815,usability,tool,tool,815,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1529,usability,error,error,1529,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1710,usability,close,closer,1710,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."". >. > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:25,deployability,automat,automated,25,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:136,deployability,automat,automatically,136,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:985,deployability,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:55,integrability,complian,compliance,55,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:486,integrability,sub,subset,486,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:985,integrability,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:121,interoperability,format,formatter,121,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:181,interoperability,format,formatted,181,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:829,interoperability,format,formatter,829,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:985,modifiability,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:580,performance,time,time,580,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:55,safety,compl,compliance,55,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:985,safety,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:55,security,compl,compliance,55,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:979,security,team,team,979,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:25,testability,automat,automated,25,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:136,testability,automat,automatically,136,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:895,testability,plan,plan,895,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:985,testability,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:35,usability,tool,tools,35,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:104,usability,prefer,prefer,104,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:161,usability,tool,tool,161,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:622,usability,close,close,622,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:127,deployability,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:559,deployability,fail,fails,559,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:127,integrability,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:516,integrability,sub,subsequent,516,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:304,interoperability,share,share,304,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:636,interoperability,conflict,conflicts,636,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:127,modifiability,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:559,reliability,fail,fails,559,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:127,safety,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:127,testability,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:55,interoperability,conflict,conflicts,55,Yes please! It’s been soooo long. I’ll happily fix the conflicts here as long as I don’t have to change the flit PR again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:999,deployability,automat,automatically,999,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1520,deployability,build,build,1520,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:229,performance,time,time,229,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:260,performance,time,time,260,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1257,reliability,doe,doesn,1257,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1166,safety,except,except,1166,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1173,safety,Except,Exception,1173,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1329,safety,test,tests,1329,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1370,safety,test,test,1370,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1445,safety,test,tests,1445,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1489,safety,test,tests,1489,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:503,security,team,team,503,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:571,security,hack,hackathon,571,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:999,testability,automat,automatically,999,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1329,testability,test,tests,1329,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1370,testability,test,test,1370,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1445,testability,test,tests,1445,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1489,testability,test,tests,1489,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:650,usability,close,close,650,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:676,usability,document,documentation,676,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:722,usability,Document,Document,722,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:779,usability,Document,Document,779,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added? In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs. - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)). - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. - [x] Add annotation to `noqa`s. - [x] Also add `TODO` annotation to `except Exception`s that have been added. - [x] Turn off E731. - [x] Turn off the rule that doesn't allow multiple leading `#` in comments. - [x] Turn off F811 for tests (rule violated by using fixture as test argument). - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this. - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:156,availability,state,state,156,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1078,availability,consist,consistently,1078,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:748,deployability,automat,automatically,748,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:898,deployability,configurat,configuration,898,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:156,integrability,state,state,156,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:898,integrability,configur,configuration,898,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1056,interoperability,format,formatter,1056,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1070,interoperability,format,formats,1070,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1112,interoperability,compatib,compatible,1112,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:650,modifiability,maintain,maintained,650,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:898,modifiability,configur,configuration,898,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:880,reliability,doe,does,880,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:650,safety,maintain,maintained,650,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:898,security,configur,configuration,898,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:748,testability,automat,automatically,748,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:442,usability,Document,Document,442,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:609,usability,document,documentation,609,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:681,usability,document,documentation,681,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1078,usability,consist,consistently,1078,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1134,usability,tool,tools,1134,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added? The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:539,availability,failur,failure,539,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:881,availability,error,errors,881,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1316,availability,consist,consistently,1316,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:539,deployability,fail,failure,539,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1179,deployability,configurat,configuration,1179,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1288,deployability,configurat,configuration,1288,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1409,deployability,automat,automatically,1409,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1440,deployability,automat,automatically,1440,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1542,deployability,automat,automate,1542,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1671,deployability,build,build,1671,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1693,deployability,build,build,1693,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:285,integrability,messag,messages,285,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1179,integrability,configur,configuration,1179,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1288,integrability,configur,configuration,1288,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:285,interoperability,messag,messages,285,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:370,interoperability,specif,specific,370,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1308,interoperability,format,formats,1308,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1350,interoperability,compatib,compatible,1350,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:986,modifiability,maintain,maintained,986,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1179,modifiability,configur,configuration,1179,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1288,modifiability,configur,configuration,1288,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1681,modifiability,pac,package,1681,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:539,performance,failur,failure,539,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:881,performance,error,errors,881,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:539,reliability,fail,failure,539,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1161,reliability,doe,does,1161,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1242,reliability,doe,does,1242,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:419,safety,review,review,419,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:655,safety,compl,complicated,655,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:881,safety,error,errors,881,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:986,safety,maintain,maintained,986,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:655,security,compl,complicated,655,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1179,security,configur,configuration,1179,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1288,security,configur,configuration,1288,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:419,testability,review,review,419,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1409,testability,automat,automatically,1409,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1440,testability,automat,automatically,1440,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1542,testability,automat,automate,1542,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:881,usability,error,errors,881,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:945,usability,document,documentation,945,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1017,usability,document,documentation,1017,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1316,usability,consist,consistently,1316,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:1372,usability,tool,tools,1372,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks? How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest? ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:68,integrability,complian,compliant,68,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:217,integrability,queue,queued,217,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:217,performance,queue,queued,217,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:68,safety,compl,compliant,68,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:68,security,compl,compliant,68,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:197,safety,review,review,197,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:197,testability,review,review,197,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:96,usability,feedback,feedback,96,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:64,deployability,configurat,configuration,64,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:114,deployability,contain,contains,114,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:312,deployability,configurat,configuration,312,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:386,deployability,continu,continues,386,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:64,integrability,configur,configuration,64,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:312,integrability,configur,configuration,312,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:237,interoperability,format,formatting,237,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:269,interoperability,format,formatter,269,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:338,interoperability,format,formatted,338,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:559,interoperability,conflict,conflicts,559,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:64,modifiability,configur,configuration,64,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:312,modifiability,configur,configuration,312,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:420,safety,except,exceptions,420,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:64,security,configur,configuration,64,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:312,security,configur,configuration,312,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:85,usability,minim,minimal,85,"@ivirshup I hope that I caught all of your comments. The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever. Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:61,deployability,releas,release,61,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:229,modifiability,variab,variable,229,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:154,reliability,pra,practices,154,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:540,safety,compl,complain,540,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:540,security,compl,complain,540,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/pull/1689:524,usability,workflow,workflows,524,"> Looks good, thanks for all the work! > . > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that? > . > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: . Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""? Everything else might just be details that people will uncover anyways since the workflows might complain :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689
https://github.com/scverse/scanpy/issues/1694:141,availability,avail,available,141,"Some ways I like to work with the code:. * Left side of screen is text editor, right side is terminal/ docs/ something else. * 86 characters available if I have a side bar open, 95 without. * Split code browser. * 84 columns with a side bar open, 95 without. 120 is too long for this. Also this is a pretty wide laptop screen (16-inch). I believe Alex uses a MacBook Air with even more limited screen real estate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694
https://github.com/scverse/scanpy/issues/1694:141,reliability,availab,available,141,"Some ways I like to work with the code:. * Left side of screen is text editor, right side is terminal/ docs/ something else. * 86 characters available if I have a side bar open, 95 without. * Split code browser. * 84 columns with a side bar open, 95 without. 120 is too long for this. Also this is a pretty wide laptop screen (16-inch). I believe Alex uses a MacBook Air with even more limited screen real estate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694
https://github.com/scverse/scanpy/issues/1694:141,safety,avail,available,141,"Some ways I like to work with the code:. * Left side of screen is text editor, right side is terminal/ docs/ something else. * 86 characters available if I have a side bar open, 95 without. * Split code browser. * 84 columns with a side bar open, 95 without. 120 is too long for this. Also this is a pretty wide laptop screen (16-inch). I believe Alex uses a MacBook Air with even more limited screen real estate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694
https://github.com/scverse/scanpy/issues/1694:141,security,availab,available,141,"Some ways I like to work with the code:. * Left side of screen is text editor, right side is terminal/ docs/ something else. * 86 characters available if I have a side bar open, 95 without. * Split code browser. * 84 columns with a side bar open, 95 without. 120 is too long for this. Also this is a pretty wide laptop screen (16-inch). I believe Alex uses a MacBook Air with even more limited screen real estate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694
https://github.com/scverse/scanpy/issues/1694:81,usability,prefer,prefer,81,Closing because I don't think that I could convince people here although I still prefer 120 :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694
https://github.com/scverse/scanpy/issues/1698:106,availability,redund,redundant,106,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:106,deployability,redundan,redundant,106,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:64,integrability,Coupl,Couple,64,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:64,modifiability,Coupl,Couple,64,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:128,modifiability,pac,packages,128,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:106,reliability,redundan,redundant,106,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:106,safety,redund,redundant,106,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:64,testability,Coupl,Couple,64,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:101,usability,tool,tool,101,hi @Hrovatin . so was it useful for your task? Curious to hear. Couple of questions:. - why having a tool redundant between two packages? . - what is SEMITONES? thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,deployability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,deployability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,deployability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:644,deployability,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:726,deployability,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1257,deployability,continu,continuous,1257,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:601,energy efficiency,current,current,601,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1349,energy efficiency,current,currently,1349,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,integrability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,integrability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,integrability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:508,integrability,sub,subtype,508,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:644,integrability,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:726,integrability,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1329,integrability,sub,subtype,1329,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,interoperability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,interoperability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,interoperability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,modifiability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:141,modifiability,variab,variable,141,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,modifiability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,modifiability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:644,modifiability,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:718,modifiability,pac,package,718,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:726,modifiability,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:847,modifiability,pac,package,847,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:882,modifiability,variab,variable,882,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:581,performance,perform,performs,581,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:805,performance,content,content,805,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,reliability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,reliability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,reliability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:486,reliability,doe,does,486,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:644,safety,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:726,safety,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1046,safety,compl,complicated,1046,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,security,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,security,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,security,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1046,security,compl,complicated,1046,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:87,testability,integr,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:176,testability,integr,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:421,testability,integr,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:644,testability,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:726,testability,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:581,usability,perform,performs,581,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. . I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:13,testability,understand,understand,13,"I see, now I understand the usage better as a metric, thank you! I'd suggest that this is more of a feature request for Scanpy then @Hrovatin , feel free to open one or contribute to (theislab/scanpy#915) !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:54,deployability,modul,module,54,This would be a good fit for a potential `sc.metrics` module as discussed recently (can't find the issue anymore),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:54,modifiability,modul,module,54,This would be a good fit for a potential `sc.metrics` module as discussed recently (can't find the issue anymore),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:54,safety,modul,module,54,This would be a good fit for a potential `sc.metrics` module as discussed recently (can't find the issue anymore),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:314,interoperability,distribut,distribution,314,"Looks like `gearys_c` and `morans_i` will be out in `1.8` (`morans_i` should be on master pretty soon). @Hrovatin, do you have any thoughts on the usefulness of calculating a p-value vs just calculating `I`? How could we tell whether the statistic could be influenced by the structure of the graph, or non-spatial distribution of the values? Also, maybe there's a faster way to calculate it than with a fixed number of iterations, especially for structured grids like we have with visium data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:435,energy efficiency,measur,measured,435,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1683,energy efficiency,current,current,1683,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1513,interoperability,distribut,distribution,1513,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1795,safety,test,testing,1795,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1782,security,sign,significance,1782,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:34,testability,context,context,34,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1795,testability,test,testing,1795,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:687,usability,clear,clear,687,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:836,usability,user,user-images,836,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1021,usability,user,user-images,1021,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1208,usability,user,user-images,1208,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1396,usability,user,user-images,1396,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1585,usability,user,user-images,1585,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>. <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>. <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>. <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>. <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:71,deployability,observ,observe,71,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:79,deployability,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:557,energy efficiency,estimat,estimated,557,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:79,integrability,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:79,modifiability,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:79,safety,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:71,testability,observ,observe,71,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:79,testability,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:172,testability,regress,regress,172,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:248,usability,user,user-images,248,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:635,usability,user,user-images,635,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin. ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png). And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:5,energy efficiency,current,current,5,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:649,energy efficiency,estimat,estimated,649,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:932,energy efficiency,draw,draw,932,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:753,interoperability,distribut,distribution,753,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:117,safety,test,testing,117,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1019,safety,test,test,1019,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1118,safety,compl,completeness,1118,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:104,security,sign,significance,104,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:952,security,sign,significance,952,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1118,security,compl,completeness,1118,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:117,testability,test,testing,117,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1019,testability,test,test,1019,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:807,usability,person,personal,807,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:912,usability,person,personally,912,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...). I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:10,modifiability,scal,scaling,10,"Maybe row-scaling of connectives should be an option to ensure that morans I stays between [-1,1] as else it may get very large values on some data. ```. morans_i=sc.metrics.morans_i(adata_pb_c). morans_i=pd.Series(morans_i,index=adata_pb_c.var_names). print(morans_i.describe()). count 1.412000e+04. mean 3.701341e+277. std inf. min -5.246069e+05. 25% 1.087433e-10. 50% 4.935376e-01. 75% 1.005601e+00. max 4.440351e+281. dtype: float64. # Row scaling so that I is between -1 and 1. adata_pb_c_temp=adata_pb_c.copy(). adata_pb_c_temp.obsp['connectivities']=csr_matrix(minmax_scale(. adata_pb_c_temp.obsp['connectivities'].todense(),axis=1)). morans_i=sc.metrics.morans_i(adata_pb_c_temp). morans_i=pd.Series(morans_i,index=adata_pb_c.var_names). del adata_pb_c_temp. print(morans_i.describe()). count 1.412000e+04. mean 1.737230e-01. std 2.733991e-01. min 0.000000e+00. 25% 7.163952e-322. 50% 1.462434e-321. 75% 4.578002e-01. max 9.320416e-01. dtype: float64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:444,modifiability,scal,scaling,444,"Maybe row-scaling of connectives should be an option to ensure that morans I stays between [-1,1] as else it may get very large values on some data. ```. morans_i=sc.metrics.morans_i(adata_pb_c). morans_i=pd.Series(morans_i,index=adata_pb_c.var_names). print(morans_i.describe()). count 1.412000e+04. mean 3.701341e+277. std inf. min -5.246069e+05. 25% 1.087433e-10. 50% 4.935376e-01. 75% 1.005601e+00. max 4.440351e+281. dtype: float64. # Row scaling so that I is between -1 and 1. adata_pb_c_temp=adata_pb_c.copy(). adata_pb_c_temp.obsp['connectivities']=csr_matrix(minmax_scale(. adata_pb_c_temp.obsp['connectivities'].todense(),axis=1)). morans_i=sc.metrics.morans_i(adata_pb_c_temp). morans_i=pd.Series(morans_i,index=adata_pb_c.var_names). del adata_pb_c_temp. print(morans_i.describe()). count 1.412000e+04. mean 1.737230e-01. std 2.733991e-01. min 0.000000e+00. 25% 7.163952e-322. 50% 1.462434e-321. 75% 4.578002e-01. max 9.320416e-01. dtype: float64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:215,deployability,modul,module,215,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:416,deployability,scale,scale,416,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:416,energy efficiency,scale,scale,416,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:215,modifiability,modul,module,215,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:416,modifiability,scal,scale,416,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:416,performance,scale,scale,416,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:215,safety,modul,module,215,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:728,safety,prevent,prevent,728,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:728,security,preven,prevent,728,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:245,testability,understand,understanding,245,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:310,usability,tool,tool-reference,310,"I think it would be generally useful to have some methods for re-weighting graphs (some discussion in #1561), I'm not sure if keyword arguments per function is the right way to do this. Maybe something in a `graph` module or `preprocessing`? My understanding ([source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-spatial-autocorrelation-moran-s-i-spatial-st.htm)) was the way to scale weights for Morans I is to be sure the edge weights are between 0 and 1. Is this right? This should already be the case for umap based connectivities. Is this the graph you were using @Hrovatin? I would definitely like to see the data you used here. Btw, you could use `maxabs_scale` on a sparse matrix to prevent densification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:153,availability,sli,slide,153,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:475,energy efficiency,current,currently,475,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:218,interoperability,specif,specific,218,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:232,modifiability,scal,scaling,232,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:357,modifiability,scal,scaling,357,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:423,modifiability,variab,variability,423,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:544,performance,time,times,544,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:153,reliability,sli,slide,153,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:192,usability,clear,clear,192,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:709,usability,user,user-images,709,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:893,usability,user,user-images,893,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:. I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode. <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:. <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:10,interoperability,share,share,10,Could you share the dataset?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:78,availability,down,down,78,"Ha, no I'm not on the mattermost. ivirshup@gmail.com. Though if you could cut down the dataset to something small and more shareable that would be helpful as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:123,interoperability,share,shareable,123,"Ha, no I'm not on the mattermost. ivirshup@gmail.com. Though if you could cut down the dataset to something small and more shareable that would be helpful as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:147,usability,help,helpful,147,"Ha, no I'm not on the mattermost. ivirshup@gmail.com. Though if you could cut down the dataset to something small and more shareable that would be helpful as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:6,availability,replic,replicate,6,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1788,availability,consist,consistent,1788,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1955,availability,consist,consistent,1955,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:243,deployability,scale,scale,243,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:243,energy efficiency,scale,scale,243,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:925,integrability,sub,subsample,925,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1259,integrability,sub,sub,1259,"ts you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1290,integrability,sub,sub,1290,"would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1333,integrability,sub,sub,1333," issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in ran",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1391,integrability,sub,sub,1391,"into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:243,modifiability,scal,scale,243,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:548,modifiability,variab,variables,548,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:568,modifiability,variab,variable,568,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:712,modifiability,variab,variables,712,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:942,modifiability,variab,variables,942,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1019,modifiability,variab,variables,1019," so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercont",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1556,modifiability,variab,variables,1556,"e 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:243,performance,scale,scale,243,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:354,performance,parallel,parallelization,354,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:2450,performance,memor,memory,2450,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:301,usability,help,helpful,301,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>. <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1788,usability,consist,consistent,1788,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1955,usability,consist,consistent,1955,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:1998,usability,user,user-images,1998,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:2128,usability,Progress,Progress,2128,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:2138,usability,Minim,Minimal,2138,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:2450,usability,memor,memory,2450,"ho's value changes. * This is especially weird since all values are changing if I run the function on the full set of variables. * If I use a smaller interval size (10), I don't get any varying results. * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent? -----------------. This is so weird. ```python. import scanpy as sc. import numpy as np. import pandas as pd. from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):. sub = X[idx]. result = np.ones(sub.shape[0], dtype=bool). first = func(g, sub). for i in range(tries):. result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each. samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while. results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({. ""var_idx"": samples.flatten(),. ""consistent"": results.flatten(), . ""sample"": np.repeat(np.arange(adata.n_vars), 100),. ""order"": np.tile(np.arange(100), adata.n_vars),. }). df.groupby(""order"").mean()[""consistent""].plot(). ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress! Minimal reproducer:. ```python. pbmc = sc.datasets.pbmc68k_reduced(). pbmc = pbmc[:411].copy(). sc.pp.neighbors(pbmc). res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]). ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would make sense for an alignment issue).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:101,availability,error,error,101,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:407,availability,error,error,407,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:281,integrability,filter,filter,281,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:22,modifiability,variab,variables,22,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:101,performance,error,error,101,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:407,performance,error,error,407,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:101,safety,error,error,101,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:407,safety,error,error,407,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:101,usability,error,error,101,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:407,usability,error,error,407,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python. np.where((adata.X[[0], :] == adata.X).all(axis=0)). ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error? --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:53,availability,consist,consistencies,53,"Thank you! This data is from Compass (they term this consistencies). They are negative, but as you already know the same happened when scaling to [0,1] per feature. . Will remove those constant values. I think the result in such a case could just contain nans and emit a warning. p.s.: I really like how you document everything you do so nicely 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:247,deployability,contain,contain,247,"Thank you! This data is from Compass (they term this consistencies). They are negative, but as you already know the same happened when scaling to [0,1] per feature. . Will remove those constant values. I think the result in such a case could just contain nans and emit a warning. p.s.: I really like how you document everything you do so nicely 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:135,modifiability,scal,scaling,135,"Thank you! This data is from Compass (they term this consistencies). They are negative, but as you already know the same happened when scaling to [0,1] per feature. . Will remove those constant values. I think the result in such a case could just contain nans and emit a warning. p.s.: I really like how you document everything you do so nicely 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:53,usability,consist,consistencies,53,"Thank you! This data is from Compass (they term this consistencies). They are negative, but as you already know the same happened when scaling to [0,1] per feature. . Will remove those constant values. I think the result in such a case could just contain nans and emit a warning. p.s.: I really like how you document everything you do so nicely 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:308,usability,document,document,308,"Thank you! This data is from Compass (they term this consistencies). They are negative, but as you already know the same happened when scaling to [0,1] per feature. . Will remove those constant values. I think the result in such a case could just contain nans and emit a warning. p.s.: I really like how you document everything you do so nicely 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:135,availability,consist,consistently,135,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:254,availability,avail,available,254,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:47,deployability,contain,contain,47,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:417,deployability,log,logic,417,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:264,energy efficiency,core,cores,264,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:254,reliability,availab,available,254,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:254,safety,avail,available,254,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:417,safety,log,logic,417,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:681,safety,reme,remember,681,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:254,security,availab,available,254,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:417,security,log,logic,417,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:417,testability,log,logic,417,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:135,usability,consist,consistently,135,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:614,usability,document,document,614,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:268,deployability,modul,module,268,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:311,energy efficiency,model,models,311,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:466,energy efficiency,model,models,466,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:268,modifiability,modul,module,268,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:295,modifiability,paramet,parametric,295,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:450,modifiability,paramet,parametric,450,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:495,modifiability,paramet,parametric,495,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:66,safety,Hot,Hotspot,66,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:268,safety,modul,module,268,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:275,safety,detect,detection,275,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:410,safety,test,testing,410,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:275,security,detect,detection,275,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:311,security,model,models,311,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:325,security,sign,significances,325,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:372,security,sign,significantly,372,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:466,security,model,models,466,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1698:410,testability,test,testing,410,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698
https://github.com/scverse/scanpy/issues/1696:15,deployability,updat,update,15,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:15,safety,updat,update,15,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:15,security,updat,update,15,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:231,availability,down,downgrading,231,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:501,availability,error,errors,501,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:221,deployability,upgrad,upgrading,221,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:251,deployability,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:658,deployability,updat,update,658,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:509,energy efficiency,Current,Currently,509,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:251,integrability,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:221,modifiability,upgrad,upgrading,221,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:251,modifiability,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:681,modifiability,pac,package,681,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:501,performance,error,errors,501,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:185,reliability,doe,doesn,185,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:251,safety,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:501,safety,error,errors,501,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:658,safety,updat,update,658,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:672,safety,compl,complete,672,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:658,security,updat,update,658,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:672,security,compl,complete,672,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:251,testability,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:501,usability,error,errors,501,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:596,usability,learn,learn,596,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:12,deployability,version,version,12,what python version are you running also btw? I remember there were issues with 0.52 on 3.9 and so they put out an RC.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:12,integrability,version,version,12,what python version are you running also btw? I remember there were issues with 0.52 on 3.9 and so they put out an RC.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:12,modifiability,version,version,12,what python version are you running also btw? I remember there were issues with 0.52 on 3.9 and so they put out an RC.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:48,safety,reme,remember,48,what python version are you running also btw? I remember there were issues with 0.52 on 3.9 and so they put out an RC.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:45,availability,error,error,45,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:6,deployability,instal,install,6,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:201,deployability,instal,install,201,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:253,deployability,instal,install,253,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:319,deployability,log,logging,319,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1776,deployability,log,logical,1776,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1830,deployability,updat,updated,1830,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1784,energy efficiency,CPU,CPU,1784,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1788,energy efficiency,core,cores,1788,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:584,modifiability,deco,decorator,584,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1013,modifiability,pac,packaging,1013,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1645,modifiability,pac,packaged,1645,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:45,performance,error,error,45,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:939,performance,network,networkx,939,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1784,performance,CPU,CPU,1784,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:45,safety,error,error,45,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:319,safety,log,logging,319,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1776,safety,log,logical,1776,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1830,safety,updat,updated,1830,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:319,security,log,logging,319,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:939,security,network,networkx,939,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1776,security,log,logical,1776,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1810,security,Session,Session,1810,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1830,security,updat,updated,1830,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:319,testability,log,logging,319,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:1776,testability,log,logical,1776,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:45,usability,error,error,45,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:137,usability,learn,learn,137,"Fresh install in a new env gives me the same error (jupyter kernel crashes):. ```. conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables. conda activate squidpy. conda install -c conda-forge leidenalg python-igraph. pip install scanpy squidpy imctools stardist. ```. And here's the `sc.logging.print_versions()`:. ```. -----. anndata 0.7.5. scanpy 1.7.1. sinfo 0.3.1. -----. PIL 8.1.2. anndata 0.7.5. asciitree NA. backcall 0.2.0. cairo 1.20.0. cffi 1.14.5. cmocean 2.0. constants NA. cycler 0.10.0. cython_runtime NA. dask 2021.03.0. dateutil 2.8.1. decorator 4.4.2. docrep 0.3.2. fasteners NA. get_version 2.1. h5py 2.10.0. highs_wrapper NA. igraph 0.8.3. imagecodecs 2020.12.24. imageio 2.9.0. ipykernel 5.5.0. ipython_genutils 0.2.0. ipywidgets 7.6.3. jedi 0.18.0. joblib 1.0.1. kiwisolver 1.3.1. legacy_api_wrap 1.2. leidenalg 0.8.3. llvmlite 0.35.0. matplotlib 3.3.4. mpl_toolkits NA. natsort 7.1.1. networkx 2.5. numba 0.52.0. numcodecs 0.7.3. numexpr 2.7.3. numpy 1.20.1. packaging 20.9. pandas 1.2.3. parso 0.8.1. pexpect 4.8.0. pickleshare 0.7.5. pkg_resources NA. prompt_toolkit 3.0.17. ptyprocess 0.7.0. pycparser 2.20. pygments 2.8.1. pyparsing 2.4.7. pytz 2021.1. pywt 1.1.1. scanpy 1.7.1. scipy 1.6.0. seaborn 0.11.1. sinfo 0.3.1. six 1.15.0. skimage 0.18.1. sklearn 0.24.1. squidpy 1.0.0. statsmodels 0.12.2. storemagic NA. tables 3.6.1. texttable 1.6.3. tifffile 2021.3.5. tornado 6.1. traitlets 5.0.5. typing_extensions NA. wcwidth 0.2.5. xarray 0.17.0. yaml 5.4.1. zarr 2.6.1. zmq 22.0.3. -----. IPython 7.21.0. jupyter_client 6.1.11. jupyter_core 4.7.1. notebook 6.2.0. -----. Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]. Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10. 72 logical CPU cores, x86_64. -----. Session information updated at 2021-03-12 11:42. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:497,testability,trace,traceback,497,"thanks @wflynny ! I just repeated exactly your snippet for fresh conda env and am not able to reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20). >>> adata. AnnData object with n_obs × n_vars = 2730 × 3451. obs: 'paul15_clusters'. uns: 'iroot', 'neighbors'. obsm: 'X_pca'. obsp: 'distances', 'connectivities'. ```. I also tried with `sc.datasets.pbmc3k_processed()` and still get it to run. Can you both please paste the full traceback? also maybe some more infos on the anndata would be useful? thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:133,availability,error,errors,133,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:204,interoperability,specif,specific,204,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:133,performance,error,errors,133,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:133,safety,error,errors,133,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:133,usability,error,errors,133,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,deployability,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:740,deployability,log,lognormed,740,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:96,integrability,transform,transforming,96,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,integrability,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:96,interoperability,transform,transforming,96,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,modifiability,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:710,modifiability,layer,layers,710,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,safety,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:740,safety,log,lognormed,740,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:740,security,log,lognormed,740,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,testability,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:740,testability,log,lognormed,740,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:. ```{python}. AnnData object with n_obs × n_vars = 68865 × 29. obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'. var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'. uns: 'spatial', 'log1p', 'pca',. obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'. varm: 'PCs'. layers: 'cleaned', 'normed', 'lognormed'. ```. I will probably raise this with `pynndescent` then because. ```. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes. sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes. ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:489,availability,ping,pinging,489,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,deployability,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:98,integrability,transform,transforming,98,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,integrability,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:98,interoperability,transform,transforming,98,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,modifiability,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,safety,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,testability,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:281,availability,redund,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:309,availability,reliab,reliably,309,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:281,deployability,redundan,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:237,integrability,transform,transform,237,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:180,interoperability,distribut,distributions,180,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:237,interoperability,transform,transform,237,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:225,reliability,stabil,stabilizing,225,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:281,reliability,redundan,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:309,reliability,reliab,reliably,309,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:121,safety,compl,completely,121,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:281,safety,redund,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:121,security,compl,completely,121,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:278,availability,redund,redundant,278,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:278,deployability,redundan,redundant,278,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:234,integrability,transform,transform,234,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:75,interoperability,distribut,distributions,75,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:234,interoperability,transform,transform,234,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:127,reliability,doe,does,127,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:222,reliability,stabil,stabilizing,222,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:278,reliability,redundan,redundant,278,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:16,safety,compl,completely,16,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:141,safety,compl,completely,141,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:278,safety,redund,redundant,278,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:16,security,compl,completely,16,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:141,security,compl,completely,141,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:173,security,ident,identical,173,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ? > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:722,interoperability,standard,standard,722,"Yes, we had what seems like an identical copy of one channel copied to another channel (which should have been empty). This appears to be an issue with the acquisition/initial file generation, but honestly I'm still scratching my head how this happened. And yeah, this vst is new to me too. At least in my experience the mean-variance relationship with scRNA-seq is roughly quadratic whereas at least empirically IMC data has a quadratic relationship at high mean expression but is dominated by a noise term at low mean expression, which leads to a `y ~ asinh(cofactor * x)`. And at least [others in the field](https://www.nature.com/articles/s43018-020-0026-6#Sec10) have reached the same conclusion. FWIW, this might be standard knowledge in other fields, but was at least new to me (mostly deal with poisson or (negative-)binomial data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:803,reliability,poisson,poisson,803,"Yes, we had what seems like an identical copy of one channel copied to another channel (which should have been empty). This appears to be an issue with the acquisition/initial file generation, but honestly I'm still scratching my head how this happened. And yeah, this vst is new to me too. At least in my experience the mean-variance relationship with scRNA-seq is roughly quadratic whereas at least empirically IMC data has a quadratic relationship at high mean expression but is dominated by a noise term at low mean expression, which leads to a `y ~ asinh(cofactor * x)`. And at least [others in the field](https://www.nature.com/articles/s43018-020-0026-6#Sec10) have reached the same conclusion. FWIW, this might be standard knowledge in other fields, but was at least new to me (mostly deal with poisson or (negative-)binomial data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:31,security,ident,identical,31,"Yes, we had what seems like an identical copy of one channel copied to another channel (which should have been empty). This appears to be an issue with the acquisition/initial file generation, but honestly I'm still scratching my head how this happened. And yeah, this vst is new to me too. At least in my experience the mean-variance relationship with scRNA-seq is roughly quadratic whereas at least empirically IMC data has a quadratic relationship at high mean expression but is dominated by a noise term at low mean expression, which leads to a `y ~ asinh(cofactor * x)`. And at least [others in the field](https://www.nature.com/articles/s43018-020-0026-6#Sec10) have reached the same conclusion. FWIW, this might be standard knowledge in other fields, but was at least new to me (mostly deal with poisson or (negative-)binomial data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:306,usability,experien,experience,306,"Yes, we had what seems like an identical copy of one channel copied to another channel (which should have been empty). This appears to be an issue with the acquisition/initial file generation, but honestly I'm still scratching my head how this happened. And yeah, this vst is new to me too. At least in my experience the mean-variance relationship with scRNA-seq is roughly quadratic whereas at least empirically IMC data has a quadratic relationship at high mean expression but is dominated by a noise term at low mean expression, which leads to a `y ~ asinh(cofactor * x)`. And at least [others in the field](https://www.nature.com/articles/s43018-020-0026-6#Sec10) have reached the same conclusion. FWIW, this might be standard knowledge in other fields, but was at least new to me (mostly deal with poisson or (negative-)binomial data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:53,availability,down,down,53,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:200,deployability,updat,update,200,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:200,safety,updat,update,200,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:200,security,updat,update,200,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:168,integrability,transform,transformation,168,"Just skimmed this very briefly. Isn't there usually a background effect that is removed for CyTOF or IMC, so that you get negative values and therefore need an arcsinh transformation? This also leads to negative values i guess. Could this be an issue for `pynndescent`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:168,interoperability,transform,transformation,168,"Just skimmed this very briefly. Isn't there usually a background effect that is removed for CyTOF or IMC, so that you get negative values and therefore need an arcsinh transformation? This also leads to negative values i guess. Could this be an issue for `pynndescent`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:385,availability,error,error,385,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:391,availability,reliab,reliably,391,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:41,deployability,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:549,deployability,observ,observations,549,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:467,energy efficiency,Current,Currently,467,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:41,integrability,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:252,integrability,rout,routinely,252,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:345,integrability,sub,subset,345,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:408,interoperability,share,share,408,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:41,modifiability,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:385,performance,error,error,385,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:391,reliability,reliab,reliably,391,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:41,safety,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:385,safety,error,error,385,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:41,testability,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:549,testability,observ,observations,549,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:385,usability,error,error,385,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:83,availability,error,error,83,Sorry for the late reply. The issue seems to only occur in Win10 but not Linux. No error on linux using the exact same codes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:83,performance,error,error,83,Sorry for the late reply. The issue seems to only occur in Win10 but not Linux. No error on linux using the exact same codes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:83,safety,error,error,83,Sorry for the late reply. The issue seems to only occur in Win10 but not Linux. No error on linux using the exact same codes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1696:83,usability,error,error,83,Sorry for the late reply. The issue seems to only occur in Win10 but not Linux. No error on linux using the exact same codes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696
https://github.com/scverse/scanpy/issues/1697:24,deployability,version,version,24,AFAIK the newest Pandas version is also 3.7+ already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:24,integrability,version,version,24,AFAIK the newest Pandas version is also 3.7+ already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:24,modifiability,version,version,24,AFAIK the newest Pandas version is also 3.7+ already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:17,deployability,modul,modules,17,"`__getattr__` on modules is in 3.7 already? nice! And yeah, `from __future__ import annotations` is going to make a lot of import order sensitive submodules less … that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:146,integrability,sub,submodules,146,"`__getattr__` on modules is in 3.7 already? nice! And yeah, `from __future__ import annotations` is going to make a lot of import order sensitive submodules less … that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:17,modifiability,modul,modules,17,"`__getattr__` on modules is in 3.7 already? nice! And yeah, `from __future__ import annotations` is going to make a lot of import order sensitive submodules less … that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:17,safety,modul,modules,17,"`__getattr__` on modules is in 3.7 already? nice! And yeah, `from __future__ import annotations` is going to make a lot of import order sensitive submodules less … that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:53,deployability,fail,failing,53,random point but in #1715 noticed that 3.6 tests are failing due to missing `typing.Literal` import. We could fix it ad hoc but otherwise other good reason to drop 3.6,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:53,reliability,fail,failing,53,random point but in #1715 noticed that 3.6 tests are failing due to missing `typing.Literal` import. We could fix it ad hoc but otherwise other good reason to drop 3.6,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:43,safety,test,tests,43,random point but in #1715 noticed that 3.6 tests are failing due to missing `typing.Literal` import. We could fix it ad hoc but otherwise other good reason to drop 3.6,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:43,testability,test,tests,43,random point but in #1715 noticed that 3.6 tests are failing due to missing `typing.Literal` import. We could fix it ad hoc but otherwise other good reason to drop 3.6,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:20,usability,custom,custom,20,"@giovp we do have a custom `Literal` thing in `compat` for this, but agree it would be nice to not have this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:89,availability,reliab,reliably,89,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:110,availability,failur,failures,110,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:77,deployability,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:110,deployability,fail,failures,110,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:127,deployability,updat,updates,127,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:77,integrability,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:77,modifiability,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:110,performance,failur,failures,110,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:150,performance,time,time,150,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:89,reliability,reliab,reliably,89,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:110,reliability,fail,failures,110,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:77,safety,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:105,safety,test,test,105,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:127,safety,updat,updates,127,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:127,security,updat,updates,127,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:77,testability,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:105,testability,test,test,105,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:31,usability,support,support,31,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:99,deployability,build,build,99,"FYI having seen this I've been trying out conda deps, and multicore t-sne doesn't currently have a build for > 3.6, just so you know. I'm seeing if I can sort it out, but I'm not a conda forge wizard (see https://github.com/conda-forge/multicore-tsne-feedstock/pull/6).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:82,energy efficiency,current,currently,82,"FYI having seen this I've been trying out conda deps, and multicore t-sne doesn't currently have a build for > 3.6, just so you know. I'm seeing if I can sort it out, but I'm not a conda forge wizard (see https://github.com/conda-forge/multicore-tsne-feedstock/pull/6).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:74,reliability,doe,doesn,74,"FYI having seen this I've been trying out conda deps, and multicore t-sne doesn't currently have a build for > 3.6, just so you know. I'm seeing if I can sort it out, but I'm not a conda forge wizard (see https://github.com/conda-forge/multicore-tsne-feedstock/pull/6).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/issues/1697:193,usability,wizard,wizard,193,"FYI having seen this I've been trying out conda deps, and multicore t-sne doesn't currently have a build for > 3.6, just so you know. I'm seeing if I can sort it out, but I'm not a conda forge wizard (see https://github.com/conda-forge/multicore-tsne-feedstock/pull/6).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697
https://github.com/scverse/scanpy/pull/1699:93,availability,error,errors,93,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:26,deployability,fail,fail,26,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:93,performance,error,errors,93,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:26,reliability,fail,fail,26,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:93,safety,error,errors,93,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:93,usability,error,errors,93,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:26,deployability,fail,failing,26,"I vaguely remember seeing failing plots like this before. @fidelram do you remember what was causing this? Worst case, we can say this is a `1.8` fix not a `1.7.2` fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:26,reliability,fail,failing,26,"I vaguely remember seeing failing plots like this before. @fidelram do you remember what was causing this? Worst case, we can say this is a `1.8` fix not a `1.7.2` fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:10,safety,reme,remember,10,"I vaguely remember seeing failing plots like this before. @fidelram do you remember what was causing this? Worst case, we can say this is a `1.8` fix not a `1.7.2` fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1699:75,safety,reme,remember,75,"I vaguely remember seeing failing plots like this before. @fidelram do you remember what was causing this? Worst case, we can say this is a `1.8` fix not a `1.7.2` fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699
https://github.com/scverse/scanpy/pull/1700:133,modifiability,concern,concerned,133,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:157,safety,test,tests,157,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:190,safety,test,tests,190,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:133,testability,concern,concerned,133,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:157,testability,test,tests,157,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:190,testability,test,tests,190,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:95,availability,restor,restore,95,"Done! For the record: To reopen a PR whose base branch was deleted (here: `flit`), you need to restore the base branch, if only to switch the base and delete it again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/pull/1700:95,reliability,restor,restore,95,"Done! For the record: To reopen a PR whose base branch was deleted (here: `flit`), you need to restore the base branch, if only to switch the base and delete it again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700
https://github.com/scverse/scanpy/issues/1701:156,deployability,depend,dependencies,156,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:156,integrability,depend,dependencies,156,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:156,modifiability,depend,dependencies,156,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:102,reliability,doe,does,102,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:156,safety,depend,dependencies,156,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:156,testability,depend,dependencies,156,"Can you show us the values of `combined_bbknn.obs['scNym']`? Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:360,availability,down,downgraded,360,"Hi, . Thanks for the quick reply! I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:264,safety,test,tested,264,"Hi, . Thanks for the quick reply! I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:264,testability,test,tested,264,"Hi, . Thanks for the quick reply! I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:140,usability,user,user-images,140,"Hi, . Thanks for the quick reply! I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:76,availability,error,errors,76,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:110,availability,consist,consistent,110,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:76,performance,error,errors,76,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:76,safety,error,errors,76,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:76,usability,error,errors,76,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:110,usability,consist,consistent,110,"Do you know how that entry could have been filled with `NaN`? The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:237,availability,error,errors,237,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:539,deployability,upgrad,upgraded,539,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:426,modifiability,variab,variable,426,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:539,modifiability,upgrad,upgraded,539,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:237,performance,error,errors,237,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:237,safety,error,errors,237,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:237,usability,error,errors,237,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:315,usability,command,command,315,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:166,availability,avail,available,166,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:157,integrability,pub,publicly,157,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:166,reliability,availab,available,166,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:166,safety,avail,available,166,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:166,security,availab,available,166,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:27,usability,minim,minimum,27,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:45,usability,command,commands,45,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:108,usability,help,helpful,108,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:251,usability,minim,minimal-bug-reports,251,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:231,deployability,updat,update,231,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:33,modifiability,concern,concerning,33,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:231,safety,updat,update,231,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:231,security,updat,update,231,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:33,testability,concern,concerning,33,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:194,availability,operat,operation,194,@cartal this seems like a merge/concat gone wrong and indexes got messed up. happens to me a lot. I'm gonna close this because as @ivirshup mentioned the problem is proabbly due to some anndata operation rather than the plotting itself. feel free to reopen this or open a new issue in case this is still a problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/issues/1701:108,usability,close,close,108,@cartal this seems like a merge/concat gone wrong and indexes got messed up. happens to me a lot. I'm gonna close this because as @ivirshup mentioned the problem is proabbly due to some anndata operation rather than the plotting itself. feel free to reopen this or open a new issue in case this is still a problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701
https://github.com/scverse/scanpy/pull/1702:593,availability,echo,echo,593,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:75,deployability,continu,continuumio,75,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:108,deployability,instal,install,108,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:136,deployability,instal,install,136,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:368,deployability,instal,install,368,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:412,deployability,instal,install,412,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:551,deployability,version,version,551,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:566,deployability,version,version,566,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:712,deployability,Instal,Install,712,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:733,deployability,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:760,deployability,instal,install,760,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:805,deployability,instal,install,805,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:551,integrability,version,version,551,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:566,integrability,version,version,566,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:733,integrability,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:551,modifiability,version,version,551,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:566,modifiability,version,version,566,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:667,modifiability,pac,packages,667,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:733,modifiability,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:856,modifiability,pac,packages,856,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:629,safety,test,test,629,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:733,safety,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:818,safety,test,test,818,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:629,testability,test,test,629,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:733,testability,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1702:818,testability,test,test,818,"Just checked using this dockerfile, works flawlessly:. ```dockerfile. FROM continuumio/miniconda. RUN conda install python=3.8. RUN pip install flit>=3.1. RUN git clone https://github.com/theislab/scanpy.git. WORKDIR /scanpy. # Go to the mainline-pip branch if it hasn’t been merged into master yet. RUN git checkout mainline-pip || true. RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy. # Make sure the dist-info folder has a plus in its name. RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \. echo $SCANPY_VERSION | grep '+' &&. test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info. # Install project that depends on scanpy. RUN pip install scvelo. # Make sure it’s still a dev install. RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702
https://github.com/scverse/scanpy/pull/1703:127,deployability,releas,releases,127,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:31,safety,compl,completely,31,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:31,security,compl,completely,31,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:69,usability,User,Users,69,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:98,deployability,version,versioning,98,"Sounds good! I was wondering if that's what this is about. We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:98,integrability,version,versioning,98,"Sounds good! I was wondering if that's what this is about. We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:89,interoperability,semant,semantic,89,"Sounds good! I was wondering if that's what this is about. We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:98,modifiability,version,versioning,98,"Sounds good! I was wondering if that's what this is about. We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:41,deployability,version,versioning,41,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:244,deployability,fail,failing,244,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:41,integrability,version,versioning,41,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:32,interoperability,semant,semantic,32,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:41,modifiability,version,versioning,41,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/pull/1703:244,reliability,fail,failing,244,"> We're trying to be good about semantic versioning, but it should be fine to merge this with the master branch once it's done (all 1.7.x stuff gets back ported to the 1.7.x branch, so we just won't backport this). Great! I'm not sure why it's failing though. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703
https://github.com/scverse/scanpy/issues/1705:71,usability,user,user-images,71,"Happens with `seaborn` too:. <img width=""564"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/109771372-d6542a00-7c50-11eb-8cd0-96fd01504464.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1705
https://github.com/scverse/scanpy/issues/1706:514,deployability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,integrability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:425,interoperability,share,share,425,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,interoperability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,modifiability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,reliability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:288,safety,prevent,prevent,288,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:288,security,preven,prevent,288,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,security,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:363,testability,simpl,simplexes,363,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:514,testability,integr,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:363,usability,simpl,simplexes,363,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:475,usability,behavi,behaviour,475,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:558,usability,behavi,behavior,558,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:662,usability,behavi,behavior,662,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with. * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1002,availability,error,error,1002,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1008,integrability,messag,message,1008,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1008,interoperability,messag,message,1008,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1002,performance,error,error,1002,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1002,safety,error,error,1002,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:1002,usability,error,error,1002,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. ```python. import scanpy as sc. adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:. sc.pp.neighbors(adata, n_neighbors=n_neighbors). print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A). ```. Output:. ```. n_neighbors = 1:. [[0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]. [0. 0. 0. 0. 0.]]. n_neighbors = 2:. [[0. 0. 0. 1. 0.]. [0. 0. 1. 0. 0.]. [0. 1. 0. 0. 0.]. [1. 0. 0. 0. 1.]. [0. 0. 0. 1. 0.]]. n_neighbors = 3:. [[0. 0.5849553 0. 1. 0.5849636 ]. [0.5849553 0. 1. 0.5849678 0. ]. [0. 1. 0. 0.58496827 0. ]. [1. 0.5849678 0.58496827 0. 1. ]. [0.5849636 0. 0. 1. 0. ]]. ```. It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:447,availability,error,error,447,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:447,performance,error,error,447,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:465,reliability,doe,does,465,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:447,safety,error,error,447,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:447,usability,error,error,447,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1706:487,usability,document,document,487,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc68k_reduced(). sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat). # array([14]). ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706
https://github.com/scverse/scanpy/issues/1708:8,security,control,control,8,"You can control which values `pandas.read_csv` reads in as `NaN` from text files with the arguments `na_filter`, `keep_default_na`, and `na_values`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708
https://github.com/scverse/scanpy/issues/1708:8,testability,control,control,8,"You can control which values `pandas.read_csv` reads in as `NaN` from text files with the arguments `na_filter`, `keep_default_na`, and `na_values`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708
https://github.com/scverse/scanpy/issues/1708:100,usability,help,helped,100,Thank you @Knievl ! Had the exact same problem ! . Changing the gene name in the features.tsv files helped solve this issue. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1708
https://github.com/scverse/scanpy/issues/1714:37,deployability,version,version,37,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714
https://github.com/scverse/scanpy/issues/1714:100,deployability,releas,releases,100,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714
https://github.com/scverse/scanpy/issues/1714:37,integrability,version,version,37,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714
https://github.com/scverse/scanpy/issues/1714:37,modifiability,version,version,37,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714
https://github.com/scverse/scanpy/pull/1715:145,interoperability,conflict,conflict,145,"thanks @jlause ! Really excited for this! shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:114,reliability,doe,does,114,"thanks @jlause ! Really excited for this! shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:65,safety,review,reviewing,65,"thanks @jlause ! Really excited for this! shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:65,testability,review,reviewing,65,"thanks @jlause ! Really excited for this! shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:283,deployability,manag,management,283,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:186,energy efficiency,cool,cool,186,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:283,energy efficiency,manag,management,283,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:149,interoperability,conflict,conflict,149,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:277,modifiability,layer,layer,277,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:118,reliability,doe,does,118,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:69,safety,review,reviewing,69,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:283,safety,manag,management,283,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:69,testability,review,reviewing,69,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:461,testability,simpl,simplify,461,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:216,usability,feedback,feedback,216,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:461,usability,simpl,simplify,461,"> thanks @jlause ! Really excited for this! > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this? > . > thank you! cool, looking forward to your feedback! I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:117,testability,simpl,simplify,117,"> it would make sense if I mirror the change in normalize_pearson_residuals(), right? I believe doing that will even simplify the function further. If ivirshup agrees, I could quickly do that :). Sounds good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:117,usability,simpl,simplify,117,"> it would make sense if I mirror the change in normalize_pearson_residuals(), right? I believe doing that will even simplify the function further. If ivirshup agrees, I could quickly do that :). Sounds good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:286,deployability,automat,automatically,286,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:286,testability,automat,automatically,286,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:141,usability,experien,experienced,141,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:496,usability,guid,guide,496,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:203,deployability,automat,automatically,203,"> One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. Could you share the output you're getting here? The output I'm seeing on the pre-commit CI looks pretty normal, so it'd be helpful to know more exactly what was happening locally. We've also just activated pre-commit, so it's a bit new to us too!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,interoperability,share,share,239,"> One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. Could you share the output you're getting here? The output I'm seeing on the pre-commit CI looks pretty normal, so it'd be helpful to know more exactly what was happening locally. We've also just activated pre-commit, so it's a bit new to us too!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:203,testability,automat,automatically,203,"> One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. Could you share the output you're getting here? The output I'm seeing on the pre-commit CI looks pretty normal, so it'd be helpful to know more exactly what was happening locally. We've also just activated pre-commit, so it's a bit new to us too!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:58,usability,experien,experienced,58,"> One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. Could you share the output you're getting here? The output I'm seeing on the pre-commit CI looks pretty normal, so it'd be helpful to know more exactly what was happening locally. We've also just activated pre-commit, so it's a bit new to us too!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:352,usability,help,helpful,352,"> One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. Could you share the output you're getting here? The output I'm seeing on the pre-commit CI looks pretty normal, so it'd be helpful to know more exactly what was happening locally. We've also just activated pre-commit, so it's a bit new to us too!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,deployability,version,version,202,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:253,deployability,automat,automatic,253,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,deployability,Fail,Failed,569,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:801,deployability,Fail,Failed,801,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:5387,deployability,contain,contains,5387,W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:542:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:543:79: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:546:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:547:79: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:550:80: E501 line too long (87 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:556:67: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:559:80: E501 line too long (87 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:560:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:560:89: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:563:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:564:1: W293 blank line contains whitespace. scanpy/preprocessing/_highly_variable_genes.py:565:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:569:1: W293 blank line contains whitespace. scanpy/preprocessing/_highly_variable_genes.py:571:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:572:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:572:89: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:575:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:576:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:579:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:584:80: E501 line too long (97 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:585:80: E501 line too long (86 > 79 characters). scanpy/preprocessing/_highly_variable_genes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:5574,deployability,contain,contains,5574,ing whitespace. scanpy/preprocessing/_highly_variable_genes.py:546:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:547:79: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:550:80: E501 line too long (87 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:556:67: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:559:80: E501 line too long (87 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:560:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:560:89: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:563:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:564:1: W293 blank line contains whitespace. scanpy/preprocessing/_highly_variable_genes.py:565:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:569:1: W293 blank line contains whitespace. scanpy/preprocessing/_highly_variable_genes.py:571:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:572:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:572:89: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:575:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:576:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:579:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:584:80: E501 line too long (97 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:585:80: E501 line too long (86 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:586:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:587:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:6883,deployability,contain,contains,6883,g/_highly_variable_genes.py:575:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:576:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:579:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:584:80: E501 line too long (97 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:585:80: E501 line too long (86 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:586:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:587:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:588:80: E501 line too long (90 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:589:76: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:590:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:592:1: W293 blank line contains whitespace. scanpy/preprocessing/_highly_variable_genes.py:596:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:608:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:613:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:618:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8098,deployability,automat,automatic,8098,"preprocessing/_highly_variable_genes.py:613:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:618:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8362,deployability,stage,staged,8362," > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8410,deployability,updat,update,8410,"variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10550,deployability,log,logg,10550,"s):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,integrability,version,version,202,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9911,integrability,sub,subset,9911,"npy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10116,integrability,batch,batches,10116," ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10375,integrability,sub,subset,10375,". - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11298,integrability,sub,subset,11298,"ariable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11911,integrability,sub,subset,11911,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11920,integrability,sub,subset,11920,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:12075,integrability,sub,subset,12075,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:12082,integrability,sub,subset,12082,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:12243,integrability,filter,filtered,12243,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,modifiability,version,version,202,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9742,modifiability,layer,layer,9742,"iable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10207,modifiability,layer,layer,10207,"seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10213,modifiability,layer,layer,10213,"_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Litera",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10235,modifiability,layer,layer,10235," @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10244,modifiability,layer,layer,10244," +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10932,modifiability,layer,layer,10932," inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inpl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11778,modifiability,layer,layer,11778,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11786,modifiability,layer,layer,11786,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11953,modifiability,layer,layer,11953,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:11959,modifiability,layer,layer,11959,"added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". - "" for `flavor='pearson_residuals'`"". + ""`pp.highly_variable_genes` requires the argument `n_top_genes`"". + "" for `flavor='pearson_residuals'`"". ). return _highly_variable_pearson_residuals(. adata,. - layer = layer,. - n_top_genes = n_top_genes,. - batch_key = batch_key,. - theta = theta,. - clip = clip,. - chunksize= chunksize,. - subset = subset,. - inplace = inplace,. + layer=layer,. + n_top_genes=n_top_genes,. + batch_key=batch_key,. + theta=theta,. + clip=clip,. + chunksize=chunksize,. + subset=subset,. + inplace=inplace,. ). - . . if batch_key is None:. df = _highly_variable_genes_single_batch(. ...skipping 1 line. . # Add 0 values for genes that were filtered out. missing_hvg = pd.DataFrame(. - np.zeros((np.sum(~filt), len(hvg.columns))), columns=hvg.columns,. + np.zeros((np.sum(~filt), len(hvg.columns))),. + columns=hvg.columns,. ). missing_hvg['highly_variable'] = missing_hvg['highly_variable'].astype(bool). missing_hvg['gene'] = gene_list[~filt]. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10116,performance,batch,batches,10116," ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,reliability,Fail,Failed,569,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:801,reliability,Fail,Failed,801,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,safety,test,test,57,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:482,safety,test,test,482,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8410,safety,updat,update,8410,"variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9155,safety,test,testedit,9155,":~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . +",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9168,safety,test,testedit,9168,"y/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10550,safety,log,logg,10550,"s):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:608,security,modif,modified,608,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8313,security,modif,modified,8313,"variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8410,security,updat,update,8410,"variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8518,security,modif,modified,8518,"s.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10550,security,log,logg,10550,"s):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,testability,test,test,57,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:253,testability,automat,automatic,253,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:482,testability,test,test,482,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:579,testability,hook,hook,579,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:625,testability,hook,hook,625,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:811,testability,hook,hook,811,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8098,testability,automat,automatic,8098,"preprocessing/_highly_variable_genes.py:613:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:618:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9155,testability,test,testedit,9155,":~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . +",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:9168,testability,test,testedit,9168,"y/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable_genes_seurat_v3(. adata: AnnData,. @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(. else:. clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape). np.putmask(. - batch_counts, batch_counts > clip_val_broad, clip_val_broad,. + batch_counts,. + batch_counts > clip_val_broad,. + clip_val_broad,. ). . if sp_sparse.issparse(batch_counts):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10550,testability,log,logg,10550,"s):. @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:327,usability,help,helps,327,"Hey @ivirshup,. below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit. Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code? hope this helps, let me know if you need anything else. <details>. <summary> </summary>. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit"". black....................................................................Failed. - hook id: black. - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py. All done! ✨ 🍰 ✨. 1 file reformatted. flake8...................................................................Failed. - hook id: flake8. - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace. scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'. scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'. scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8066,usability,statu,status,8066,"long (84 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:613:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:618:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. fro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:8200,usability,statu,status,8200,"rocessing/_highly_variable_genes.py:618:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:621:80: E501 line too long (89 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:623:80: E501 line too long (93 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters). scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status. On branch pearson_residuals_1.7. Changes to be committed:. (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:. (use ""git add <file>..."" to update what will be committed). (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:. (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py . diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py. index 03b01940..e2851f50 100644. --- a/scanpy/preprocessing/_highly_variable_genes.py. +++ b/scanpy/preprocessing/_highly_variable_genes.py. @@ -15,7 +15,8 @@ from ._utils import _get_mean_var. from ._distributed import materialize_as_ndarray. from ._simple import filter_genes. . -#testedit. +# testedit. +. . def _highly_variable",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:10555,usability,hint,hint,10555,"@@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(. df = df.drop(['highly_variable_nbatches'], axis=1). return df. . +. def _highly_variable_pearson_residuals(. adata: AnnData,. layer: Optional[str] = None,. @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 100,. subset: bool = False,. - inplace: bool = True. + inplace: bool = True,. ) -> Optional[pd.DataFrame]:. """"""\. See `highly_variable_genes`. @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(. in all batches. """""". . - view_to_actual(adata) . + view_to_actual(adata). X = _get_obs_rep(adata, layer=layer). computed_on = layer if layer else 'adata.X'. . @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(. df = df.loc[adata.var_names]. . if inplace or subset:. - adata.uns['hvg'] = {'flavor': 'pearson_residuals',. - 'computed_on':computed_on}. + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}. logg.hint(. 'added\n'. ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line. ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1. ). return df. - . - . - . +. . def _highly_variable_genes_single_batch(. adata: AnnData,. @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(. return df. . . -. def highly_variable_genes(. adata: AnnData,. layer: Optional[str] = None,. @@ -493,7 +493,9 @@ def highly_variable_genes(. theta: float = 100,. clip: Union[Literal['auto', 'none'], float] = 'auto',. chunksize: int = 1000,. - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',. + flavor: Literal[. + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'. + ] = 'seurat',. subset: bool = False,. inplace: bool = True,. batch_key: Optional[str] = None,. @@ -651,21 +653,20 @@ def highly_variable_genes(. if flavor == 'pearson_residuals':. if n_top_genes is None:. raise ValueError(. - ""`pp.highly_variable_genes` requires the argumen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:167,deployability,instal,installed,167,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:260,deployability,manag,manageable,260,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,deployability,log,logic,847,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:260,energy efficiency,manag,manageable,260,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:394,integrability,interfac,interface,394,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:628,integrability,interfac,interface,628,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:394,interoperability,interfac,interface,394,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:628,interoperability,interfac,interface,628,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:394,modifiability,interfac,interface,394,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:628,modifiability,interfac,interface,628,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:260,safety,manag,manageable,260,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,safety,log,logic,847,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,security,log,logic,847,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:158,testability,hook,hook,158,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,testability,log,logic,847,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:448,usability,user,user-images,448,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:746,usability,intuit,intuitive,746,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:169,deployability,instal,installed,169,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:268,deployability,manag,manageable,268,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:909,deployability,log,logic,909,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:268,energy efficiency,manag,manageable,268,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:402,integrability,interfac,interface,402,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:690,integrability,interfac,interface,690,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:402,interoperability,interfac,interface,402,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:690,interoperability,interfac,interface,690,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:402,modifiability,interfac,interface,402,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:690,modifiability,interfac,interface,690,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:268,safety,manag,manageable,268,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:909,safety,log,logic,909,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:909,security,log,logic,909,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:160,testability,hook,hook,160,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:909,testability,log,logic,909,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:808,usability,intuit,intuitive,808,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. > . > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! . The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:887,availability,error,errors,887,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:467,modifiability,required chang,required changes,467,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:217,performance,time,time,217,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:887,performance,error,errors,887,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:101,safety,review,review,101,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,safety,test,tests,239,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:373,safety,review,review,373,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:533,safety,test,tests,533,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:591,safety,test,tested,591,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:620,safety,test,tests,620,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:685,safety,test,test,685,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:708,safety,test,tests,708,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:744,safety,input,input,744,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:858,safety,test,tests,858,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:887,safety,error,errors,887,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:957,safety,input,input,957,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1026,safety,test,tested,1026,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:101,testability,review,review,101,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,testability,test,tests,239,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:373,testability,review,review,373,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:533,testability,test,tests,533,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:591,testability,test,tested,591,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:620,testability,test,tests,620,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:685,testability,test,test,685,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:708,testability,test,tests,708,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:858,testability,test,tests,858,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1026,testability,test,tested,1026,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:397,usability,help,helpful,397,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:558,usability,guid,guidelines,558,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:744,usability,input,input,744,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:887,usability,error,errors,887,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:957,usability,input,input,957,"> thank you @jlause for the PR! This is really exciting and super useful! > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ? > please let me know if anything unclear and also thanks in advance for code explanations! Hey @giovp ,. thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:179,availability,error,errors,179,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:738,availability,consist,consistent,738,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:753,availability,robust,robust,753,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:896,deployability,releas,release,896,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:686,interoperability,specif,specific,686,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:179,performance,error,errors,179,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:753,reliability,robust,robust,753,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2,safety,test,tests,2,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:38,safety,input,input,38,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:150,safety,test,tests,150,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:179,safety,error,errors,179,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:249,safety,input,input,249,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:396,safety,test,tested,396,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:753,safety,robust,robust,753,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2,testability,test,tests,2,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:150,testability,test,tests,150,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:396,testability,test,tested,396,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:38,usability,input,input,38,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:179,usability,error,errors,179,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:249,usability,input,input,249,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:738,usability,consist,consistent,738,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ). another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:276,availability,error,errors,276,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,availability,consist,consistent,847,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:862,availability,robust,robust,862,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1087,deployability,releas,release,1087,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:795,interoperability,specif,specific,795,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:276,performance,error,errors,276,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:862,reliability,robust,robust,862,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:39,safety,review,review,39,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:95,safety,test,tests,95,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:131,safety,input,input,131,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:247,safety,test,tests,247,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:276,safety,error,errors,276,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:346,safety,input,input,346,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:499,safety,test,tested,499,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:862,safety,robust,robust,862,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:975,safety,input,input,975,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1002,safety,test,tests,1002,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:39,testability,review,review,39,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:95,testability,test,tests,95,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:247,testability,test,tests,247,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:499,testability,test,tested,499,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1002,testability,test,tests,1002,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:131,usability,input,input,131,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:276,usability,error,errors,276,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:346,usability,input,input,346,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:847,usability,consist,consistent,847,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:975,usability,input,input,975,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function. > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). > . > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ). > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week. . > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:123,deployability,modul,module,123,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:833,interoperability,specif,specifically,833,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:123,modifiability,modul,module,123,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:390,reliability,doe,does,390,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:123,safety,modul,module,123,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:300,safety,review,review,300,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:971,safety,review,review,971,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:300,testability,review,review,300,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:971,testability,review,review,971,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:328,usability,clear,clear,328,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1315,deployability,log,logic,1315,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:160,energy efficiency,core,core,160,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:198,energy efficiency,core,core,198,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:258,energy efficiency,core,core,258,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1446,energy efficiency,model,model,1446,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:557,integrability,pub,published,557,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:787,integrability,pub,published,787,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:808,integrability,pub,published,808,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:839,integrability,pub,published,839,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:699,safety,review,review,699,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1315,safety,log,logic,1315,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1496,safety,review,review,1496,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:321,security,team,team,321,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1315,security,log,logic,1315,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1446,security,model,model,1446,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:699,testability,review,review,699,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:719,testability,understand,understand,719,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1315,testability,log,logic,1315,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1496,testability,review,review,1496,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that? Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here? One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1439,availability,avail,available,1439,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:782,deployability,continu,continuous,782,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:94,energy efficiency,core,core,94,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:132,energy efficiency,core,core,132,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1227,energy efficiency,core,core,1227,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:443,integrability,pub,published,443,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1455,performance,perform,performance,1455,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:669,reliability,doe,does,669,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1439,reliability,availab,available,1439,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:554,safety,review,reviewed,554,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1439,safety,avail,available,1439,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1239,security,team,team,1239,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1439,security,availab,available,1439,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:554,testability,review,reviewed,554,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1207,usability,guid,guidelines,1207,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1455,usability,perform,performance,1455,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings? > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,availability,incid,incidentally,57,Re deviance residuals -- this is a good a question (that incidentally we were *not* asked by the formal reviewers of our paper). We may want to look into it briefly @jlause.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,reliability,incid,incidentally,57,Re deviance residuals -- this is a good a question (that incidentally we were *not* asked by the formal reviewers of our paper). We may want to look into it briefly @jlause.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:104,safety,review,reviewers,104,Re deviance residuals -- this is a good a question (that incidentally we were *not* asked by the formal reviewers of our paper). We may want to look into it briefly @jlause.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:104,testability,review,reviewers,104,Re deviance residuals -- this is a good a question (that incidentally we were *not* asked by the formal reviewers of our paper). We may want to look into it briefly @jlause.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:124,availability,heal,healthy,124,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1148,deployability,modul,module,1148,"n it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1971,deployability,stage,stage,1971,"d be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull/823. - sctransform: https://github.com/theislab/scanpy/issues/1643. the first two attempts are imho dead, and for the last one is a bit too early to say and maybe it will just take a bit more time? in any case, ideally we'll get all of those, and more, to scanpy, but since this is free labour for everyone, it's very very (very) easy that these attempts ends up being last in the to do list of virtually everyone. So, my take is: let's get the `pearson residuals` from @jlause @dkobak in scanpy, and keep pushing to get the others metho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:378,energy efficiency,core,core,378,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:624,energy efficiency,core,core,624,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:85,integrability,coupl,couple,85,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:482,integrability,messag,messages,482,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:482,interoperability,messag,messages,482,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1263,interoperability,standard,standard,1263,"de the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) seve",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:85,modifiability,coupl,couple,85,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:577,modifiability,scal,scalability,577,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1148,modifiability,modul,module,1148,"n it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:577,performance,scalab,scalability,577,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1469,performance,time,time,1469," take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull/823. - sctransform: https://github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2629,performance,time,time,2629,"nt, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull/823. - sctransform: https://github.com/theislab/scanpy/issues/1643. the first two attempts are imho dead, and for the last one is a bit too early to say and maybe it will just take a bit more time? in any case, ideally we'll get all of those, and more, to scanpy, but since this is free labour for everyone, it's very very (very) easy that these attempts ends up being last in the to do list of virtually everyone. So, my take is: let's get the `pearson residuals` from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:156,reliability,doe,does,156,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1148,safety,modul,module,1148,"n it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1243,safety,review,review,1243,"out what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1376,safety,review,review,1376,"ore` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/iss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1432,safety,review,reviewed,1432,"mi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1278,security,legitim,legitimacy,1278,"py, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:85,testability,coupl,couple,85,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:521,testability,simpl,simplicity,521,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1243,testability,review,review,1243,"out what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1376,testability,review,review,1376,"ore` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/iss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1432,testability,review,reviewed,1432,"mi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:521,usability,simpl,simplicity,521,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1408,usability,person,personally,1408,"/github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1455,usability,tool,tools,1455,"s/1 , two key take home messages from that were (imho):. - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`. - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull/823. - sctransform: h",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:3047,usability,user,users,3047,"nt, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):. - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes). - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ? With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't possibly know about the discussion here https://github.com/berenslab/umi-normalization/issues/1 ). Another very important feature of this process though is that it needs to be **fast**. Normalization methods are actually a great example of why: there are (or has been) several attempts to provide more normalization methods in scanpy:. - glm-pca: https://github.com/theislab/scanpy/issues/868. - scran: https://github.com/theislab/scanpy/pull/823. - sctransform: https://github.com/theislab/scanpy/issues/1643. the first two attempts are imho dead, and for the last one is a bit too early to say and maybe it will just take a bit more time? in any case, ideally we'll get all of those, and more, to scanpy, but since this is free labour for everyone, it's very very (very) easy that these attempts ends up being last in the to do list of virtually everyone. So, my take is: let's get the `pearson residuals` from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1004,deployability,log,log,1004," everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2545,deployability,log,log,2545,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1725,energy efficiency,core,core,1725,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:228,interoperability,standard,standard,228,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:468,interoperability,standard,standard,468,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:434,performance,time,time,434,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1881,reliability,pra,practically,1881,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2161,reliability,doe,does,2161,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:208,safety,review,review,208,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:341,safety,review,review,341,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:397,safety,review,reviewed,397,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:445,safety,review,review,445,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:532,safety,review,reviewed,532,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1004,safety,log,log,1004," everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1326,safety,review,review,1326,"ion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2545,safety,log,log,2545,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:243,security,legitim,legitimacy,243,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1004,security,log,log,1004," everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2545,security,log,log,2545,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:208,testability,review,review,208,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:341,testability,review,review,341,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:397,testability,review,reviewed,397,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:445,testability,review,review,445,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:490,testability,understand,understand,490,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:532,testability,review,reviewed,532,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1004,testability,log,log,1004," everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1326,testability,review,review,1326,"ion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2134,testability,context,context,2134,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2307,testability,regress,regression,2307,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2545,testability,log,log,2545,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:373,usability,person,personally,373,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:420,usability,tool,tools,420,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ? Yes! > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1616,usability,user,users,1616,"process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials cove",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1623,usability,Person,Personally,1623,"ased on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1978,usability,workflow,workflows,1978,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2000,usability,clear,clear,2000,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2182,usability,workflow,workflow,2182,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2378,usability,clear,clear,2378,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2487,usability,visual,visualization,2487,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2516,usability,workflow,workflow,2516,"s of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both? > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found. 2. Should people be using these values for visualization? 3. What's the workflow if people need both log normalized and pearson residuals? 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,deployability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,deployability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:392,energy efficiency,core,core,392,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:649,energy efficiency,core,core,649,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,integrability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:740,integrability,pub,publication,740,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,integrability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,interoperability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,interoperability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,modifiability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:778,modifiability,exten,extend,778,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,modifiability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:344,reliability,doe,does,344,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,reliability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,reliability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:733,safety,review,review,733,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1093,safety,review,review,1093,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1220,safety,test,tests,1220,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,security,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,security,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:444,testability,integr,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:733,testability,review,review,733,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:894,testability,integr,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1093,testability,review,review,1093,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1220,testability,test,tests,1220,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:496,usability,workflow,workflow,496,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:560,usability,workflow,workflows,560,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:795,usability,support,support,795,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:885,usability,workflow,workflow,885,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:995,usability,help,help,995,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward? In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:97,deployability,version,version,97,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:97,integrability,version,version,97,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:97,modifiability,version,version,97,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1233,modifiability,concern,concerns,1233,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:172,performance,content,content,172,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1077,safety,review,review,1077,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1098,safety,review,review,1098,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1151,safety,review,reviewers,1151,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1077,testability,review,review,1077,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1098,testability,review,review,1098,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1151,testability,review,reviewers,1151,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1233,testability,concern,concerns,1233,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:460,usability,user,user-images,460,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:45,deployability,version,version,45,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:45,integrability,version,version,45,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:45,modifiability,version,version,45,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:265,modifiability,concern,concerns,265,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:130,safety,review,review,130,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:183,safety,review,reviewers,183,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:291,safety,review,review,291,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,safety,review,review,342,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:130,testability,review,review,130,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:183,testability,review,reviewers,183,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:265,testability,concern,concerns,265,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:291,testability,review,review,291,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,testability,review,review,342,"> we looked into in in detail for the second version of the manuscript . I appreciate your work for being thorough :) . > Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here. Maybe peer review would be better off styled like a GitHub PR review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:116,energy efficiency,current,currently,116,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:678,performance,time,time,678,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:85,safety,test,tests,85,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,safety,test,tests,239,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:309,safety,test,tests,309,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:324,safety,test,tests,324,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,safety,test,test,342,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:351,safety,input,input,351,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:385,safety,except,exception,385,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:555,safety,test,tests,555,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:612,safety,test,tests,612,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:707,safety,test,tests,707,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:85,testability,test,tests,85,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,testability,test,tests,239,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:309,testability,test,tests,309,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:324,testability,test,tests,324,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,testability,test,test,342,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:555,testability,test,tests,555,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:612,testability,test,tests,612,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:707,testability,test,tests,707,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:351,usability,input,input,351,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:643,usability,feedback,feedback,643,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak ! I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:138,performance,time,time,138,"Hey @giovp ,. thanks for fixing the doc/precommit issues! What are the next steps? Will be another round of code review? I will have some time to work on suggestions/required improvements next week - just let me know :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:113,safety,review,review,113,"Hey @giovp ,. thanks for fixing the doc/precommit issues! What are the next steps? Will be another round of code review? I will have some time to work on suggestions/required improvements next week - just let me know :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:113,testability,review,review,113,"Hey @giovp ,. thanks for fixing the doc/precommit issues! What are the next steps? Will be another round of code review? I will have some time to work on suggestions/required improvements next week - just let me know :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:248,integrability,translat,translated,248,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:248,interoperability,translat,translated,248,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:542,modifiability,paramet,parameterisation,542,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:119,performance,perform,perform,119,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:140,safety,test,test,140,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:532,safety,avoid,avoid,532,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:587,security,ident,identification,587,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:140,testability,test,test,140,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:119,usability,perform,perform,119,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:225,availability,slo,slots,225,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:159,deployability,API,API,159,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:285,energy efficiency,current,current,285,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:159,integrability,API,API,159,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:159,interoperability,API,API,159,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:698,interoperability,convers,conversation,698,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:225,reliability,slo,slots,225,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,safety,test,tests,57,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:57,testability,test,tests,57,"@jlause sorry for not getting back. I just had a look at tests and it looks good! as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments! I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:41,deployability,modul,module,41,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:113,energy efficiency,current,currently,113,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:126,energy efficiency,core,core,126,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:41,modifiability,modul,module,41,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:247,modifiability,variab,variable,247,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:41,safety,modul,module,41,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:313,usability,support,supports,313,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core. e.g. ```. sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(). ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ... Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:168,integrability,translat,translated,168,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:168,interoperability,translat,translated,168,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:39,performance,perform,perform,39,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:60,safety,test,test,60,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:491,safety,test,test,491,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:60,testability,test,test,60,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:491,testability,test,test,491,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:39,usability,perform,perform,39,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented? I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:12,safety,test,testing,12,"I found the testing in the [original R implementation](https://github.com/willtownes/scrna2019/blob/master/util/functions.R) of the deviances, and then mirrored it in [my implementation](https://github.com/theislab/scanpy/pull/1765). Glad to hear there seems to be potential for something analogous here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:12,testability,test,testing,12,"I found the testing in the [original R implementation](https://github.com/willtownes/scrna2019/blob/master/util/functions.R) of the deviances, and then mirrored it in [my implementation](https://github.com/theislab/scanpy/pull/1765). Glad to hear there seems to be potential for something analogous here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,deployability,version,version,342,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,deployability,version,version,569,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:842,deployability,releas,release,842,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:222,energy efficiency,adapt,adapted,222,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:170,integrability,batch,batch,170,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:222,integrability,adapt,adapted,222,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,integrability,version,version,342,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,integrability,version,version,569,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:222,interoperability,adapt,adapted,222,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:222,modifiability,adapt,adapted,222,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:342,modifiability,version,version,342,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,modifiability,version,version,569,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:814,modifiability,pac,package,814,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:170,performance,batch,batch,170,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:29,safety,review,review,29,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:216,safety,test,tests,216,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:430,safety,input,input,430,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:29,testability,review,review,29,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:216,testability,test,tests,216,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:430,usability,input,input,430,"Hey @giovp ! Thanks for your review and sorry for the delay, but I think I addressed all requests now:. - code moved to experimental. - fixed broken column ordering when batch argument was used with HVG selection. - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that? Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:211,usability,feedback,feedback,211,Hey all! Just wanted to let you know that I've also made a PR for the tutorial as @giovp suggested:. https://github.com/theislab/scanpy-tutorials/pull/43. Let me know what's left to do - looking forward to your feedback! Jan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:196,availability,error,error,196,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:599,availability,ping,ping,599,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:127,deployability,fail,failing,127,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:652,deployability,API,API,652,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:685,energy efficiency,current,current,685,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:652,integrability,API,API,652,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:652,interoperability,API,API,652,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:196,performance,error,error,196,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:498,performance,perform,performed,498,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:127,reliability,fail,failing,127,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:196,safety,error,error,196,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:818,safety,review,reviewed,818,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:818,testability,review,reviewed,818,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:196,usability,error,error,196,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:498,usability,perform,performed,498,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs. - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:388,performance,perform,performed,388,"Hey @giovp,. thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:135,usability,experien,experienced,135,"Hey @giovp,. thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:388,usability,perform,performed,388,"Hey @giovp,. thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:709,usability,clarit,clarity,709,"Hey @giovp,. thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen? Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:309,availability,consist,consistent,309,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:562,energy efficiency,green,green,562,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:402,modifiability,pac,package,402,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:533,modifiability,exten,extent,533,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:309,usability,consist,consistent,309,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:697,usability,document,documentation,697,"@jlause I think I caught the remaining bug for the docs, now they should pass. . EDIT: they do! I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:. - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:91,availability,consist,consistency,91,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:176,availability,consist,consistent,176,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:71,safety,input,input,71,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:71,usability,input,input,71,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:91,usability,consist,consistency,91,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:176,usability,consist,consistent,176,"Hey @giovp! Thanks for fixing the doc issues once more - I checked the input arguments for consistency. I made some minor changes to make argument order and doc string wording consistent across functions, where applicable. Hope this did not break the docs again :crossed_fingers: (EDIT: it did not ;-)). It was quite a ride indeed but I'm happy that we did it! Looking forward to contribute more in the future :). I'll make another small change to the tutorial (briefly mention the two 'bundle' functions), and will then be waiting for @ivirshup's verdict!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:24,usability,feedback,feedback,24,"Thanks for the detailed feedback @ivirshup , I've started to and will be working on that in the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:348,deployability,continu,continue,348,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:827,deployability,API,API,827,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:470,integrability,batch,batches,470,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:827,integrability,API,API,827,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:986,integrability,wrap,wraps,986,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:827,interoperability,API,API,827,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:470,performance,batch,batches,470,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:172,safety,test,tests,172,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:229,safety,test,tests,229,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:339,safety,input,input,339,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:172,testability,test,tests,172,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:229,testability,test,tests,229,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:245,usability,efficien,efficient,245,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:339,usability,input,input,339,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:. - [ ] Make tests faster (re-use results where possible). - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:. - on the keyword/positional argument issue. - on the the ""is median rank a good way to do HVG selection across batches""-issue. - on the question what the final names of the functions should be - your suggestions were:. > `normalize_pearson_residuals` -> `pearson_residuals`. > It's a bit more like log1p. >. > `sc.experimental.pp.highly_variable_genes` -> something else. > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) . Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:33,availability,failur,failures,33,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:93,availability,failur,failures,93,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:33,deployability,fail,failures,33,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:59,deployability,automat,automatic,59,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:93,deployability,fail,failures,93,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:33,performance,failur,failures,33,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:93,performance,failur,failures,93,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:33,reliability,fail,failures,33,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:93,reliability,fail,failures,93,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:28,safety,test,test,28,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:69,safety,test,tests,69,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:28,testability,test,test,28,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:59,testability,automat,automatic,59,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:69,testability,test,tests,69,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:102,usability,clear,clearly,102,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1178,availability,consist,consistency,1178,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1224,availability,avail,available,1224,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:468,energy efficiency,optim,optimized,468,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:617,energy efficiency,current,current,617,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:672,energy efficiency,current,currently,672,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:124,integrability,transform,transformed,124,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:655,integrability,batch,batch,655,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:726,integrability,batch,batch,726,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:822,integrability,batch,batch,822,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:973,integrability,batch,batch-corrected,973,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1073,integrability,batch,batch,1073,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:124,interoperability,transform,transformed,124,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,modifiability,concern,concern,569,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1209,modifiability,paramet,parameters,1209,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1550,modifiability,pac,package,1550,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1623,modifiability,pac,package,1623,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:468,performance,optimiz,optimized,468,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:655,performance,batch,batch,655,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:726,performance,batch,batch,726,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:822,performance,batch,batch,822,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:973,performance,batch,batch-corrected,973,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1073,performance,batch,batch,1073,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:941,reliability,doe,does,941,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1224,reliability,availab,available,1224,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1224,safety,avail,available,1224,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1224,security,availab,available,1224,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:569,testability,concern,concern,569,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:848,testability,simpl,simply,848,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:525,usability,efficien,efficiency,525,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:848,usability,simpl,simply,848,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1178,usability,consist,consistency,1178,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1296,usability,tool,tooling,1296,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1607,usability,experien,experience,1607,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix. > . > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. . This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know! > ## Docs consistency. > . > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1099,availability,consist,consistency,1099,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1219,deployability,fail,failing,1219,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:674,integrability,batch,batches,674,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:674,performance,batch,batches,674,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1219,reliability,fail,failing,1219,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:77,safety,test,tests,77,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:134,safety,test,tests,134,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:283,safety,test,tests,283,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:407,safety,input,input,407,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1227,safety,test,tests,1227,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1401,safety,test,tests,1401,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:77,testability,test,tests,77,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:134,testability,test,tests,134,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:283,testability,test,tests,283,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1227,testability,test,tests,1227,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1362,testability,understand,understand,1362,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1401,testability,test,tests,1401,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:150,usability,efficien,efficient,150,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:407,usability,input,input,407,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1099,usability,consist,consistency,1099,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1549,usability,feedback,feedback,1549,"Hey, just as a quick summary of how things stand from my view:. . - [x] Make tests faster (re-use results where possible). - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it? - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment). - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)). - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)). - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?! I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1840,availability,avail,available,1840,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1859,availability,consist,consistency,1859,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1929,availability,avail,available,1929,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:834,deployability,API,API,834,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:972,deployability,modul,module,972,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1523,deployability,API,API,1523,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1563,deployability,modul,module,1563,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1754,deployability,modul,module,1754,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:408,integrability,batch,batches,408,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:756,integrability,transform,transformation,756,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:834,integrability,API,API,834,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1259,integrability,event,eventually,1259,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1523,integrability,API,API,1523,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:756,interoperability,transform,transformation,756,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:834,interoperability,API,API,834,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1419,interoperability,compatib,compatibility,1419,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1523,interoperability,API,API,1523,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:21,modifiability,extens,extensive,21,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:972,modifiability,modul,module,972,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1270,modifiability,refact,refactor,1270,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1563,modifiability,modul,module,1563,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1754,modifiability,modul,module,1754,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1914,modifiability,paramet,parameters,1914,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:408,performance,batch,batches,408,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1270,performance,refactor,refactor,1270,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1840,reliability,availab,available,1840,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1929,reliability,availab,available,1929,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:972,safety,modul,module,972,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1563,safety,modul,module,1563,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1754,safety,modul,module,1754,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1840,safety,avail,available,1840,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1929,safety,avail,available,1929,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1840,security,availab,available,1840,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1929,security,availab,available,1929,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:813,testability,understand,understand,813,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:496,usability,clear,clear,496,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it? enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1582,usability,user,users,1582,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1655,usability,clear,clearer,1655,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1859,usability,consist,consistency,1859,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2001,usability,tool,tooling,2001,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:2096,usability,help,help,2096,"emove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. . If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post). > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these? what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:189,deployability,fail,failing,189,"@giovp thanks for your reply, I agree on all points :) Have a good vacation! @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:189,reliability,fail,failing,189,"@giovp thanks for your reply, I agree on all points :) Have a good vacation! @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:197,safety,test,tests,197,"@giovp thanks for your reply, I agree on all points :) Have a good vacation! @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:197,testability,test,tests,197,"@giovp thanks for your reply, I agree on all points :) Have a good vacation! @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:115,usability,feedback,feedback,115,"@giovp thanks for your reply, I agree on all points :) Have a good vacation! @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:88,energy efficiency,current,current,88,"Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:53,performance,time,time,53,"Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:96,performance,time,timeline,96,"Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:90,energy efficiency,current,current,90,"> Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that. Wondering about the same thing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:55,performance,time,time,55,"> Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that. Wondering about the same thing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:98,performance,time,timeline,98,"> Dear all, are we still on track to have it merged in time for v1.9? Not sure what's the current timeline for that. Wondering about the same thing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:56,deployability,updat,updates,56,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:86,performance,time,timeline,86,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:56,safety,updat,updates,56,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:296,safety,input,input,296,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:56,security,updat,updates,56,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:296,usability,input,input,296,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:462,usability,feedback,feedback,462,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up! Cheers,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:159,performance,time,time,159,"Hey @ivirshup,. thanks for your comments, that looks like good feedback! Hope your moving went well :). I'm busier than expected this week, but will take some time next week to respond / make the changes you suggested. Cheers, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:63,usability,feedback,feedback,63,"Hey @ivirshup,. thanks for your comments, that looks like good feedback! Hope your moving went well :). I'm busier than expected this week, but will take some time next week to respond / make the changes you suggested. Cheers, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:214,integrability,batch,batch,214,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:214,performance,batch,batch,214,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:417,performance,cach,cache,417,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:440,performance,time,time,440,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:144,safety,except,except,144,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:451,safety,test,testing,451,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:207,testability,simpl,simple,207,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:451,testability,test,testing,451,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:98,usability,feedback,feedback,98,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:207,usability,simpl,simple,207,"Hey all! Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182). - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that? Hope you enjoy your Christmas holidays! Best,. Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:282,availability,ping,ping,282,"Hey @giovp & @ivirshup,. hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet? For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do! Looking forward to wrap this up :). Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:379,integrability,wrap,wrap,379,"Hey @giovp & @ivirshup,. hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet? For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do! Looking forward to wrap this up :). Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:172,performance,time,timeline,172,"Hey @giovp & @ivirshup,. hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet? For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do! Looking forward to wrap this up :). Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:307,usability,feedback,feedback,307,"Hey @giovp & @ivirshup,. hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet? For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do! Looking forward to wrap this up :). Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:52,availability,sla,slacking,52,"@jlause hey,really sorry for late reply and general slacking on this, I have TAC on Monday and after that will be able to take a look, you'll get to hear from me for sure next week. Also remember about the notebook, we'll make sure to merge that as well once this is ready (very soon). Thanks again and sorry for late reply.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:52,reliability,sla,slacking,52,"@jlause hey,really sorry for late reply and general slacking on this, I have TAC on Monday and after that will be able to take a look, you'll get to hear from me for sure next week. Also remember about the notebook, we'll make sure to merge that as well once this is ready (very soon). Thanks again and sorry for late reply.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:187,safety,reme,remember,187,"@jlause hey,really sorry for late reply and general slacking on this, I have TAC on Monday and after that will be able to take a look, you'll get to hear from me for sure next week. Also remember about the notebook, we'll make sure to merge that as well once this is ready (very soon). Thanks again and sorry for late reply.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:274,availability,error,error,274,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,deployability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,deployability,automat,automated,202,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:227,deployability,fail,failing,227,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:158,integrability,batch,batch,158,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,integrability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:280,integrability,messag,messages,280,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,interoperability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:280,interoperability,messag,messages,280,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,modifiability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:158,performance,batch,batch,158,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:274,performance,error,error,274,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:435,performance,time,time,435,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,reliability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:227,reliability,fail,failing,227,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:212,safety,test,tests,212,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:218,safety,test,test,218,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:274,safety,error,error,274,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,security,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:164,testability,integr,integration,164,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,testability,automat,automated,202,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:212,testability,test,tests,212,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:218,testability,test,test,218,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:259,testability,understand,understand,259,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:274,usability,error,error,274,"Hi @giovp,. no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? If there is anything I should look into, let me know - I have some time for this next week! Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:99,availability,error,error,99,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:27,deployability,automat,automated,27,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:52,deployability,fail,failing,52,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:105,integrability,messag,messages,105,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:105,interoperability,messag,messages,105,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:99,performance,error,error,99,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:239,performance,cach,cache,239,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:52,reliability,fail,failing,52,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:37,safety,test,tests,37,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:43,safety,test,test,43,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:99,safety,error,error,99,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:27,testability,automat,automated,27,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:37,testability,test,tests,37,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:43,testability,test,test,43,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:84,testability,understand,understand,84,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:209,testability,understand,understand,209,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:99,usability,error,error,99,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR? yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:420,integrability,wrap,wraps,420,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:488,integrability,wrap,wraps,488,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:505,integrability,wrap,wrapper,505,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:590,integrability,wrap,wrapper,590,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:505,interoperability,wrapper,wrapper,505,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:590,interoperability,wrapper,wrapper,590,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:43,performance,cach,cache,43,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:102,performance,cach,cache,102,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:158,performance,cach,cache,158,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:354,performance,cach,cached,354,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:374,safety,test,testing,374,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:129,testability,Simpl,Simple,129,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:374,testability,test,testing,374,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:129,usability,Simpl,Simple,129,"Thanks for taking a look at this @giovp! `@cache` is new in 3.8, but the implementation is:. ```. def cache(user_function, /):. 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'. return lru_cache(maxsize=None)(user_function). ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python. from functools import wraps. import scanpy as sc. def cached_dataset(func):. store = []. @wraps(func). def wrapper():. if len(store) < 1:. store.append(func()). return store[0].copy(). return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k). pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced). pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:175,deployability,updat,updating,175,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:331,integrability,wrap,wrapper,331,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:331,interoperability,wrapper,wrapper,331,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:109,performance,cach,cached,109,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:213,performance,cach,cache,213,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:129,safety,test,testing,129,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:175,safety,updat,updating,175,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:199,safety,test,test,199,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:175,security,updat,updating,175,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:129,testability,test,testing,129,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:168,testability,simpl,simply,168,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:199,testability,test,test,199,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:168,usability,simpl,simply,168,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:372,deployability,stage,stage,372,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:398,deployability,build,build,398,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:539,deployability,build,building-the-docs,539,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:747,deployability,releas,release,747,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:866,deployability,fail,failing,866,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:109,integrability,transform,transformation,109,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:212,integrability,transform,transformation,212,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:109,interoperability,transform,transformation,109,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:212,interoperability,transform,transformation,212,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:184,reliability,doe,does,184,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:866,reliability,fail,failing,866,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:520,usability,document,documentation,520,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs. and check that:. - arguments and doc params match. - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:. - [x] add release note. - [ ] go over scanpy_tutorials and re run tutorial and merge. - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1146,availability,consist,consistently,1146,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:406,deployability,stage,stage,406,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:432,deployability,build,build,432,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:573,deployability,build,building-the-docs,573,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1249,deployability,releas,release,1249,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1519,deployability,releas,release,1519,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:116,integrability,transform,transformation,116,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:225,integrability,transform,transformation,225,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1695,integrability,wrap,wrapped,1695,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:116,interoperability,transform,transformation,116,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:225,interoperability,transform,transformation,225,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:982,interoperability,format,format,982,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1159,interoperability,format,format,1159,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1326,interoperability,share,shared,1326,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:197,reliability,doe,does,197,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:554,usability,document,documentation,554,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:1146,usability,consist,consistently,1146,"Hi! > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > . > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix? > . > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > . > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:. > . > * arguments and doc params match. > . > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree. - I think the `.._pca` function is missing from the release note. should I add it there? - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? > . > . > if this gets approval, before merging to master todo:. > . > * [x] add release note. > . > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :). Best! Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:308,deployability,releas,release,308,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text? I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ? > I think the .._pca function is missing from the release note. should I add it there? The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:581,integrability,wrap,wrap,581,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text? I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ? > I think the .._pca function is missing from the release note. should I add it there? The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:145,interoperability,format,format,145,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text? I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ? > I think the .._pca function is missing from the release note. should I add it there? The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:381,interoperability,share,shared,381,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text? I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ? > I think the .._pca function is missing from the release note. should I add it there? The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that? must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:84,deployability,releas,release,84,"Hi, I worked over the docs completely once but still need to do a few small things (release note, final spell check) but need to leave my desk now - will do the final bits either later today or tomorrow! Best jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:27,safety,compl,completely,27,"Hi, I worked over the docs completely once but still need to do a few small things (release note, final spell check) but need to leave my desk now - will do the final bits either later today or tomorrow! Best jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:27,security,compl,completely,27,"Hi, I worked over the docs completely once but still need to do a few small things (release note, final spell check) but need to leave my desk now - will do the final bits either later today or tomorrow! Best jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:242,availability,consist,consistent,242,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:165,deployability,releas,release,165,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:274,deployability,build,building,274,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:295,deployability,fail,failing,295,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:295,reliability,fail,failing,295,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:242,usability,consist,consistent,242,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:453,usability,help,help,453,"Hi @giovp ,. I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up! Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:511,availability,consist,consistent,511,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:188,deployability,releas,release,188,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:436,deployability,releas,release,436,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:511,usability,consist,consistent,511,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:530,usability,clear,clear,530,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:81,safety,review,review,81,LGTM. Thanks for pushing and getting this through! And thanks for picking up the review on this @giovp!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:81,testability,review,review,81,LGTM. Thanks for pushing and getting this through! And thanks for picking up the review on this @giovp!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,availability,restor,restore,202,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:307,availability,down,downstream,307,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:162,deployability,fail,failed,162,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:318,energy efficiency,current,currently,318,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:162,reliability,fail,failed,162,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:202,reliability,restor,restore,202,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:293,safety,compl,completed,293,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1715:293,security,compl,completed,293,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently? <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715
https://github.com/scverse/scanpy/pull/1717:35,usability,user,user-images,35,Proof it worked:. ![image](https://user-images.githubusercontent.com/8238804/109921702-63aa8380-7d10-11eb-8636-338b7a3836a9.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1717
https://github.com/scverse/scanpy/issues/1718:111,interoperability,specif,specific,111,"You can set adata.obsm['X_umap'] to the array you have, given that the row order is conserved. Also creating a specific matrix is ok, say adata.obsm['X_seurat'], then use that as basis for all visualizations and calculations (e.g. scv.pl.scatter(adata, basis='seurat'…))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:193,usability,visual,visualizations,193,"You can set adata.obsm['X_umap'] to the array you have, given that the row order is conserved. Also creating a specific matrix is ok, say adata.obsm['X_seurat'], then use that as basis for all visualizations and calculations (e.g. scv.pl.scatter(adata, basis='seurat'…))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:265,interoperability,coordinat,coordinate,265,"Very appreciate your reply @dawe. it was fine by me. But i confused the order of AnnData's obsm['X_umap'] 's barcode ,because of it was array without rownames, so how can i obtain . the cell barcode that was identical with X_umap order. . i want to save the X_umap coordinate with the cell barcode into csv file. how should i do for this condition. any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:208,security,ident,identical,208,"Very appreciate your reply @dawe. it was fine by me. But i confused the order of AnnData's obsm['X_umap'] 's barcode ,because of it was array without rownames, so how can i obtain . the cell barcode that was identical with X_umap order. . i want to save the X_umap coordinate with the cell barcode into csv file. how should i do for this condition. any advice would be appreciated. Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:76,testability,simpl,simply,76,"Retrieve cell names in the Seurat object used to create the embedding, then simply reorder AnnData accordingly (adata = adata[cell_names]).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:76,usability,simpl,simply,76,"Retrieve cell names in the Seurat object used to create the embedding, then simply reorder AnnData accordingly (adata = adata[cell_names]).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:332,deployability,log,logically,332,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:715,performance,time,time,715,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:332,safety,log,logically,332,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:332,security,log,logically,332,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:332,testability,log,logically,332,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:96,usability,indicat,indicate,96,"Hi @dawe ,. I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction. In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:232,interoperability,specif,specific,232,"Hi @honghh2018,. There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. I would report this in the [scvelo github repo](https://github.com/theislab/scvelo), where you can get a more specific answer about this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:54,reliability,doe,doesn,54,"Hi @honghh2018,. There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. I would report this in the [scvelo github repo](https://github.com/theislab/scvelo), where you can get a more specific answer about this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:722,availability,robust,robust,722,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:748,availability,slo,slot,748,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:156,deployability,log,logically,156,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:795,interoperability,specif,specified,795,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:689,performance,time,time,689,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:722,reliability,robust,robust,722,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:748,reliability,slo,slot,748,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:156,safety,log,logically,156,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:722,safety,robust,robust,722,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:156,security,log,logically,156,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:156,testability,log,logically,156,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:160,deployability,resourc,resource,160,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:160,energy efficiency,resourc,resource,160,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:255,energy efficiency,model,model,255,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:160,performance,resourc,resource,160,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:56,reliability,doe,doesn,56,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:160,safety,resourc,resource,160,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:255,security,model,model,255,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:160,testability,resourc,resource,160,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:169,usability,support,supporting,169,"Hello @LuckyMD . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. That's interesting! Do you have any resource supporting this? I'm asking as I'm using `scvelo` a lot these days on a developmental model.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:215,deployability,scale,scales,215,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:311,deployability,observ,observe,311,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:405,deployability,observ,observing,405,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:215,energy efficiency,scale,scales,215,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:360,energy efficiency,model,model,360,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:215,modifiability,scal,scales,215,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:210,performance,time,time,210,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:215,performance,scale,scales,215,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:352,performance,time,time,352,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:360,security,model,model,360,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:311,testability,observ,observe,311,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:405,testability,observ,observing,405,"Hi @dawe,. @VolkerBergen is just writing a manuscript on this as it has been coming up a in scvelo issues. He will be able to explain a lot better, but I can try to give you an idea. For developmental datasets time scales of splicing kinetics and development don't really match up very well, so you often don't observe a fixed kinetic for a sufficient time to model it before it changes. Also, you may be observing multiple different kinetics e.g., in the blood.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:240,interoperability,specif,specific,240,"> Hi @honghh2018,. > . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. I would report this in the [scvelo github repo](https://github.com/theislab/scvelo), where you can get a more specific answer about this. Thanks the reply, this tools was great so far, but still had improve space on your reported. i hope this issue can be solved. Best,. hanhuihong .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:62,reliability,doe,doesn,62,"> Hi @honghh2018,. > . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. I would report this in the [scvelo github repo](https://github.com/theislab/scvelo), where you can get a more specific answer about this. Thanks the reply, this tools was great so far, but still had improve space on your reported. i hope this issue can be solved. Best,. hanhuihong .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:291,usability,tool,tools,291,"> Hi @honghh2018,. > . > There are several cases where scvelo doesn't work as expected such as for blood, early development, etc. I would report this in the [scvelo github repo](https://github.com/theislab/scvelo), where you can get a more specific answer about this. Thanks the reply, this tools was great so far, but still had improve space on your reported. i hope this issue can be solved. Best,. hanhuihong .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:750,availability,robust,robust,750,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:776,availability,slo,slot,776,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:158,deployability,log,logically,158,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:875,energy efficiency,current,currently,875,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:823,interoperability,specif,specified,823,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:711,performance,time,time,711,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:750,reliability,robust,robust,750,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:776,reliability,slo,slot,776,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:158,safety,log,logically,158,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:750,safety,robust,robust,750,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:158,security,log,logically,158,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1718:158,testability,log,logically,158,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. > . > As @LuckyMD said, this is a question for `scvelo`. > . > > I guess what i make the cell order was wrong ? > . > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > . > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time? > . > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. > . > d. Thanks i would check currently, and reported the result as soon as possible. . Best,. hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718
https://github.com/scverse/scanpy/issues/1719:156,interoperability,standard,standard,156,"I personally like putting the ensembl ids as the `var_names` and keeping the gene symbols as a column in `var`. This way it's always unique matching with a standard. People can use different sets of gene symbols, so they're just hard to match against. If there was broader interest, this could be added to `anndata`. But for now it should be pretty straight forward for users to do with:. ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:2,usability,person,personally,2,"I personally like putting the ensembl ids as the `var_names` and keeping the gene symbols as a column in `var`. This way it's always unique matching with a standard. People can use different sets of gene symbols, so they're just hard to match against. If there was broader interest, this could be added to `anndata`. But for now it should be pretty straight forward for users to do with:. ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:370,usability,user,users,370,"I personally like putting the ensembl ids as the `var_names` and keeping the gene symbols as a column in `var`. This way it's always unique matching with a standard. People can use different sets of gene symbols, so they're just hard to match against. If there was broader interest, this could be added to `anndata`. But for now it should be pretty straight forward for users to do with:. ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:97,integrability,pub,published,97,"I also prefer ENSEMBL IDs. Would it make sense to add this to the main tutorial? I see that many published papers and collaborators use `var_names_make_unique` with numeric index (sometimes deleting ENSEMBL ID altogether) - so it seems useful to have . ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```. as an example to encourage this behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:7,usability,prefer,prefer,7,"I also prefer ENSEMBL IDs. Would it make sense to add this to the main tutorial? I see that many published papers and collaborators use `var_names_make_unique` with numeric index (sometimes deleting ENSEMBL ID altogether) - so it seems useful to have . ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```. as an example to encourage this behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:368,usability,behavi,behaviour,368,"I also prefer ENSEMBL IDs. Would it make sense to add this to the main tutorial? I see that many published papers and collaborators use `var_names_make_unique` with numeric index (sometimes deleting ENSEMBL ID altogether) - so it seems useful to have . ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```. as an example to encourage this behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:123,energy efficiency,optim,optimal,123,"Actually, I just realised that . ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```. is not optimal because it adds ENSEMBL to every single var name, making the names very long (which is what most people are likely trying to avoid)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:256,safety,avoid,avoid,256,"Actually, I just realised that . ```python. adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]. ```. is not optimal because it adds ENSEMBL to every single var name, making the names very long (which is what most people are likely trying to avoid)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:492,availability,error,error,492,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:154,integrability,sub,subset,154,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:492,performance,error,error,492,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:492,safety,error,error,492,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:145,usability,user,user,145,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:492,usability,error,error,492,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1719:616,usability,support,supported,616,"How about MultiIndex? ```python. df = adata.var.reset_index().set_index(['gene_ids', 'index']). adata2 = sc.AnnData(X = adata.X, var = df). ```. user can subset genes based on:. ```python. genes = ['CD69', 'CD44', 'CXCR5']. idx = adata2.var.index.get_level_values('index').isin(genes). adata2[:,idx]. ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:. ```python. genes2 = adata2[:,idx].var.index. sc.pl.violin(adata2, genes2). ## error. gene_names = pd.Series(adata.var_names, index=adata.var_names). ```. `initializing a Series from a MultiIndex is not supported`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719
https://github.com/scverse/scanpy/issues/1720:0,reliability,Doe,Does,0,Does it get better if you run the following before saving the figure as PDF:. ```python. import matplotlib. matplotlib.rcParams['pdf.fonttype'] = 42. matplotlib.rcParams['ps.fonttype'] = 42. ```. @ivirshup Does it make sense to add this to `sc.set_figure_params()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:206,reliability,Doe,Does,206,Does it get better if you run the following before saving the figure as PDF:. ```python. import matplotlib. matplotlib.rcParams['pdf.fonttype'] = 42. matplotlib.rcParams['ps.fonttype'] = 42. ```. @ivirshup Does it make sense to add this to `sc.set_figure_params()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:202,reliability,doe,doesn,202,"Thanks @gokceneraslan, I had no idea, and this is super useful! Also, the option for `svg` (so fonts won't be paths) is:. ```python. ""svg.fonttype"": None. ```. I think I'd want to know why `matplotlib` doesn't have that as the default. For the `svg` option, it looks like it can lead to issues with viewing the output file on other machines – so I don't think this is a good default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:259,integrability,pub,publications,259,"@ivirshup Not having Type 2 fonts by default in mpl might be due to the increasing file size (since whole font is embedded into the pdf) https://github.com/matplotlib/matplotlib/issues/11303, but considering that plots we make in scanpy are typically used in publications so they likely get edited in vector graphics software, we should maybe consider making it the default, my 2 cents. At least we should make it an option in `set_figure_params` like `editable_fonts=True/False` with a warning saying that the file size might increase, I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:146,deployability,fail,failing,146,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:607,integrability,pub,publications,607,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:319,interoperability,specif,specific,319,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:198,modifiability,paramet,parameters,198,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:243,modifiability,paramet,parameters,243,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:302,performance,time,time,302,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:516,performance,time,time,516,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:146,reliability,fail,failing,146,"@gokceneraslan thanks so much for your suggestion, that did the trick for me! I was trying to figure this out via sc.set_figure_params() and kept failing. It did not occur to me that the matplotlib parameters could overwrite the scanpy figure parameters, and then it would definitely take me some more time to find the specific `matplotlib.rcParams['pdf.fonttype'] = 42` setting. I think adding it as an option `editable_fonts=True/False` in `set_figure_params` would be a good idea, at least I am sure it will save time for other bioinformaticians trying to incorporate figures with editable text in their publications.Thanks again! : )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:88,deployability,configurat,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:88,integrability,configur,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:576,integrability,pub,publication,576,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:88,modifiability,configur,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:113,modifiability,pac,packages,113,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:88,security,configur,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:427,usability,user,users,427,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages. 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users? What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:380,integrability,wrap,wrapping,380,"> can we just make finding out about this easier for users? That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:399,safety,compl,complex,399,"> can we just make finding out about this easier for users? That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:399,security,compl,complex,399,"> can we just make finding out about this easier for users? That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:53,usability,user,users,53,"> can we just make finding out about this easier for users? That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:37,usability,guid,guide,37,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:52,usability,document,documentation,52,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:83,usability,guid,guides,83,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/issues/1720:126,usability,guid,guide,126,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720
https://github.com/scverse/scanpy/pull/1722:143,deployability,releas,release,143,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:400,deployability,updat,updated,400,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:440,deployability,releas,release,440,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:22,performance,content,content,22,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:400,safety,updat,updated,400,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:400,security,updat,updated,400,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:51,usability,help,helpful,51,"Thanks for the PR! On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes? I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:36,deployability,releas,release,36,"Hi! Thanks for the addition. In the release notes I guess something like ""Triku, a new feature selection method was added to our ecosystem"" would be fine. . As for committing to the master, sorry for that. I am eager to see it in the 1.8.0 release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:240,deployability,releas,release,240,"Hi! Thanks for the addition. In the release notes I guess something like ""Triku, a new feature selection method was added to our ecosystem"" would be fine. . As for committing to the master, sorry for that. I am eager to see it in the 1.8.0 release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:48,deployability,releas,release,48,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:145,deployability,releas,release,145,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/pull/1722:137,energy efficiency,current,current,137,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722
https://github.com/scverse/scanpy/issues/1724:727,integrability,wrap,wrapper,727,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:727,interoperability,wrapper,wrapper,727,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:609,usability,user,user,609,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:635,usability,user,users,635,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:371,energy efficiency,heat,heatmap,371,"I think making access to entires in `obsm` for plotting functions is a good idea. This is definitely on our roadmap, and has started to be implemented (https://github.com/theislab/anndata/pull/342), but is a bit stalled at the moment. Am I correct in understanding that being able to things like:. ```python. adata.obsm[""pathways""] = pathway_dataframe_func(adata). sc.pl.heatmap(adata, groupby=""leiden"", obsm=""pathway""). sc.pl.umap(adata, color=[""pathways/pathway-1"", ""leiden""]). ```. would solve most of the barriers you're facing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:15,security,access,access,15,"I think making access to entires in `obsm` for plotting functions is a good idea. This is definitely on our roadmap, and has started to be implemented (https://github.com/theislab/anndata/pull/342), but is a bit stalled at the moment. Am I correct in understanding that being able to things like:. ```python. adata.obsm[""pathways""] = pathway_dataframe_func(adata). sc.pl.heatmap(adata, groupby=""leiden"", obsm=""pathway""). sc.pl.umap(adata, color=[""pathways/pathway-1"", ""leiden""]). ```. would solve most of the barriers you're facing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:509,security,barrier,barriers,509,"I think making access to entires in `obsm` for plotting functions is a good idea. This is definitely on our roadmap, and has started to be implemented (https://github.com/theislab/anndata/pull/342), but is a bit stalled at the moment. Am I correct in understanding that being able to things like:. ```python. adata.obsm[""pathways""] = pathway_dataframe_func(adata). sc.pl.heatmap(adata, groupby=""leiden"", obsm=""pathway""). sc.pl.umap(adata, color=[""pathways/pathway-1"", ""leiden""]). ```. would solve most of the barriers you're facing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:251,testability,understand,understanding,251,"I think making access to entires in `obsm` for plotting functions is a good idea. This is definitely on our roadmap, and has started to be implemented (https://github.com/theislab/anndata/pull/342), but is a bit stalled at the moment. Am I correct in understanding that being able to things like:. ```python. adata.obsm[""pathways""] = pathway_dataframe_func(adata). sc.pl.heatmap(adata, groupby=""leiden"", obsm=""pathway""). sc.pl.umap(adata, color=[""pathways/pathway-1"", ""leiden""]). ```. would solve most of the barriers you're facing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:329,deployability,updat,update,329,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:242,energy efficiency,model,model,242,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:231,integrability,wrap,wrapper,231,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:231,interoperability,wrapper,wrapper,231,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:329,safety,updat,update,329,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:78,security,access,accessible,78,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:242,security,model,model,242,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:329,security,updat,update,329,"Thanks for the quick responses @LuckyMD and @ivirshup. If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it. Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:148,deployability,api,api,148,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:148,integrability,api,api,148,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:148,interoperability,api,api,148,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:34,testability,simpl,simple,34,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:34,usability,simpl,simple,34,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:90,modifiability,layer,layer,90,I was thinking a quick thing to do would be to add an `obsm` argument to the plots (like `layer`). In this case we probably just wouldn't allow using values from `X` and `obsm[whatever]` together for the moment.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:214,interoperability,compatib,compatible,214,"Thank you all for the feedback! In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:22,usability,feedback,feedback,22,"Thank you all for the feedback! In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:196,usability,tool,tools,196,"Thank you all for the feedback! In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:34,interoperability,compatib,compatible,34,"> Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem? Please do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1724:16,usability,tool,tools,16,"> Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem? Please do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724
https://github.com/scverse/scanpy/issues/1725:18,energy efficiency,cool,cool,18,"Oh maaaan, that's cool. I had noticed that something was off about the batch argument indeed! Thanks so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1725
https://github.com/scverse/scanpy/issues/1725:71,integrability,batch,batch,71,"Oh maaaan, that's cool. I had noticed that something was off about the batch argument indeed! Thanks so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1725
https://github.com/scverse/scanpy/issues/1725:71,performance,batch,batch,71,"Oh maaaan, that's cool. I had noticed that something was off about the batch argument indeed! Thanks so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1725
https://github.com/scverse/scanpy/issues/1729:309,integrability,batch,batch,309,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:401,integrability,batch,batch,401,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:96,modifiability,variab,variables,96,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:309,performance,batch,batch,309,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:401,performance,batch,batch,401,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:431,usability,user,user-images,431,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python. import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(). genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs). sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]). ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1729:28,integrability,sub,subset,28,"Thank you, ivirshup. How to subset cells in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729
https://github.com/scverse/scanpy/issues/1730:85,integrability,sub,subset,85,"This might be a more appropriate question for the discourse group. In scanpy you can subset in the same way you would subset a pandas dataframe. For example:. `adata_sub = adata[adata.obs['cell type'].isin(['Type A', 'Type B'])]`. This gives you the set of cells that have a `'cell type'` value of `'Type A'` or `'Type B'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1730
https://github.com/scverse/scanpy/issues/1730:118,integrability,sub,subset,118,"This might be a more appropriate question for the discourse group. In scanpy you can subset in the same way you would subset a pandas dataframe. For example:. `adata_sub = adata[adata.obs['cell type'].isin(['Type A', 'Type B'])]`. This gives you the set of cells that have a `'cell type'` value of `'Type A'` or `'Type B'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1730
https://github.com/scverse/scanpy/issues/1731:125,usability,tool,tools,125,"Is there a reason not to `gzip` these files? While they aren't big, you get ~4 fold storage savings and most text inspection tools (e.g. less) can read gzipped files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:30,energy efficiency,core,core,30,We get non gzipped files from core facility and they also seemed to have worked with non gzipped files beforehand in the workflow. But maybe there was another reason for them to not use gzipped files (maybe older reading functions or something).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:121,usability,workflow,workflow,121,We get non gzipped files from core facility and they also seemed to have worked with non gzipped files beforehand in the workflow. But maybe there was another reason for them to not use gzipped files (maybe older reading functions or something).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:70,performance,I/O,I/O,70,At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I guess it's always a memory vs storage question.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:198,performance,memor,memory,198,At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I guess it's always a memory vs storage question.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:162,usability,minim,minimization,162,At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I guess it's always a memory vs storage question.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:198,usability,memor,memory,198,At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I guess it's always a memory vs storage question.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:1011,availability,slo,slower,1011,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:124,interoperability,specif,specific,124,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:747,performance,I/O,I/O,747,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:1011,reliability,slo,slower,1011,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:222,security,modif,modification,222,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:280,security,expos,exposing,280,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:455,security,expos,expose,455,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:31,usability,support,supporting,31,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:249,usability,support,supporting,249,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:353,usability,custom,custom,353,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:839,usability,minim,minimization,839,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:40,usability,close,close,40,That is a fair point. Maybe we can just close this issue then?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/issues/1731:97,performance,time,time,97,> I believe this only applies to writing `h5ad`. Hmm... i think i must have misunderstood at the time then... i thought it was faster for both.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731
https://github.com/scverse/scanpy/pull/1732:572,availability,sli,slightly,572,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1320,availability,toler,tolerance,1320,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:2,deployability,updat,updated,2,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:10,deployability,releas,release,10,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:296,deployability,fail,failed,296,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:355,deployability,version,version,355,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:483,deployability,fail,fail,483,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:347,energy efficiency,current,current,347,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:355,integrability,version,version,355,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:784,integrability,batch,batch,784,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:824,integrability,batch,batch,824,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:852,integrability,batch,batch,852,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:862,integrability,batch,batch,862,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1028,integrability,batch,batch,1028,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:50,interoperability,specif,specific,50,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1360,interoperability,Mismatch,Mismatched,1360,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:355,modifiability,version,version,355,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:784,performance,batch,batch,784,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:824,performance,batch,batch,824,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:852,performance,batch,batch,852,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:862,performance,batch,batch,862,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1028,performance,batch,batch,1028,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:296,reliability,fail,failed,296,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:474,reliability,doe,does,474,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:483,reliability,fail,fail,483,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:572,reliability,sli,slightly,572,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1150,reliability,rto,rtol,1150,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1236,reliability,rto,rtol,1236,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1320,reliability,toleran,tolerance,1320,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1330,reliability,rto,rtol,1330,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:2,safety,updat,updated,2,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:36,safety,test,test,36,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:86,safety,test,tests,86,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:125,safety,test,tests,125,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:217,safety,test,test,217,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:291,safety,test,test,291,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:370,safety,test,test,370,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:597,safety,Test,Test,597,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1095,safety,test,testing,1095,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1178,safety,test,testing,1178,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1266,safety,Test,Test,1266,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1582,safety,test,tests,1582,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:2,security,updat,updated,2,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:259,security,ddo,ddof,259,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:960,security,ddo,ddof,960,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:36,testability,test,test,36,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:86,testability,test,tests,86,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:125,testability,test,tests,125,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:217,testability,test,test,217,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:291,testability,test,test,291,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:370,testability,test,test,370,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:597,testability,Test,Test,597,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1095,testability,test,testing,1095,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1178,testability,test,testing,1178,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1266,testability,Test,Test,1266,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1287,testability,Assert,AssertionError,1287,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1582,testability,test,tests,1582,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:1623,testability,Assert,AssertionError,1623,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here? Test code with numpy ground truth:. ```. def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():. pbmc = sc.datasets.pbmc3k(). pbmc.var_names_make_unique(). n_cells = pbmc.shape[0]. batch = np.zeros((n_cells), dtype=int). batch[1500:] = 1. pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0). true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(. pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False. ). np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05). np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05). ```. Test output:. ```. E AssertionError: . E Not equal to tolerance rtol=2e-05, atol=2e-05. E . E Mismatched elements: 172 / 32738 (0.525%). E Max absolute difference: 0.01117667. E Max relative difference: 0.00013328. E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32). E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:82,availability,sli,slightly,82,"> Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:82,reliability,sli,slightly,82,"> Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:84,availability,sli,slightly,84,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? > . > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? > . > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?! I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:463,energy efficiency,current,currently,463,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? > . > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? > . > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?! I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:84,reliability,sli,slightly,84,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? > . > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? > . > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?! I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:429,usability,experien,experienced,429,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? > . > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? > . > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?! I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:495,usability,efficien,efficient,495,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here? > . > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ? > . > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?! I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:776,modifiability,interm,intermediate,776,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:526,safety,test,testing,526,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:943,safety,input,input,943,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:385,security,ddo,ddof,385,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:438,security,ddo,ddof,438,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:888,security,control,control,888,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:897,security,ddo,ddof,897,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:526,testability,test,testing,526,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:602,testability,assert,assert,602,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:888,testability,control,control,888,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:516,usability,close,close,516,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:943,usability,input,input,943,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python. import scanpy as sc, numpy as np. from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1). var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64). _, var_scanpy = _get_mean_var(pbmc.X). # These are close. np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different. assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(). ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:49,energy efficiency,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:49,integrability,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:49,interoperability,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:149,interoperability,convers,conversion,149,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:49,modifiability,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:61,safety,test,test,61,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:61,testability,test,test,61,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/pull/1732:34,usability,clear,clear,34,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732
https://github.com/scverse/scanpy/issues/1733:225,reliability,doe,does,225,"I think the solution here is for someone to figure out the Seurat sorting heuristic (didn't have bandwidth to really dig into the R code, and I don't do any R programming), and then correct the docstring based on what Seurat does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,deployability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:383,deployability,version,versions,383,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:465,energy efficiency,current,current,465,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:717,energy efficiency,current,currently,717,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,integrability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:383,integrability,version,versions,383,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:398,integrability,topic,topics,398,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,interoperability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,modifiability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:367,modifiability,pac,packages,367,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:383,modifiability,version,versions,383,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:130,reliability,integr,integrating,130,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
https://github.com/scverse/scanpy/issues/1733:727,reliability,doe,does,727,"Hey, thanks for your reply! I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733
