id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/657:4125,testability,depend,dependency,4125," broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4265,testability,depend,dependency,4265,"://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4751,testability,depend,dependency,4751," pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4891,testability,depend,dependency,4891,"lowing dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5016,testability,depend,dependency,5016,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5156,testability,depend,dependency,5156,"ly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5373,testability,depend,dependency,5373,"ensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5513,testability,depend,dependency,5513,"=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8813,testability,depend,dependency,8813," 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/war",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8953,testability,depend,dependency,8953,"of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pypar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10054,testability,depend,dependency,10054,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10194,testability,depend,dependency,10194,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:308,usability,command,command,308,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:341,usability,command,commands,341,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:597,usability,stop,stops,597,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:894,usability,help,help,894,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:906,usability,help,help,906,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:915,usability,command,commands,915,"@pgrosu . Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ===",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1110,usability,command,command,1110,"azel bin. . ```. > bazel. WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ==========",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1231,usability,Stop,Stops,1231," directory having a WORKSPACE file). [bazel release 5.3.0]. Usage: bazel <command> <options> ... Available commands:. analyze-profile Analyzes build profile data. aquery Analyzes the given targets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] St",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1424,usability,help,help,1424,"gets and queries the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1437,usability,help,help,1437,"ies the action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Rec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1443,usability,command,command,1443,"action graph. build Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1460,usability,help,help,1460,"uild Builds the specified targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1482,usability,command,command,1482,"d targets. canonicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Curr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1498,usability,help,help,1498,"onicalize-flags Canonicalizes a list of bazel options. clean Removes output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1561,usability,help,help,1561,"moves output files and optionally stops the server. coverage Generates code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1631,usability,help,help,1631," code coverage report for specified test targets. cquery Loads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1688,usability,command,command,1688,"ads, analyzes, and queries the specified targets w/ configurations. dump Dumps the internal state of the bazel server process. fetch Fetches external repositories that are prerequisites to the targets. help Prints help for commands, or the index. info Displays runtime info about the bazel server. license Prints the license of this software. mobile-install Installs targets to mobile devices. print_action Prints the command line args for compiling a file. query Executes a dependency graph query. run Runs the specified target. shutdown Stops the bazel server. sync Syncs all repositories specified in the workspace file. test Builds and runs the specified test targets. version Prints version information for bazel. Getting more help:. bazel help <command>. Prints help and options for <command>. bazel help startup_options. Options for the JVM hosting bazel. bazel help target-syntax. Explains the syntax for specifying targets. bazel help info-keys. Displays a list of keys used by the info command. ```. What should we do next? ```. > ./build-prereq.sh. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting. ========== This script is only maintained for Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2978,usability,prefer,prefer,2978,"r Ubuntu 20.04. ========== Load config settings. ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting. ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pypar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3112,usability,user,user,3112," [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ER",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3166,usability,behavi,behaviour,3166,"list' starting. ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting. % Total % Received % Xferd Average Speed Time Time Time Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3478,usability,ERROR,ERROR,3478," Current. Dload Upload Total Spent Left Speed. 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k. Collecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3590,usability,behavi,behaviour,3590,"ollecting pip. Using cached pip-23.1.2-py3-none-any.whl (2.1 MB). Installing collected packages: pip. Attempting uninstall: pip. Found existing installation: pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3750,usability,ERROR,ERROR,3750,"pip 23.1.2. Uninstalling pip-23.1.2:. Successfully uninstalled pip-23.1.2. WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3862,usability,behavi,behaviour,3862,".8 are installed in '/root/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. Successfully installed pip-23.1.2. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4112,usability,ERROR,ERROR,4112,"r can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4224,usability,behavi,behaviour,4224," use a virtual environment instead: https://pip.pypa.io/warnings/venv. Python 3.8.10. pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4738,usability,ERROR,ERROR,4738,"patible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-prot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4850,usability,behavi,behaviour,4850,". This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5003,usability,ERROR,ERROR,5003,"patible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5115,usability,behavi,behaviour,5115,"ip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM E",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5360,usability,ERROR,ERROR,5360,"incompatible. tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:5472,usability,behavi,behaviour,5472,"tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible. tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible. Installing standard CPU-only TensorFlow 2.11.0 wheel. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible. ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already instal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6888,usability,support,support,6888,"ompatible. pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6912,usability,support,support,6912,"quires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:6994,usability,support,support,6994,"===== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:7018,usability,support,support,7018,"0:22:31 PM EDT] Stage 'run-prereq.sh complete' starting. ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting. Calling wait_for_dpkg_lock. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE defaul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:7318,usability,interact,interactively,7318,"3 10:22:32 PM EDT] Stage 'Install bazel' starting. WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on. Bazel 5.3.0 already installed on the machine, not reinstalling. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:7533,usability,command,command,7533," 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting. CLIF already installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual envi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:7605,usability,support,support,7605,"y installed. ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1. You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:7810,usability,support,support,7810,"You have bazel 5.3.0 installed. Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8069,usability,support,support,8069,"download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8118,usability,support,support,8118," [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8401,usability,user,user,8401,"RKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8455,usability,behavi,behaviour,8455,"figs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details. 	--config=mkl 	# Build with MKL support. 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL). 	--config=monolithic 	# Config for mostly static monolithic build. 	--config=numa 	# Build with NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8800,usability,ERROR,ERROR,8800,"h NUMA support. 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects. 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:8912,usability,behavi,behaviour,8912,"1 	# Build with TensorFlow 1 API instead of TF 2 API. Preconfigured Bazel build configs to DISABLE default on features:. 	--config=nogcp 	# Disable GCP support. 	--config=nonccl 	# Disable NVIDIA NCCL support. Configuration finished. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:9204,usability,user,user,9204," pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:9258,usability,behavi,behaviour,9258,"nstallation: pyparsing 3.0.9. Uninstalling pyparsing-3.0.9:. Successfully uninstalled pyparsing-3.0.9. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:9642,usability,user,user,9642,"ackages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:9696,usability,behavi,behaviour,9696,"ing cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10041,usability,ERROR,ERROR,10041,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10153,usability,behavi,behaviour,10153,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10445,usability,user,user,10445,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10499,usability,behavi,behaviour,10499,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:10838,usability,error,error,10838,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting. Found existing installation: pyparsing 2.2.0. Uninstalling pyparsing-2.2.0:. Successfully uninstalled pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8). Collecting pyparsing==2.2.0. Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB). Installing collected packages: pyparsing. ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible. Successfully installed pyparsing-2.2.0. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0. exec /opt/deepvariant/bin/run_deepvariant: exec format error. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,availability,error,errors,28,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:118,deployability,version,version,118,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:118,integrability,version,version,118,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:62,modifiability,pac,packages,62,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:110,modifiability,pac,package,110,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:118,modifiability,version,version,118,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,performance,error,errors,28,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,safety,error,errors,28,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,usability,error,errors,28,I am tempted to fix all the errors by uninstalling the python packages and reinstalling based on the required package version.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:98,deployability,build,building,98,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:297,deployability,version,version,297,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:282,integrability,wrap,wraps,282,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:297,integrability,version,version,297,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:347,integrability,translat,translation,347,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:347,interoperability,translat,translation,347,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:297,modifiability,version,version,297,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:337,modifiability,layer,layers,337,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:155,availability,Down,Downloaded,155,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:73,deployability,instal,installed,73,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:92,deployability,Instal,Installed,92,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:112,deployability,Instal,Installed,112,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:144,deployability,Modul,Module,144,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:242,deployability,version,version,242,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:273,deployability,build,build,273,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:581,deployability,build,build,581,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:821,deployability,version,version,821,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1413,deployability,build,build,1413," will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1597,deployability,instal,installation,1597," compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1723,deployability,build,build,1723,"a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1845,deployability,build,build,1845,"ython package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1990,deployability,build,build,1990,"epVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2157,deployability,patch,patching,2157,"/configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2262,deployability,build,build,2262,"separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants.zip-0.params. freeze_graph. freeze_graph.temp. ... ```. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2794,deployability,Build,Build,2794,"separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants.zip-0.params. freeze_graph. freeze_graph.temp. ... ```. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:170,integrability,configur,configured,170,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:242,integrability,version,version,242,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:402,integrability,event,eventually,402,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:728,integrability,coupl,couple,728,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:821,integrability,version,version,821,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1119,integrability,configur,configure,1119,"e Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1162,integrability,configur,configure,1162,"nd configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:511,interoperability,share,shared,511,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:557,interoperability,incompatib,incompatible,557,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:144,modifiability,Modul,Module,144,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:170,modifiability,configur,configured,170,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:242,modifiability,version,version,242,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:728,modifiability,coupl,couple,728,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:821,modifiability,version,version,821,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:854,modifiability,pac,package,854,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1119,modifiability,configur,configure,1119,"e Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1162,modifiability,configur,configure,1162,"nd configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1232,performance,perform,perform,1232,"roper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2684,performance,time,time,2684,"separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants.zip-0.params. freeze_graph. freeze_graph.temp. ... ```. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,safety,compl,completed,28,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:144,safety,Modul,Module,144,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1275,safety,isol,isolate,1275,"the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1698,safety,compl,complete,1698,"e surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Cr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2157,safety,patch,patching,2157,"/configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2800,safety,compl,completed,2800,"separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants.zip-0.params. freeze_graph. freeze_graph.temp. ... ```. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:28,security,compl,completed,28,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:170,security,configur,configured,170,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1119,security,configur,configure,1119,"e Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1162,security,configur,configure,1162,"nd configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1275,security,iso,isolate,1275,"the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1698,security,compl,complete,1698,"e surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Cr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2157,security,patch,patching,2157,"/configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2800,security,compl,completed,2800,"separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```. ... bazel-bin/deepvariant/postprocess_variants. bazel-bin/deepvariant/postprocess_variants.zip. bazel-bin/deepvariant/runtime_by_region_vis. bazel-bin/deepvariant/runtime_by_region_vis.zip. bazel-bin/deepvariant/show_examples. bazel-bin/deepvariant/show_examples.zip. bazel-bin/deepvariant/vcf_stats_report. bazel-bin/deepvariant/vcf_stats_report.zip. (09:13:54) INFO: Elapsed time: 227.413s, Critical Path: 215.22s. (09:13:54) INFO: 47 processes: 1 internal, 46 local. (09:13:54) INFO: Build completed successfully, 47 total actions. $. ```. The zip files will be under the `bazel-bin/deepvariant/` folder, which will look something like this:. ```. $ ls bazel-bin/deepvariant/ . call_variants. call_variants_keras. call_variants_keras.temp. call_variants_keras.zip. call_variants_keras.zip-0.params. call_variants.temp. call_variants.zip. call_variants.zip-0.params. freeze_graph. freeze_graph.temp. ... ```. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:728,testability,coupl,couple,728,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1275,testability,isol,isolate,1275,"the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:695,usability,minim,minimize,695,". So from what I see you've completed the following steps:. 1) Built and installed CLIF. 2) Installed Bazel. 3) Installed the Tensorflow Python Module. 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1232,usability,perform,perform,1232,"roper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```. # Assuming you are in the DeepVariant directory. cd ../tensorflow. git checkout origin/r2.11 -f. # Here configure like before with the defaults. ./configure. cd ../deepvariant. ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```. #!/bin/bash. source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries. ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf. ```. ```. #!/bin/bash. source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio. ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:350,availability,error,error,350,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:363,availability,ERROR,ERROR,363,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:373,availability,error,error,373,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1411,availability,Error,Error,1411,"tory 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1451,availability,Error,Error,1451,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1480,availability,Error,Error,1480,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1530,availability,ERROR,ERROR,1530,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2624,availability,Error,Error,2624,"xternal:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2664,availability,Error,Error,2664,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2693,availability,Error,Error,2693,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build abort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3637,availability,ERROR,ERROR,3637,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3714,availability,Error,Error,3714,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4215,availability,error,error-free,4215,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:106,deployability,instal,installation,106,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:241,deployability,build,build,241,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1389,deployability,fail,fail,1389,"ing the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1460,deployability,fail,fail,1460,"ent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1466,deployability,Configurat,Configuration,1466,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2602,deployability,fail,fail,2602,"thon_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2673,deployability,fail,fail,2673,"ent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2679,deployability,Configurat,Configuration,2679,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3677,deployability,fail,failed,3677,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3685,deployability,build,build,3685,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3700,deployability,Configurat,Configuration,3700,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3833,deployability,FAIL,FAILED,3833,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3841,deployability,Build,Build,3841,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4134,deployability,updat,updating,4134,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3889,energy efficiency,load,loaded,3889,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4174,energy efficiency,GPU,GPU,4174,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:408,integrability,repositor,repository,408,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1466,integrability,Configur,Configuration,1466,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2679,integrability,Configur,Configuration,2679,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2749,integrability,Repositor,Repository,2749,"8033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3442,integrability,Repositor,Repository,3442,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3700,integrability,Configur,Configuration,3700,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3910,integrability,configur,configured,3910,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:408,interoperability,repositor,repository,408,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2749,interoperability,Repositor,Repository,2749,"8033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3442,interoperability,Repositor,Repository,3442,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1466,modifiability,Configur,Configuration,1466,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2679,modifiability,Configur,Configuration,2679,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3700,modifiability,Configur,Configuration,3700,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3880,modifiability,pac,packages,3880,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3910,modifiability,configur,configured,3910,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:350,performance,error,error,350,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:363,performance,ERROR,ERROR,363,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:373,performance,error,error,373,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:498,performance,cach,cache,498,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:734,performance,cach,cache,734,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:978,performance,cach,cache,978,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1227,performance,cach,cache,1227,"/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e0339027",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1411,performance,Error,Error,1411,"tory 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1451,performance,Error,Error,1451,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1480,performance,Error,Error,1480,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1530,performance,ERROR,ERROR,1530,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1711,performance,cach,cache,1711," 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1947,performance,cach,cache,1947,"on_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2191,performance,cach,cache,2191," python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2440,performance,cach,cache,2440," msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2624,performance,Error,Error,2624,"xternal:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2664,performance,Error,Error,2664,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2693,performance,Error,Error,2693,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build abort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2868,performance,cach,cache,2868,"impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete success",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3009,performance,cach,cache,3009,"/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting ther",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3161,performance,cach,cache,3161,"n_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and impleme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3311,performance,cach,cache,3311,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3500,performance,cach,cache,3500,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3637,performance,ERROR,ERROR,3637,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3714,performance,Error,Error,3714,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3778,performance,time,time,3778,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3889,performance,load,loaded,3889,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3976,performance,time,time,3976,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4174,performance,GPU,GPU,4174,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4215,performance,error,error-free,4215,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1389,reliability,fail,fail,1389,"ing the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1460,reliability,fail,fail,1460,"ent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2602,reliability,fail,fail,2602,"thon_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2673,reliability,fail,fail,2673,"ent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3677,reliability,fail,failed,3677,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3833,reliability,FAIL,FAILED,3833,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:350,safety,error,error,350,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:363,safety,ERROR,ERROR,363,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:373,safety,error,error,373,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1411,safety,Error,Error,1411,"tory 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1451,safety,Error,Error,1451,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1480,safety,Error,Error,1480,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1530,safety,ERROR,ERROR,1530,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2624,safety,Error,Error,2624,"xternal:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2664,safety,Error,Error,2664,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2693,safety,Error,Error,2693,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build abort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3637,safety,ERROR,ERROR,3637,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3714,safety,Error,Error,3714,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3855,safety,compl,complete,3855,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4134,safety,updat,updating,4134,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4215,safety,error,error-free,4215,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1466,security,Configur,Configuration,1466,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2679,security,Configur,Configuration,2679,"st):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3700,security,Configur,Configuration,3700,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3855,security,compl,complete,3855,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3910,security,configur,configured,3910,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4134,security,updat,updating,4134,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:443,testability,Trace,Traceback,443,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1656,testability,Trace,Traceback,1656,"l. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:136,usability,user,user,136,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:217,usability,command,command,217,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:350,usability,error,error,350,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:363,usability,ERROR,ERROR,363,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:373,usability,error,error,373,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:492,usability,user,user,492,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:728,usability,user,user,728,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:972,usability,user,user,972,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder. Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`. The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1221,usability,user,user,1221,"d `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:. ```. ERROR: An error occurred during the fetch of repository 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1411,usability,Error,Error,1411,"tory 'local_config_python':. Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1451,usability,Error,Error,1451,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1480,usability,Error,Error,1480,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1530,usability,ERROR,ERROR,1530,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1543,usability,user,user,1543,"f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1548,usability,Document,Documents,1548,"a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1705,usability,user,user,1705,"y_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1941,usability,user,user,1941,"x, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2185,usability,user,user,2185,": %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2434,usability,user,user,2434,"_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2624,usability,Error,Error,2624,"xternal:local_config_python: Traceback (most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2664,usability,Error,Error,2664,"most recent call last):. 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2693,usability,Error,Error,2693,"e ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build abort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2791,usability,user,user,2791,"hird_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2796,usability,Document,Documents,2796,"ty/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:2862,usability,user,user,2862,"toconf_impl. 		_create_local_python_repository(repository_ctx). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3003,usability,user,user,3003,"xternal/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository. 		_check_python_lib(repository_ctx, python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3155,usability,user,user,3155,", python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3305,usability,user,user,3305,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3494,usability,user,user,3494,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3637,usability,ERROR,ERROR,3637,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:3714,usability,Error,Error,3714,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4178,usability,command,command,4178,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:4215,usability,error,error-free,4215,"90275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib. 		auto_config_fail(""Invalid python library path: %s"" % python_lib). 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail. 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)). Error in fail: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Repository go_sdk instantiated at:. /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk. Repository rule _go_download_sdk defined at:. /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>. (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: "". (11:50:12) INFO: Elapsed time: 0.552s. (11:50:12) INFO: 0 processes. (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\. argets configured). ```. Do you have any suggestions? Thank you for your time! Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull. After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:300,deployability,build,build,300,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:361,deployability,build,build,361,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:430,deployability,build,build,430,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:476,deployability,build,build,476,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:512,deployability,build,build,512,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1111,deployability,build,building,1111,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:674,energy efficiency,gpu,gpu,674,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:744,energy efficiency,gpu,gpu,744,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:801,energy efficiency,gpu,gpu,801,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:879,energy efficiency,gpu,gpu,879,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:282,modifiability,pac,packages,282,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:419,modifiability,pac,packages,419,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1171,modifiability,pac,packages,1171,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:674,performance,gpu,gpu,674,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:744,performance,gpu,gpu,744,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:801,performance,gpu,gpu,801,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:879,performance,gpu,gpu,879,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:1052,performance,content,contents,1052,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:553,safety,test,test,553,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:583,safety,test,test,583,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:622,safety,test,test,622,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:660,safety,test,test,660,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:691,safety,test,test,691,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:730,safety,test,test,730,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:749,safety,test,test,749,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:787,safety,test,test,787,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:826,safety,test,test,826,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:865,safety,test,test,865,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:916,safety,permiss,permissions,916,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:498,security,sign,sign-compare,498,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:539,security,sign,sign-compare,539,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:553,testability,test,test,553,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:583,testability,test,test,583,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:622,testability,test,test,622,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:660,testability,test,test,660,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:691,testability,test,test,691,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:730,testability,test,test,730,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:749,testability,test,test,749,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:787,testability,test,test,787,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:826,testability,test,test,826,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:865,testability,test,test,865,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:939,usability,user,username,939,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```. build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3"". build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages"". build --python_path=""/usr/local/bin/python3"". build:opt --copt=-Wno-sign-compare. build:opt --host_copt=-Wno-sign-compare. test --flaky_test_attempts=3. test --test_size_filters=small,medium. test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial. test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu. test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only. test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only. ```. It will have these permissions (with your username instead of mine):. ```. -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc. ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:104,energy efficiency,current,currently,104,"Thanks @pgrosu for helping out! One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:76,safety,test,tested,76,"Thanks @pgrosu for helping out! One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:76,testability,test,tested,76,"Thanks @pgrosu for helping out! One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:19,usability,help,helping,19,"Thanks @pgrosu for helping out! One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:143,usability,support,support,143,"Thanks @pgrosu for helping out! One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:224,deployability,instal,installation,224,"Thank you for saying these words, @pichuan. I believe we are at the very last steps of getting the DeepVariant running on Apple silicon, but since the singularity on linux system works for us, I think I will not pursue this installation anymore. I am grateful to @pgrosu who has been helping me with these 2 options. . Perhaps this thread could help others who want to pursue the same pathway. Regards,. Hez",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:284,usability,help,helping,284,"Thank you for saying these words, @pichuan. I believe we are at the very last steps of getting the DeepVariant running on Apple silicon, but since the singularity on linux system works for us, I think I will not pursue this installation anymore. I am grateful to @pgrosu who has been helping me with these 2 options. . Perhaps this thread could help others who want to pursue the same pathway. Regards,. Hez",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:345,usability,help,help,345,"Thank you for saying these words, @pichuan. I believe we are at the very last steps of getting the DeepVariant running on Apple silicon, but since the singularity on linux system works for us, I think I will not pursue this installation anymore. I am grateful to @pgrosu who has been helping me with these 2 options. . Perhaps this thread could help others who want to pursue the same pathway. Regards,. Hez",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:76,usability,close,close,76,Thanks @heznanda for letting us know that you have a working solution. I'll close this issue. Other users can still find it later if they search of relevant keywords :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:100,usability,user,users,100,Thanks @heznanda for letting us know that you have a working solution. I'll close this issue. Other users can still find it later if they search of relevant keywords :),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:271,interoperability,platform,platforms,271,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:178,performance,time,times,178,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:366,performance,time,time,366,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:70,usability,help,helpful,70,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:208,usability,clarit,clarity,208,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/657:414,usability,help,help,414,"Thank you for the kind words @heznanda and @pichuan. I am glad it was helpful, and might be to others as well. . The nice thing about having now gone through this exercise a few times is that it has given me clarity of the moving parts, that porting DeepVariant to other platforms should be a fairly trivial task, having multiple avenues to success. In any case, as time permits I'm always here if folks need more help. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/657
https://github.com/google/deepvariant/issues/659:252,deployability,pipelin,pipeline,252,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:336,deployability,log,log,336,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:252,integrability,pipelin,pipeline,252,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:340,integrability,messag,messages,340,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:354,integrability,messag,message,354,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:340,interoperability,messag,messages,340,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:354,interoperability,messag,message,354,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:336,safety,log,log,336,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:336,security,log,log,336,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:336,testability,log,log,336,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:440,testability,context,context,440,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:70,usability,command,command,70,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:172,usability,help,help,172,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:451,usability,help,help,451,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:585,usability,command,command,585,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:. 1. Did you get this Nextflow pipeline from somewhere online or write it yourself? 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this. 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run. 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,availability,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,availability,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3980,availability,error,error,3980, nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,availability,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4307,availability,error,error,4307,[Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,availability,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4634,availability,error,error,4634,run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,availability,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4896,availability,error,error,4896,n below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,availability,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5031,availability,ERROR,ERROR,5031,line/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 example,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5072,availability,Error,Error,5072,11]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I06,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5176,availability,error,error,5176, to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7153,availability,operat,operations,7153,"in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7219,availability,operat,operations,7219,"8661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9813,availability,error,error,9813,"les_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11390,availability,operat,operations,11390,"in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,availability,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12654,availability,error,error,12654,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:60,deployability,pipelin,pipeline,60,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:121,deployability,log,log,121,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1364,deployability,Version,Version,1364,.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1381,deployability,build,build,1381,acade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1773,deployability,Pipelin,Pipeline,1773,@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' >,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1938,deployability,Pipelin,Pipeline,1938,3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_v,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2107,deployability,Observ,Observer,2107, Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,deployability,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3342,deployability,contain,container,3342,ating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,deployability,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4030,deployability,Pipelin,Pipeline,4030,Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERR,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,deployability,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4357,deployability,Pipelin,Pipeline,4357,tyCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,deployability,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4684,deployability,Pipelin,Pipeline,4684,flow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,deployability,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4946,deployability,Pipelin,Pipeline,4946,1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,deployability,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5296,deployability,resourc,resources,5296,t: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6724,deployability,resourc,resources,6724,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8193,deployability,Fail,Failed,8193,"o enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8486,deployability,modul,module,8486,"Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10961,deployability,resourc,resources,10961,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12352,deployability,Pipelin,Pipeline,12352,"tructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12446,deployability,continu,continue,12446,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,deployability,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12983,deployability,fail,failedCount,12983,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13132,deployability,fail,failedDuration,13132,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1617,energy efficiency,CPU,CPUs,1617,tflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.Executo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,energy efficiency,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2778,energy efficiency,cpu,cpus,2778,e/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,energy efficiency,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,energy efficiency,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,energy efficiency,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,energy efficiency,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,energy efficiency,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5296,energy efficiency,resourc,resources,5296,t: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6724,energy efficiency,resourc,resources,6724,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6966,energy efficiency,core,core,6966,"066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Wri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7032,energy efficiency,optim,optimized,7032,"n 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postproce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7112,energy efficiency,CPU,CPU,7112,"_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10961,energy efficiency,resourc,resources,10961,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11203,energy efficiency,core,core,11203,"066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/Long",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11269,energy efficiency,optim,optimized,11269,"n 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11349,energy efficiency,CPU,CPU,11349,"_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,energy efficiency,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13175,energy efficiency,load,loadCpus,13175,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13187,energy efficiency,load,loadMemory,13187,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:60,integrability,pipelin,pipeline,60,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:710,integrability,Discover,Discovered,710,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1364,integrability,Version,Version,1364,.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1773,integrability,Pipelin,Pipeline,1773,@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' >,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1938,integrability,Pipelin,Pipeline,1938,3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_v,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3544,integrability,sub,submitter,3544, execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3668,integrability,sub,submitter,3668,05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinica,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3715,integrability,Sub,Submitted,3715,itor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3875,integrability,sub,submitted,3875, DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMP,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4030,integrability,Pipelin,Pipeline,4030,Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERR,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4202,integrability,sub,submitted,4202,ng termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4357,integrability,Pipelin,Pipeline,4357,tyCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clin,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4529,integrability,sub,submitted,4529,011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4684,integrability,Pipelin,Pipeline,4684,flow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4946,integrability,Pipelin,Pipeline,4946,1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5677,integrability,batch,batches,5677,ad/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5799,integrability,batch,batches,5799,"or - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5921,integrability,batch,batches,5921,"ed/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6043,integrability,batch,batches,6043,"rocessor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6165,integrability,batch,batches,6165,"ith an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6287,integrability,batch,batches,6287,"Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I060",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6409,integrability,batch,batches,6409,"utput_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7850,integrability,Transform,Transforming,7850,"al/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9914,integrability,batch,batches,9914,"azel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10036,integrability,batch,batches,10036,"iants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10158,integrability,batch,batches,10158,"ne 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10280,integrability,batch,batches,10280,"third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10402,integrability,batch,batches,10402,"s). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10524,integrability,batch,batches,10524,".py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10646,integrability,batch,batches,10646,"not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12352,integrability,Pipelin,Pipeline,12352,"tructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13045,integrability,sub,submittedCount,13045,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:369,interoperability,plug,plugin,369,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:376,interoperability,Plug,PluginsFacade,376,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:392,interoperability,Plug,Plugins,392,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:454,interoperability,plug,plugin,454,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:461,interoperability,Plug,PluginsFacade,461,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:477,interoperability,Plug,Plugins,477,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:710,interoperability,Discover,Discovered,710,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1748,interoperability,share,shared,1748,ecret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task moni,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1913,interoperability,share,shared,1913,1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3457,interoperability,share,shared,3457,ession start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4005,interoperability,share,shared,4005,ing dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4332,interoperability,share,shared,4332,xtflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4659,interoperability,share,shared,4659,[Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4921,interoperability,share,shared,4921,d: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 bat,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5266,interoperability,share,shared,5266,ricall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 35000,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5350,interoperability,share,shared,5350,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5429,interoperability,share,shared,5429, monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6694,interoperability,share,shared,6694,"er 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 1405244",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6843,interoperability,share,shared,6843,"042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6971,interoperability,platform,platform,6971,"794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7850,interoperability,Transform,Transforming,7850,"al/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8091,interoperability,share,shared,8091,"use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8220,interoperability,share,shared,8220,"ations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8582,interoperability,platform,platform,8582,"ariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.Vcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9680,interoperability,share,shared,9680,"f). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10931,interoperability,share,shared,10931,"er 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11080,interoperability,share,shared,11080,"042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_wr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11208,interoperability,platform,platform,11208,"794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Da",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12185,interoperability,share,shared,12185,": I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12327,interoperability,share,shared,12327," use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1364,modifiability,Version,Version,1364,.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2021,modifiability,Extens,Extension,2021,s_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow netwo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5247,modifiability,PAC,PACBIO,5247,id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:4,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8486,modifiability,modul,module,8486,"Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8555,modifiability,pac,packages,8555,"08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1617,performance,CPU,CPUs,1617,tflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.Executo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2191,performance,cach,cache,2191, pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunn,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2197,performance,Cach,CacheFactory,2197,leTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Aw,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2227,performance,cach,cache,2227,Size=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2251,performance,cach,cache,2251,edBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBU,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2778,performance,cpu,cpus,2778,e/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task m,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2787,performance,memor,memory,2787,t2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DE,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3021,performance,network,network,3021,nsion executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/Long,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3482,performance,cach,cache,3482,2:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3980,performance,error,error,3980, nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4307,performance,error,error,4307,[Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4634,performance,error,error,4634,run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4896,performance,error,error,4896,n below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5031,performance,ERROR,ERROR,5031,line/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 example,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5072,performance,Error,Error,5072,11]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I06,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5176,performance,error,error,5176, to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5296,performance,resourc,resources,5296,t: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5677,performance,batch,batches,5677,ad/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5799,performance,batch,batches,5799,"or - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5921,performance,batch,batches,5921,"ed/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6043,performance,batch,batches,6043,"rocessor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6165,performance,batch,batches,6165,"ith an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6287,performance,batch,batches,6287,"Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I060",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6409,performance,batch,batches,6409,"utput_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: H",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6634,performance,time,time,6634,"py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6724,performance,resourc,resources,6724,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7032,performance,optimiz,optimized,7032,"n 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postproce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7066,performance,Network,Network,7066,". I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing outpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7112,performance,CPU,CPU,7112,"_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7132,performance,perform,performance-critical,7132,"300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9813,performance,error,error,9813,"les_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9914,performance,batch,batches,9914,"azel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10036,performance,batch,batches,10036,"iants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10158,performance,batch,batches,10158,"ne 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10280,performance,batch,batches,10280,"third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10402,performance,batch,batches,10402,"s). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10524,performance,batch,batches,10524,".py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10646,performance,batch,batches,10646,"not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10871,performance,time,time,10871,"py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10961,performance,resourc,resources,10961,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11269,performance,optimiz,optimized,11269,"n 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11303,performance,Network,Network,11303,". I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11349,performance,CPU,CPU,11349,"_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11369,performance,perform,performance-critical,11369,"300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12654,performance,error,error,12654,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13014,performance,cach,cachedCount,13014,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13156,performance,cach,cachedDuration,13156,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13175,performance,load,loadCpus,13175,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13187,performance,load,loadMemory,13187,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13287,performance,cach,cache,13287,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13293,performance,Cach,CacheDB,13293,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13311,performance,Cach,CacheDB,13311,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1869,reliability,doe,does,1869,main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.8,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,reliability,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,reliability,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,reliability,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,reliability,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,reliability,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,reliability,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8193,reliability,Fail,Failed,8193,"o enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,reliability,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12983,reliability,fail,failedCount,12983,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13132,reliability,fail,failedDuration,13132,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:121,safety,log,log,121,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:141,safety,Input,Input,141,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:189,safety,input,input,189,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,safety,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,safety,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3859,safety,compl,completed,3859,02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3980,safety,error,error,3980, nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,safety,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4186,safety,compl,completed,4186,unner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit stat,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4307,safety,error,error,4307,[Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,safety,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4513,safety,compl,completed,4513,Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_sha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4634,safety,error,error,4634,run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,safety,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4813,safety,compl,completed,4813,onitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4876,safety,COMPL,COMPLETED,4876,ted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4896,safety,error,error,4896,n below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,safety,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5031,safety,ERROR,ERROR,5031,line/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 example,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5072,safety,Error,Error,5072,11]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I06,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5176,safety,error,error,5176, to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5296,safety,resourc,resources,5296,t: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6724,safety,resourc,resources,6724,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8486,safety,modul,module,8486,"Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9813,safety,error,error,9813,"les_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10961,safety,resourc,resources,10961,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,safety,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12654,safety,error,error,12654,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12939,safety,compl,completed,12939,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13401,safety,compl,complete,13401,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:121,security,log,log,121,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:892,security,Session,Session,892,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:902,security,Session,Session,902,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:996,security,Session,Session,996,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1078,security,Session,Session,1078," me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1722,security,Session,Session,1722,ts providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor -,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:1842,security,Session,Session,1842,96. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInter,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2097,security,Session,Session,2097, size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor -,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2449,security,Session,Session,2449,: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/d,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2459,security,Session,Session,2459,4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2896,security,Session,Session,2896,ctory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2993,security,Session,Session,2993,cutor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3021,security,network,network,3021,nsion executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/Long,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3265,security,Session,Session,3265,Factory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_va,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3275,security,Session,Session,3275,un-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3693,security,Session,Session,3693,ocessor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3859,security,compl,completed,3859,02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4186,security,compl,completed,4186,unner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit stat,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4513,security,compl,completed,4513,Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_sha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4813,security,compl,completed,4813,onitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4876,security,COMPL,COMPLETED,4876,ted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7066,security,Network,Network,7066,". I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing outpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11303,security,Network,Network,11303,". I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12572,security,Session,Session,12572,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12582,security,Session,Session,12582,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12719,security,Session,Session,12719,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12729,security,Session,Session,12729,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12811,security,Session,Session,12811,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12821,security,Session,Session,12821,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12841,security,barrier,barriers,12841,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12939,security,compl,completed,12939,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:13401,security,compl,complete,13401,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:121,testability,log,log,121,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2107,testability,Observ,Observer,2107, Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2747,testability,monitor,monitor,2747,red/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3779,testability,monitor,monitor,3779,96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4106,testability,monitor,monitor,4106, process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4433,testability,monitor,monitor,4433,t:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4760,testability,monitor,monitor,4760,7:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5022,testability,monitor,monitor,5022,ead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5296,testability,resourc,resources,5296,t: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6724,testability,resourc,resources,6724,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:8330,testability,Trace,Traceback,8330,"s_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. [E::hts_open_format] Failed to open file ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1112, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 312, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 17",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10961,testability,resourc,resources,10961,"39794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12548,testability,monitor,monitor,12548,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12900,testability,trace,trace,12900,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:141,usability,Input,Input,141,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:189,usability,input,input,189,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:710,usability,Discov,Discovered,710,"Hi @MariaNattestad ,. Thank you for the reply,. 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out. ```. Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]. Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]. Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json. Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright. Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96. Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false. Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2325,usability,Custom,CustomThreadPool,2325,] DEBUG nextflow.cli.CmdRun -. Version: 22.10.7 build 5853. Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT). System: Linux 5.4.0-146-generic. Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04. Encoding: UTF-8 (UTF-8). Process: 683315@victor [127.0.1.1]. CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB). Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2787,usability,memor,memory,2787,t2/ext3]. Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DE,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:2906,usability,Workflow,Workflow,2906,ta/shared/clinical/LongRead/Pipeline/bin. Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]. Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory. Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory. Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000. Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start. Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution. Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~>,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3629,usability,command,command,3629,taskConfig executor: null. Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall. Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -;,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3954,usability,statu,status,3954, 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/wo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:3980,usability,error,error,3980, nextflow.Session - Igniting dataflow network (2). Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall. Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination. Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4281,usability,statu,status,4281,wait. Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/Lo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4307,usability,error,error,4307,[Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img. Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4608,usability,statu,status,4608,e: /bin/bash -ue .command.run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 13979,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4634,usability,error,error,4634,run. Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1). Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4868,usability,statu,status,4868, -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:4896,usability,error,error,4896,n below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5031,usability,ERROR,ERROR,5031,line/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 example,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5072,usability,Error,Error,5072,11]. Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I06,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5176,usability,error,error,5176, to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5187,usability,statu,status,5187,leted: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 10,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5199,usability,Command,Command,5199,ubmitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5542,usability,Command,Command,5542,re shown below. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 exa,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5555,usability,statu,status,5555,ow. ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5567,usability,Command,Command,5567,ndler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]. Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:. Process `pbc_varicall (1)` terminated with an error exit status (1). Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:. 1. Command output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. use,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6568,usability,user,user,6568,"mand output:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:6619,usability,command,command,6619,"all_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:7132,usability,perform,performance-critical,7132,"300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002. 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz. 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233. I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes. I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants. I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF. I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9776,usability,user,user,9776,"in(main, args). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 39023",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9805,usability,Command,Command,9805,"l.runfiles_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:9813,usability,error,error,9813,"les_21tufdoh/runfiles/absl_py/absl/app.py"", line 258, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in main. write_variants_to_vcf(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Command error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10805,usability,user,user,10805,"mmand error:. I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:10856,usability,command,command,10856,"all_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]. I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]. I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]. I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 examples in 489 batches [0.080 sec per 100]. I0608 12:15:59.773940 139794368661312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:11369,usability,perform,performance-critical,11369,"300001 examples in 586 batches [0.079 sec per 100]. I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]. I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]. I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s. user 294m6.601s. sys 10m18.739s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12281,usability,user,user,12281," oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12401,usability,Tip,Tip,12401,"VX2 AVX512F AVX512_VNNI FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12489,usability,resum,resume,12489,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12508,usability,command,command,12508,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12654,usability,error,error,12654,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12665,usability,statu,status,12665,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12906,usability,Workflow,WorkflowStatsObserver,12906,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12930,usability,Workflow,Workflow,12930,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:12951,usability,Workflow,WorkflowStats,12951,"I FMA. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf. with vcf.VcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer. return NativeVcfWriter(. File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__. self._writer = vcf_writer.VcfWriter.to_file(output_path, header,. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s. user 0m8.421s. sys 0m8.363s. Work dir:. /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line. Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1). Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished. Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed. Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]. Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done. Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:24,availability,error,error,24,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:30,integrability,messag,message,30,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:30,interoperability,messag,message,30,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:105,interoperability,share,shared,105,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:358,interoperability,share,shared,358,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:24,performance,error,error,24,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:24,safety,error,error,24,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:24,usability,error,error,24,"Looks like the relevant error message is:. ```. ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. ```. On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```. mkdir -p /data/shared/clinical/LongRead/Data//Analysis/. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/659:5,usability,close,close,5,I'll close this issue now. @kiranpatil222 feel free to reopen if you have more questions.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/659
https://github.com/google/deepvariant/issues/660:32,interoperability,share,share,32,"@PengJia6 ,. Hi, can you please share the command that you are running for the PacBio HiFi BAM file? Also, are you saying the variant is reported with HiFi data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:79,modifiability,Pac,PacBio,79,"@PengJia6 ,. Hi, can you please share the command that you are running for the PacBio HiFi BAM file? Also, are you saying the variant is reported with HiFi data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:42,usability,command,command,42,"@PengJia6 ,. Hi, can you please share the command that you are running for the PacBio HiFi BAM file? Also, are you saying the variant is reported with HiFi data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1776,availability,echo,echo,1776,"st/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1].",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2272,deployability,log,log,2272,"t/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2331,deployability,log,log,2331,"""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2402,deployability,log,log,2402,"tet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3444,deployability,log,log,3444,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3452,deployability,log,log,3452,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3092,modifiability,PAC,PACBIO,3092,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:87,performance,content,contents,87,"@kishwarshafin The command is in the `deepvariant_small_test.smk` file. Below are it's contents:. ```. # ======================================================================================================================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1012,safety,input,input,1012,"The command is in the `deepvariant_small_test.smk` file. Below are it's contents:. ```. # ======================================================================================================================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1189,safety,input,input,1189,"===============================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1718,safety,input,input,1718,"/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).sp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1844,safety,input,input,1844,"ome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2272,safety,log,log,2272,"t/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2331,safety,log,log,2331,"""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2402,safety,log,log,2402,"tet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2572,safety,input,input,2572,"un:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2616,safety,input,input,2616,"}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2658,safety,input,input,2658,"end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2708,safety,input,input,2708,"O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2752,safety,input,input,2752,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2944,safety,input,input,2944,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3138,safety,input,input,3138,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3170,safety,input,input,3170,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3444,safety,log,log,3444,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3452,safety,log,log,3452,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3645,safety,input,input,3645,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3729,safety,input,input,3729,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:312,security,Auth,Author,312,"@kishwarshafin The command is in the `deepvariant_small_test.smk` file. Below are it's contents:. ```. # ======================================================================================================================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2272,security,log,log,2272,"t/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2331,security,log,log,2331,"""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2402,security,log,log,2402,"tet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3444,security,log,log,3444,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3452,security,log,log,3452,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2272,testability,log,log,2272,"t/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2331,testability,log,log,2331,"""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2402,testability,log,log,2402,"tet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--interme",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3444,testability,log,log,3444,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3452,testability,log,log,3452,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:19,usability,command,command,19,"@kishwarshafin The command is in the `deepvariant_small_test.smk` file. Below are it's contents:. ```. # ======================================================================================================================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{reg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1012,usability,input,input,1012,"The command is in the `deepvariant_small_test.smk` file. Below are it's contents:. ```. # ======================================================================================================================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1189,usability,input,input,1189,"===============================. # Project: Project_Human_iharbor. # Script : deepvariant_small_test.smk TODO check . # Author : Peng Jia. # Date : 2023/6/13. # Email : pengjia@stu.xjtu.edu.cn. # Description: TODO. # ======================================================================================================================. regions = [(""chr10"", 59597476, 59597478), (""chr10"", 89013076, 89013077), (""chr18"", 75132618, 75132618)]. samtools = ""/data/home/pengjia/miniconda3/envs/default/bin/samtools"". dir_work = ""/data/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1718,usability,input,input,1718,"/home/pengjia/Project_Human_DATA/reNBT2/01_deepvariant_test/"". path_ref_grch38 = ""/data/DATA/Reference/human/GRCh38.d1.vd1/genome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).sp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1844,usability,input,input,1844,"ome/GRCh38.d1.vd1.fa"". path_ref = ""/data/DATA/Reference/human/GRCh38_full_analysis_set_plus_decoy_hla/genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"". rule all:. input:. expand(dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",region=[f""{i[0]}_{i[1]}_{i[2]}"" for i in regions]). def TODO1(wildcard):. return. rule extract_bam:. input:. bam=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",. output:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. run:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2572,usability,input,input,2572,"un:. chrom, start, end = f""{wildcards.region}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2616,usability,input,input,2616,"}"".split(""_""). start = int(start) - 1000. end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2658,usability,input,input,2658,"end = int(end) + 1000. shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2708,usability,input,input,2708,"O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2752,usability,input,input,2752,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2944,usability,input,input,2944,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3138,usability,input,input,3138,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3170,usability,input,input,3170,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3645,usability,input,input,3645,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:3729,usability,input,input,3729,"t}-{end} > {output.bam}""). shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:. input:. bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",. bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",. bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",. ref=path_ref. output:. vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",. gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz"". # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz"". log:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". benchmark:. dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log"". threads: 48. run:. dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp"". file_tmp = dir_tmp.split(""/"")[-1]. shell(""mkdir -p "" + dir_tmp). bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]). bam_file = str(input.bam).split(""/"")[-1]. bed_file = str(input.bed).split(""/"")[-1]. ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]). ref_file = str(input.ref).split(""/"")[-1]. output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]). output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '. '-v ""{bam_dir}"":""/input"" '. '-v ""{ref_dir}"":""/ref"" '. '-v ""{output_dir}"":""/output"" '. 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '. '--model_type=PACBIO '. '--ref=/ref/{ref_file} '. '--reads=/input/{bam_file} '. '--regions /input/{bed_file} '. '--output_vcf=/output/{output_file}.vcf '. '--output_gvcf=/output/{output_file}.g.vcf '. '--num_shards={threads} '. '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '. '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'). shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""). shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:. input:. ""{preifx}.bam"". output:. ""{preifx}.bam.bai"". run:. shell(""{samtools} index {input}""). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:592,deployability,version,version,592,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:897,energy efficiency,load,load,897,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:592,integrability,version,version,592,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:592,modifiability,version,version,592,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:897,performance,load,load,897,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:921,performance,time,time,921,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:621,usability,Prefer,Preference,621,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:. ```bash. chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47. chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49. chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43. ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:. <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:484,availability,consist,consistently,484,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:637,availability,consist,consistent,637,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:122,integrability,event,events,122,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:272,integrability,event,event,272,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:396,integrability,event,event,396,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:575,performance,perform,perform,575,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1140,performance,time,time,1140,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1072,reliability,doe,does,1072,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:742,safety,valid,validated,742,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:742,security,validat,validated,742,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:41,usability,indicat,indicates,41,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:61,usability,visual,visualize,61,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:484,usability,consist,consistently,484,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:575,usability,perform,perform,575,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:637,usability,consist,consistent,637,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:820,usability,minim,minimap,820,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1179,usability,confirm,confirm,1179,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:550,energy efficiency,model,models,550,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:137,reliability,anomal,anomaly,137,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:60,safety,review,reviewed,60,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:137,safety,anomal,anomaly,137,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:550,security,model,models,550,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:60,testability,review,reviewed,60,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:277,testability,simpl,simpler,277,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:268,usability,prefer,prefer,268,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:277,usability,simpl,simpler,277,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:338,availability,consist,consistent,338,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:458,availability,replic,replicate,458,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:170,deployability,roll,rolling,170,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:686,deployability,version,version,686,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:849,deployability,releas,release,849,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:933,deployability,observ,observe,933,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1194,deployability,version,version,1194,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1301,deployability,version,version,1301,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1432,deployability,updat,update,1432,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:686,integrability,version,version,686,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1194,integrability,version,version,1194,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1301,integrability,version,version,1301,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1475,integrability,sub,substantially,1475,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:219,modifiability,paramet,parameters,219,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:686,modifiability,version,version,686,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1194,modifiability,version,version,1194,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1301,modifiability,version,version,1301,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1596,modifiability,Pac,PacBio,1596,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1432,safety,updat,update,1432,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1809,safety,compl,complex,1809,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1432,security,updat,update,1432,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1809,security,compl,complex,1809,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:804,testability,trace,trace,804,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:933,testability,observ,observe,933,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:338,usability,consist,consistent,338,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:483,usability,minim,minimap,483,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1561,usability,efficien,efficiently,1561,"Hi @PengJia6 . Please see . ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:283,availability,slo,slowing,283,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:291,availability,down,down,291,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:600,deployability,updat,updated,600,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:375,integrability,encapsulat,encapsulated,375,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1787,integrability,SUB,SUBSTITUTION,1787,"ust a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes around a specific position, you might be able to use that information from a VCF to consolidate it into one variant. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1903,integrability,SUB,SUBSTITUTION,1903,"ust a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes around a specific position, you might be able to use that information from a VCF to consolidate it into one variant. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:1976,integrability,sub,substitution,1976,"ust a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes around a specific position, you might be able to use that information from a VCF to consolidate it into one variant. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:473,interoperability,specif,specific,473,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2007,interoperability,specif,specific,2007,"ust a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes around a specific position, you might be able to use that information from a VCF to consolidate it into one variant. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:375,modifiability,encapsul,encapsulated,375,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:207,performance,perform,perform,207,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:614,performance,time,time,614,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:283,reliability,slo,slowing,283,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:600,safety,updat,updated,600,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:600,security,updat,updated,600,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:801,security,ident,identify,801,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:142,testability,simpl,simple,142,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:54,usability,minim,minimal,54,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:142,usability,simpl,simple,142,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:207,usability,perform,perform,207,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes ar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:2123,usability,help,helps,2123,"ust a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```. position {. reference_name: ""chr10"". position: 89013074. }. ref_base: ""T"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""TC"". type: INSERTION. count: 1. }. }. ... ```. ##### For Position: 89013076: . ```. position {. reference_name: ""chr10"". position: 89013075. }. ref_base: ""C"". ref_supporting_read_count: 35. read_alleles {. key: ""m64154_210327_091530/103023686/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/128910218/ccs/0"". value {. bases: ""CA"". type: DELETION. count: 1. }. }. ... ```. ##### For Position: 89013077: . ```. position {. reference_name: ""chr10"". position: 89013076. }. ref_base: ""A"". read_alleles {. key: ""m64154_210327_091530/142213575/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. read_alleles {. key: ""m64154_210327_091530/4130912/ccs/0"". value {. bases: ""C"". type: SUBSTITUTION. count: 1. }. }. ... ```. Given that the allele type (indel/substitution) changes around a specific position, you might be able to use that information from a VCF to consolidate it into one variant. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:352,deployability,upgrad,upgrade,352,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:364,deployability,version,version,364,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:399,deployability,releas,release,399,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:364,integrability,version,version,364,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:352,modifiability,upgrad,upgrade,352,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:364,modifiability,version,version,364,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:575,reliability,Doe,Does,575,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:204,security,sign,significant,204,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:71,testability,understand,understand,71,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:50,usability,clear,clear,50,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:589,usability,support,support,589,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)? Best! . Peng.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:61,availability,operat,operate,61,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:189,deployability,build,building,189,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:332,energy efficiency,model,model,332,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:119,modifiability,exten,extended,119,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:234,modifiability,extens,extensively,234,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:440,modifiability,exten,extending,440,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:485,performance,time,time,485,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:332,security,model,model,332,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/660:35,usability,help,help,35,Thank you all for your answers and help. I have received answers to all my questions! Best wishes to you all.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/660
https://github.com/google/deepvariant/issues/661:50,deployability,releas,releasing,50,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:132,deployability,version,version,132,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:282,deployability,releas,release,282,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:75,energy efficiency,model,models,75,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:96,energy efficiency,model,model,96,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:132,integrability,version,version,132,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:132,modifiability,version,version,132,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:407,modifiability,exten,extended,407,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:75,security,model,models,75,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:96,security,model,model,96,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:307,availability,error,errors,307,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:813,deployability,releas,releasing,813,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:895,deployability,version,version,895,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1045,deployability,releas,release,1045,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:453,energy efficiency,model,model,453,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:838,energy efficiency,model,models,838,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:859,energy efficiency,model,model,859,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:633,integrability,Messag,Message,633,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:895,integrability,version,version,895,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1619,integrability,Messag,Message,1619,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:633,interoperability,Messag,Message,633,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1619,interoperability,Messag,Message,1619,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:895,modifiability,version,version,895,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1175,modifiability,exten,extended,1175,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:149,performance,perform,perform,149,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:307,performance,error,errors,307,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:307,safety,error,errors,307,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:453,security,model,model,453,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:602,security,secur,secure,602,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:838,security,model,models,838,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:859,security,model,model,859,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1509,security,auth,auth,1509,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:149,usability,perform,perform,149,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:307,usability,error,errors,307,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks. Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------. On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:86,availability,state,state,86,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:404,deployability,releas,releasing,404,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:486,deployability,version,version,486,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:636,deployability,releas,release,636,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:54,energy efficiency,model,model,54,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:78,energy efficiency,current,current,78,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:429,energy efficiency,model,models,429,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:450,energy efficiency,model,model,450,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:86,integrability,state,state,86,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:253,integrability,Messag,Message,253,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:486,integrability,version,version,486,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1210,integrability,Messag,Message,1210,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:253,interoperability,Messag,Message,253,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1210,interoperability,Messag,Message,1210,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:486,modifiability,version,version,486,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:766,modifiability,exten,extended,766,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:35,performance,perform,performance,35,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:127,performance,time,time,127,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:54,security,model,model,54,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:429,security,model,models,429,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:450,security,model,model,450,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1100,security,auth,auth,1100,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:35,usability,perform,performance,35,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------. On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. >. > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this? >. > Thanks,. > Andrew. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:405,availability,avail,available,405,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:631,deployability,scale,scales,631,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:135,energy efficiency,cloud,cloud,135,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:631,energy efficiency,scale,scales,631,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:631,modifiability,scal,scales,631,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1218,modifiability,interm,intermediate,1218,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:631,performance,scale,scales,631,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:405,reliability,availab,available,405,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:572,reliability,doe,doesn,572,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:405,safety,avail,available,405,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:405,security,availab,available,405,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:477,security,assess,assessing,477,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:890,security,sign,signature,890,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:796,usability,support,support,796,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1155,usability,support,support,1155,"Hi @Axze-rgb . It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. . The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:828,availability,avail,available,828,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1059,deployability,scale,scales,1059,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:553,energy efficiency,cloud,cloud,553,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1059,energy efficiency,scale,scales,1059,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:317,integrability,Messag,Message,317,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1982,integrability,Messag,Message,1982,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:317,interoperability,Messag,Message,317,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1982,interoperability,Messag,Message,1982,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1059,modifiability,scal,scales,1059,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1654,modifiability,interm,intermediate,1654,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1059,performance,scale,scales,1059,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:828,reliability,availab,available,828,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1000,reliability,doe,doesn,1000,"oncur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.*",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:828,safety,avail,available,828,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:828,security,availab,available,828,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:905,security,assess,assessing,905,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1321,security,sign,signature,1321,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1872,security,auth,auth,1872,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1227,usability,support,support,1227,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/661:1591,usability,support,support,1591,"ur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------. On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb). >. > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),. >. > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it. >. > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions. >. > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results. >. > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future. >. > . > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE). > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/661
https://github.com/google/deepvariant/issues/664:285,energy efficiency,current,current,285,"@mwhitesi it looks like there is a related issue here: https://github.com/bazelbuild/bazel/issues/4815. I'm not sure it can be modified very easily. However, since you don't have a `/usr/bin/python3` file, I wonder if you can create a bash script that passes commands to whatever your current environment is using for `python3`? You could try to create a file that looks like this:. ```. #!/bin/bash. /usr/bin/env python3 $@. ```. Then save this as `/usr/bin/python3`, and use `sudo chmod +x /usr/bin/python3`. I can't promise it will work, but it is worth a shot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:127,security,modif,modified,127,"@mwhitesi it looks like there is a related issue here: https://github.com/bazelbuild/bazel/issues/4815. I'm not sure it can be modified very easily. However, since you don't have a `/usr/bin/python3` file, I wonder if you can create a bash script that passes commands to whatever your current environment is using for `python3`? You could try to create a file that looks like this:. ```. #!/bin/bash. /usr/bin/env python3 $@. ```. Then save this as `/usr/bin/python3`, and use `sudo chmod +x /usr/bin/python3`. I can't promise it will work, but it is worth a shot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:259,usability,command,commands,259,"@mwhitesi it looks like there is a related issue here: https://github.com/bazelbuild/bazel/issues/4815. I'm not sure it can be modified very easily. However, since you don't have a `/usr/bin/python3` file, I wonder if you can create a bash script that passes commands to whatever your current environment is using for `python3`? You could try to create a file that looks like this:. ```. #!/bin/bash. /usr/bin/env python3 $@. ```. Then save this as `/usr/bin/python3`, and use `sudo chmod +x /usr/bin/python3`. I can't promise it will work, but it is worth a shot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:40,availability,error,error,40,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:78,availability,error,errors,78,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:674,deployability,modul,module,674,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:843,deployability,modul,module,843,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:877,deployability,Modul,ModuleNotFoundError,877,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:901,deployability,modul,module,901,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:146,interoperability,share,share,146,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:70,modifiability,pac,package,70,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:674,modifiability,modul,module,674,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:843,modifiability,modul,module,843,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:877,modifiability,Modul,ModuleNotFoundError,877,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:901,modifiability,modul,module,901,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:40,performance,error,error,40,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:78,performance,error,errors,78,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:16,reliability,doe,does,16,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:40,safety,error,error,40,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:78,safety,error,errors,78,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:674,safety,modul,module,674,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:843,safety,modul,module,843,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:877,safety,Modul,ModuleNotFoundError,877,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:901,safety,modul,module,901,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:527,testability,Trace,Traceback,527,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:40,usability,error,error,40,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:78,usability,error,errors,78,"The bash script does bypass the python3 error, but then other missing package errors appear:. ```/opt/conda/envs/dv/bin/python /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_qb8gn44e/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:73,deployability,instal,install,73,Can you confirm that you had activated the Conda environment you used to install DeepVariant when you ran that command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:8,usability,confirm,confirm,8,Can you confirm that you had activated the Conda environment you used to install DeepVariant when you ran that command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:111,usability,command,command,111,Can you confirm that you had activated the Conda environment you used to install DeepVariant when you ran that command?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:691,deployability,modul,module,691,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:860,deployability,modul,module,860,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:894,deployability,Modul,ModuleNotFoundError,894,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:918,deployability,modul,module,918,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1059,deployability,Version,Version,1059,"on3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1067,deployability,Build,Build,1067,"pt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5118,deployability,modul,modules,5118,_h80387f5_0 conda-forge. libprotobuf 3.18.0 h780b84a_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.10.0 haa6b8db_3 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lz4-c 1.9.3 h9c3ff4c_1 conda-forge. markdown 3.4.3 pyhd8ed1ab_0 conda-forge. markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge. mock 5.0.2 pyhd8ed1ab_0 conda-forge. multidict 5.2.0 py36h8f6f2f9_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. numpy 1.16.6 py36h2aa4a07_0 conda-forge. oauth2client 4.1.3 py_0 conda-forge. oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge. openjdk 8.0.332 h166bdaf_0 conda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1263,energy efficiency,gpu,gpu,1263,"g --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2697,energy efficiency,cloud,cloud-sdk,2697,4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b_1114 conda-forge. htslib 1.17 h6bc39ce_1 bioconda . httplib2 0.22.0 pyhd8ed1ab_0 conda-forge. icu 68.2 h9c3ff4c_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. idna_ssl 1.1.0 pyhd8ed1ab_1002 conda-forge. importlib-metadata 4.8.1 py36h5fab9bb_0 conda-forge. intervaltree 3.1.0 pyhd8ed1ab_1 conda-forge. jinja2 3.0.3 pyhd8ed1ab_0 conda-forge. jsonschema 4.1.2 pyhd8ed1ab_0 conda-forge. keras-applications 1.0.8 py_1 conda-forge. keras-preprocessing 1.1.2 pyhd8ed1ab_0 conda-forge. keyutils 1.6.1 h166bdaf_0 conda-forge. krb5 1.20.1 hf9c8cef_0 conda-forge. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libcurl 7.87.0 h6312ad2_0 conda-forge. libdeflate 1.18 h0b41bf4_0 conda-forge. libedit 3.1.20191231 h,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6230,energy efficiency,estimat,estimator,6230,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6272,energy efficiency,gpu,gpu,6272,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1059,integrability,Version,Version,1059,"on3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6657,integrability,wrap,wrapt,6657,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:163,interoperability,share,share,163,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6095,interoperability,plug,plugin-wit,6095,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:691,modifiability,modul,module,691,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:860,modifiability,modul,module,860,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:894,modifiability,Modul,ModuleNotFoundError,894,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:918,modifiability,modul,module,918,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1007,modifiability,pac,packages,1007," ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1059,modifiability,Version,Version,1059,"on3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5118,modifiability,modul,modules,5118,_h80387f5_0 conda-forge. libprotobuf 3.18.0 h780b84a_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.10.0 haa6b8db_3 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lz4-c 1.9.3 h9c3ff4c_1 conda-forge. markdown 3.4.3 pyhd8ed1ab_0 conda-forge. markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge. mock 5.0.2 pyhd8ed1ab_0 conda-forge. multidict 5.2.0 py36h8f6f2f9_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. numpy 1.16.6 py36h2aa4a07_0 conda-forge. oauth2client 4.1.3 py_0 conda-forge. oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge. openjdk 8.0.332 h166bdaf_0 conda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6417,modifiability,extens,extensions,6417,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1263,performance,gpu,gpu,1263,"g --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1441,performance,time,timeout,1441,"ample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1804,performance,cach,cached-property,1804,"eepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e879",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1898,performance,cach,cachetools,1898,"FoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:4874,performance,parallel,parallel,4874,orge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.0 he5830b7_0 conda-forge. liblapack 3.9.0 17_linux64_openblas conda-forge. libnghttp2 1.51.0 hdcd2b5c_0 conda-forge. libnsl 2.0.0 h7f98852_0 conda-forge. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libprotobuf 3.18.0 h780b84a_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.10.0 haa6b8db_3 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lz4-c 1.9.3 h9c3ff4c_1 conda-forge. markdown 3.4.3 pyhd8ed1ab_0 conda-forge. markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge. mock 5.0.2 pyhd8ed1ab_0 conda-forge. multidict 5.2.0 py36h8f6f2f9_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. numpy 1.16.6 py36h2aa4a07_0 conda-forge. oauth2client 4.1.3 py_0 conda-forge. oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge. openjdk 8.0.332 h166bdaf_0 conda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6272,performance,gpu,gpu,6272,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:691,safety,modul,module,691,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:860,safety,modul,module,860,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:894,safety,Modul,ModuleNotFoundError,894,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:918,safety,modul,module,918,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1441,safety,timeout,timeout,1441,"ample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5118,safety,modul,modules,5118,_h80387f5_0 conda-forge. libprotobuf 3.18.0 h780b84a_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.10.0 haa6b8db_3 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lz4-c 1.9.3 h9c3ff4c_1 conda-forge. markdown 3.4.3 pyhd8ed1ab_0 conda-forge. markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge. mock 5.0.2 pyhd8ed1ab_0 conda-forge. multidict 5.2.0 py36h8f6f2f9_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. numpy 1.16.6 py36h2aa4a07_0 conda-forge. oauth2client 4.1.3 py_0 conda-forge. oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge. openjdk 8.0.332 h166bdaf_0 conda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1758,security,certif,certificates,1758,"files_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1941,security,certif,certifi,1941,"dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b_1114 conda-forge. htslib 1.17 h6bc39ce_1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2247,security,cryptograph,cryptography,2247,ect 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b_1114 conda-forge. htslib 1.17 h6bc39ce_1 bioconda . httplib2 0.22.0 pyhd8ed1ab_0 conda-forge. icu 68.2 h9c3ff4c_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. idna_ssl 1.1.0 pyhd8ed1ab_1002 conda-forge. importlib-metadata 4.8.1 py36h5fab9bb_0 conda-forge. intervaltree 3.1.0 pyhd8ed1ab_1 conda-forge. jinja2 3.0.3 pyhd8ed1ab_0 conda-forge. jsonsc,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2599,security,auth,auth,2599,pp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b_1114 conda-forge. htslib 1.17 h6bc39ce_1 bioconda . httplib2 0.22.0 pyhd8ed1ab_0 conda-forge. icu 68.2 h9c3ff4c_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. idna_ssl 1.1.0 pyhd8ed1ab_1002 conda-forge. importlib-metadata 4.8.1 py36h5fab9bb_0 conda-forge. intervaltree 3.1.0 pyhd8ed1ab_1 conda-forge. jinja2 3.0.3 pyhd8ed1ab_0 conda-forge. jsonschema 4.1.2 pyhd8ed1ab_0 conda-forge. keras-applications 1.0.8 py_1 conda-forge. keras-preprocessing 1.1.2 pyhd8ed1ab_0 conda-forge. keyutils 1.6.1 h166bdaf_0 conda-forge. krb5 1.20.1 hf9c8cef_0 conda-forge. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2644,security,auth,auth-oauthlib,2644,y36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 py36hd8eec40_1 conda-forge. chardet 4.0.0 py36h5fab9bb_1 conda-forge. charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge. click 8.0.1 py36h5fab9bb_0 conda-forge. contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge. crcmod 1.7 py36h8f6f2f9_1006 conda-forge. cryptography 35.0.0 py36hb60f036_0 conda-forge. cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge. cudnn 7.6.5.32 ha8d7eb6_1 conda-forge. cupti 10.0.130 0 . curl 7.87.0 h6312ad2_0 conda-forge. deepvariant 1.5.0 py36hf3e76ba_0 bioconda . entrypoints 0.4 pyhd8ed1ab_0 conda-forge. enum34 1.1.10 py36h9f0ad1d_2 conda-forge. gast 0.2.2 py_0 conda-forge. google-auth 2.20.0 pyh1a96a4e_0 conda-forge. google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge. google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge. google-pasta 0.2.0 pyh8c360ce_0 conda-forge. grpcio 1.38.1 py36h8e87921_0 conda-forge. h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge. hdf5 1.10.6 nompi_h6a2412b_1114 conda-forge. htslib 1.17 h6bc39ce_1 bioconda . httplib2 0.22.0 pyhd8ed1ab_0 conda-forge. icu 68.2 h9c3ff4c_0 conda-forge. idna 3.4 pyhd8ed1ab_0 conda-forge. idna_ssl 1.1.0 pyhd8ed1ab_1002 conda-forge. importlib-metadata 4.8.1 py36h5fab9bb_0 conda-forge. intervaltree 3.1.0 pyhd8ed1ab_1 conda-forge. jinja2 3.0.3 pyhd8ed1ab_0 conda-forge. jsonschema 4.1.2 pyhd8ed1ab_0 conda-forge. keras-applications 1.0.8 py_1 conda-forge. keras-preprocessing 1.1.2 pyhd8ed1ab_0 conda-forge. keyutils 1.6.1 h166bdaf_0 conda-forge. krb5 1.20.1 hf9c8cef_0 conda-forge. ld_impl_linux-64 2.40 h41732ed_0 conda-forge. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libcurl 7.87.0 h6312ad2_0 conda-forge. libdeflate ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5737,security,rsa,rsa,5737,onda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:544,testability,Trace,Traceback,544,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:4475,testability,mock,mock,4475,2.40 h41732ed_0 conda-forge. libblas 3.9.0 17_linux64_openblas conda-forge. libcblas 3.9.0 17_linux64_openblas conda-forge. libcurl 7.87.0 h6312ad2_0 conda-forge. libdeflate 1.18 h0b41bf4_0 conda-forge. libedit 3.1.20191231 he28a2e2_2 conda-forge. libev 4.33 h516909a_1 conda-forge. libffi 3.4.2 h7f98852_5 conda-forge. libgcc-ng 13.1.0 he5830b7_0 conda-forge. libgfortran-ng 13.1.0 h69a702a_0 conda-forge. libgfortran5 13.1.0 h15d22d2_0 conda-forge. libgomp 13.1.0 he5830b7_0 conda-forge. liblapack 3.9.0 17_linux64_openblas conda-forge. libnghttp2 1.51.0 hdcd2b5c_0 conda-forge. libnsl 2.0.0 h7f98852_0 conda-forge. libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge. libprotobuf 3.18.0 h780b84a_1 conda-forge. libsqlite 3.42.0 h2797004_0 conda-forge. libssh2 1.10.0 haa6b8db_3 conda-forge. libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge. libzlib 1.2.13 hd590300_5 conda-forge. lz4-c 1.9.3 h9c3ff4c_1 conda-forge. markdown 3.4.3 pyhd8ed1ab_0 conda-forge. markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge. mock 5.0.2 pyhd8ed1ab_0 conda-forge. multidict 5.2.0 py36h8f6f2f9_0 conda-forge. ncurses 6.4 hcb278e6_0 conda-forge. numpy 1.16.6 py36h2aa4a07_0 conda-forge. oauth2client 4.1.3 py_0 conda-forge. oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge. openjdk 8.0.332 h166bdaf_0 conda-forge. openssl 1.1.1u hd590300_0 conda-forge. opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge. pandas 1.1.5 py36h284efc9_0 conda-forge. parallel 20230522 ha770c72_0 conda-forge. perl 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:0,usability,Confirm,Confirmed,0,"Confirmed:. ```. (dv) dpipe@4de3e1b4384c:/app/dpipe$ which python3. /opt/conda/envs/dv/bin/python3. (dv) dpipe@4de3e1b4384c:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>. from etils import epath. ModuleNotFoundError: No module named 'etils'. (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv. List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel . . _libgcc_mutex 0.1 conda_forge conda-forge. _openmp_mutex 4.5 2_gnu conda-forge. _tflow_select 2.1.0 gpu . absl-py 0.15.0 pyhd8ed1ab_0 conda-forge. aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge. altair 4.2.0 pyhd8ed1ab_0 conda-forge. astor 0.8.1 pyh9f0ad1d_0 conda-forge. async-timeout 3.0.1 py_1000 conda-forge. attrs 22.2.0 pyh71513ae_0 conda-forge. blinker 1.5 pyhd8ed1ab_0 conda-forge. boost 1.75.0 py36h355b2fd_0 conda-forge. boost-cpp 1.75.0 hc6e9bd1_0 conda-forge. brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge. bzip2 1.0.8 h7f98852_4 conda-forge. c-ares 1.19.1 hd590300_0 conda-forge. ca-certificates 2023.5.7 hbcca054_0 conda-forge. cached-property 1.5.2 hd8ed1ab_1 conda-forge. cached_property 1.5.2 pyha770c72_1 conda-forge. cachetools 5.0.0 pyhd8ed1ab_0 conda-forge. certifi 2021.5.30 py36h5fab9bb_0 conda-forge. cffi 1.14.6 p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6371,usability,tool,toolz,6371,l 5.32.1 2_h7f98852_perl5 conda-forge. pip 21.3.1 pyhd8ed1ab_0 conda-forge. protobuf 3.18.0 py36hc4f0c31_0 conda-forge. psutil 5.8.0 py36h8f6f2f9_1 conda-forge. pyasn1 0.4.8 py_0 conda-forge. pyasn1-modules 0.2.7 py_0 conda-forge. pycparser 2.21 pyhd8ed1ab_0 conda-forge. pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge. pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge. pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge. pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge. pysocks 1.7.1 py36h5fab9bb_3 conda-forge. python 3.6.15 hb7a2778_0_cpython conda-forge. python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge. python_abi 3.6 2_cp36m conda-forge. pytz 2023.3 pyhd8ed1ab_0 conda-forge. pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge. readline 8.2 h8228510_1 conda-forge. requests 2.28.1 pyhd8ed1ab_0 conda-forge. requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge. rsa 4.9 pyhd8ed1ab_0 conda-forge. scipy 1.5.3 py36h9e8f40b_0 conda-forge. setuptools 58.0.4 py36h5fab9bb_2 conda-forge. six 1.16.0 pyh6c4a22f_0 conda-forge. sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge. sqlite 3.42.0 h2c6b66d_0 conda-forge. tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge. tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge. tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge. tensorflow 2.0.0 gpu_py36h6b29c10_0 . tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 . tensorflow-estimator 2.0.0 pyh2649769_0 . tensorflow-gpu 2.0.0 h0d30ee6_0 . termcolor 1.1.0 pyhd8ed1ab_3 conda-forge. tk 8.6.12 h27826a3_0 conda-forge. toolz 0.12.0 pyhd8ed1ab_0 conda-forge. typing-extensions 4.1.1 hd8ed1ab_0 conda-forge. typing_extensions 4.1.1 pyha770c72_0 conda-forge. unzip 6.0 h7f98852_3 conda-forge. urllib3 1.26.15 pyhd8ed1ab_0 conda-forge. werkzeug 0.16.1 py_0 conda-forge. wheel 0.37.1 pyhd8ed1ab_0 conda-forge. wrapt 1.13.1 py36h8f6f2f9_0 conda-forge. xz 5.2.6 h166bdaf_0 conda-forge. yarl 1.6.3 py36h8f6f2f9_2 conda-forge. zipp 3.6.0 pyhd8ed1ab_0 conda-forge. zlib 1.2.13 hd590300_5 conda-forge. zstd 1.4.9 ha95c52a_0 conda-forge. (dv) dpipe@4de3e1b4384c:/app/dpipe$ . ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7221,availability,sli,slim,7221,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7270,availability,sli,slim,7270,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:700,deployability,modul,module,700,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:869,deployability,modul,module,869,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:898,deployability,Modul,ModuleNotFoundError,898,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:922,deployability,modul,module,922,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:978,deployability,modul,modules,978,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1039,deployability,depend,dependencies,1039,"proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1052,deployability,instal,installed,1052,"onment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_159361046425",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1169,deployability,version,version,1169,"s/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. bl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1575,deployability,modul,module,1575,"ll last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1584,deployability,Modul,ModuleNotFoundError,1584,"""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1608,deployability,modul,module,1608,"les_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5223,deployability,modul,modules,5223,/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work. Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work. MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work. mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work. multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work. numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work. oauth2client==4.1.3. oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work. opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work. pandas==1.1.5. protobuf==3.18.0. psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work. pyasn1==0.4.8. pyasn1-modules==0.2.7. pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work. PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work. pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work. pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work. pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1610146795286/work. PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1610291458349/work. python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthli,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7108,energy efficiency,estimat,estimator,7108,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1039,integrability,depend,dependencies,1039,"proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1169,integrability,version,version,1169,"s/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. bl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7619,integrability,wrap,wrapt,7619,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:140,interoperability,share,share,140,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6926,interoperability,plug,plugin-wit,6926,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7001,interoperability,plug,plugin-,7001,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:700,modifiability,modul,module,700,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:869,modifiability,modul,module,869,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:898,modifiability,Modul,ModuleNotFoundError,898,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:922,modifiability,modul,module,922,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:978,modifiability,modul,modules,978,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1031,modifiability,pac,package,1031,"ng to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1039,modifiability,depend,dependencies,1039,"proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1169,modifiability,version,version,1169,"s/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. bl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1334,modifiability,pac,packaged,1334,"un1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_arti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1575,modifiability,modul,module,1575,"ll last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1584,modifiability,Modul,ModuleNotFoundError,1584,"""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1608,modifiability,modul,module,1608,"les_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5223,modifiability,modul,modules,5223,/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work. Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work. MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work. mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work. multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work. numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work. oauth2client==4.1.3. oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work. opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work. pandas==1.1.5. protobuf==3.18.0. psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work. pyasn1==0.4.8. pyasn1-modules==0.2.7. pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work. PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work. pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work. pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work. pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1610146795286/work. PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1610291458349/work. python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthli,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2071,performance,time,timeout,2071,"nment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2276,performance,cach,cached-property,2276,"@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2380,performance,cach,cachetools,2380,"49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7221,reliability,sli,slim,7221,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7270,reliability,sli,slim,7270,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:700,safety,modul,module,700,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:869,safety,modul,module,869,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:898,safety,Modul,ModuleNotFoundError,898,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:922,safety,modul,module,922,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:978,safety,modul,modules,978,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1039,safety,depend,dependencies,1039,"proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1575,safety,modul,module,1575,"ll last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1584,safety,Modul,ModuleNotFoundError,1584,"""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1608,safety,modul,module,1608,"les_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2071,safety,timeout,timeout,2071,"nment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:5223,safety,modul,modules,5223,/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work. Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work. MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work. mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work. multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work. numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work. oauth2client==4.1.3. oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work. opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work. pandas==1.1.5. protobuf==3.18.0. psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work. pyasn1==0.4.8. pyasn1-modules==0.2.7. pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work. PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work. pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work. pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work. pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1610146795286/work. PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1610291458349/work. python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthli,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:2474,security,certif,certifi,2474,"rmation. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:3040,security,cryptograph,cryptography,3040,593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1624380494797/work. h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:3254,security,auth,auth,3254,k. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1624380494797/work. h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work. Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work. jsonschema @ file:///home/conda/feedstock_root/build_a,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:3350,security,auth,auth-oauthlib,3350,ty_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1624380494797/work. h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work. Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work. jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:3423,security,auth,auth-,3423,/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work. chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work. charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work. click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1624380494797/work. h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work. Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work. jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_16107135598,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:3823,security,ssl,ssl,3823,ifacts/click_1621503698523/work. contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work. crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work. cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work. entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work. gast==0.2.2. google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work. google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work. google-pasta==0.2.0. grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1624380494797/work. h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work. Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work. jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work. Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work. MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work. mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work. multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work. numpy @ file:///home/conda/feeds,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:6249,security,rsa,rsa,6249,file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work. PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work. pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work. pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work. pyrsistent @ file:///home/conda/feedstock_root/build_artifacts/pyrsistent_1610146795286/work. PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1610291458349/work. python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:553,testability,Trace,Traceback,553,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1039,testability,depend,dependencies,1039,"proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1511,testability,Trace,Traceback,1511,"k 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work. certifi==2021.5.30. cffi @ file:///home/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:4619,testability,mock,mock,4619,h5py_1604753633596/work. httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work. idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work. idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work. importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work. intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work. Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work. jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work. Keras-Applications==1.0.8. Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work. Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work. MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work. mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work. multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work. numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work. oauth2client==4.1.3. oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work. opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work. pandas==1.1.5. protobuf==3.18.0. psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work. pyasn1==0.4.8. pyasn1-modules==0.2.7. pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work. PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work. pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work. pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work. pyrsistent @ file:///h,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:15,usability,indicat,indicates,15,"`which python` indicates its pointing to the proper environment python. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///hom",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:1422,usability,help,help,1422,"ple_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0. /opt/conda/envs/dv/bin/python3. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>. from deepvariant import make_examples_core. File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>. import dataclasses. ModuleNotFoundError: No module named 'dataclasses'. ```. Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc. ```. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version. Python 3.6.15. (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python. /opt/conda/envs/dv/bin/python. (dv) dpipe@3a2ea3796f25:/app/dpipe$ python. Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) . [GCC 9.4.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import dataclasses. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ModuleNotFoundError: No module named 'dataclasses'. >>> import tensorflow. >>> quit(). (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze. absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work. aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work. altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work. astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work. async-timeout==3.0.1. attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work. blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work. brotlipy==0.7.0. cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work. cachetools @ file:///home/conda/feedstock_ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:7321,usability,tool,toolz,7321,6286081/work. pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work. pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work. requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work. requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work. rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work. scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work. six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work. sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work. tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl. tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl. tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl. tensorflow==2.0.0. tensorflow-estimator==2.0.0. termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work. tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d. toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work. typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work. urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work. Werkzeug==0.16.1. wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work. yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work. zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work. ```.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:35,deployability,upgrad,upgrade,35,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:47,deployability,version,version,47,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:195,deployability,instal,install,195,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:229,deployability,upgrad,upgrading,229,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:47,integrability,version,version,47,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:35,modifiability,upgrad,upgrade,35,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:47,modifiability,version,version,47,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:209,modifiability,pac,packages,209,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:229,modifiability,upgrad,upgrading,229,"One last thing you could try is to upgrade the version of python you are using. While dataclasses were backported to 3.6, they were formally introduced in python 3.7. . You could try to manually install those packages and/or try upgrading python? I'm not sure what to try next, but I can ask around.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:170,availability,state,state,170,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:214,availability,operat,operating,214,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:432,availability,operat,operating,432,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:590,availability,cluster,cluster,590,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:68,deployability,continu,continue,68,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:260,deployability,version,version,260,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:352,deployability,releas,release,352,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:377,deployability,releas,release,377,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:398,deployability,releas,release,398,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:590,deployability,cluster,cluster,590,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:833,deployability,fail,fail,833,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:420,energy efficiency,CPU,CPU,420,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:511,energy efficiency,cpu,cpuinfo,511,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:170,integrability,state,state,170,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:260,integrability,version,version,260,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:562,interoperability,share,shared,562,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:260,modifiability,version,version,260,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:420,performance,CPU,CPU,420,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:511,performance,cpu,cpuinfo,511,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:833,reliability,fail,fail,833,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:319,security,command-lin,command-line,319,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:541,security,access,access,541,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:105,testability,understand,understand,105,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:319,usability,command,command-line,319,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:624,usability,user,user-space,624,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:657,usability,prefer,preferred,657,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case? 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:75,deployability,version,version,75,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:169,deployability,continu,continue,169,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:75,integrability,version,version,75,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:75,modifiability,version,version,75,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:101,modifiability,maintain,maintained,101,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:101,safety,maintain,maintained,101,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:119,security,team,team,119,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:153,testability,context,context,153,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/664:202,usability,help,help,202,"H @mwhitesi ,. thanks for the question! One thing I'll mention is that the version on bioconda isn't maintained by our team. But if you can provide more context, we can continue to try to see if we can help here. Let us know if you want to follow up in this thread. I'll keep this open for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/664
https://github.com/google/deepvariant/issues/666:146,modifiability,Pac,PacBio,146,@Npaffen Could you try generating `.bai` index files for the bam files with `samtools index`. I'm not sure `htslib` in Nucleus is recognizing the PacBio `.pbi` index files.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:52,modifiability,Pac,Pacbio,52,I will do that. So there is no difference between a Pacbio `.pbi` index file and a `samtools index` output `.bai` file?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:612,availability,error,error,612,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:351,deployability,Fail,Failed,351,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2053,deployability,modul,module,2053,"nt/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2862,deployability,modul,module,2862,"/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3408,deployability,modul,module,3408,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3436,deployability,fail,failed,3436,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:124,energy efficiency,Current,Current,124,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:633,energy efficiency,Current,Current,633,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2053,modifiability,modul,module,2053,"nt/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2862,modifiability,modul,module,2862,"/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3408,modifiability,modul,module,3408,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:612,performance,error,error,612,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3417,performance,parallel,parallel,3417,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:108,reliability,doe,does,108,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:351,reliability,Fail,Failed,351,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3436,reliability,fail,failed,3436,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:612,safety,error,error,612,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2053,safety,modul,module,2053,"nt/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2862,safety,modul,module,2862,"/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3408,safety,modul,module,3408,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:460,usability,statu,statusor,460,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:483,usability,statu,status,483,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:499,usability,statu,status,499,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:612,usability,error,error,612,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. [E::fai_retrieve] Failed to retrieve block: unexpected end of file. 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000. Fatal Python error: Aborted. ```. Current thread 0x00007fe7f6689740 (most recent call first):. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4011,usability,user,user,4011,"196 in <module>. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 258 in _run_main. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/absl_py/absl/app.py"", line 312 in run. File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpcawuh1vp/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 0. real	0m19,089s. user	0m15,039s. sys	0m1,066s. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:175,availability,error,error,175,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:377,availability,error,error,377,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:152,interoperability,format,formats,152,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:175,performance,error,error,175,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:377,performance,error,error,377,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:175,safety,error,error,175,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:377,safety,error,error,377,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:175,usability,error,error,175,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:377,usability,error,error,377,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:450,usability,command,command,450,"@Npaffen BAI and PBI have similar concepts, but different approaches. BAI is an R-tree approach, while PBI seems to be HDF5-like -- basically different formats. Regarding the error you see is because your BAM and reference don't match in their sequence contig names. For instance, the reference sequence has the chromosome naming convention of `chr1`, `chr2`, etc., but in the error I see `1` as a name to match against. So if you type the following command for you BAM file:. ```. samtools view -H GFX.bam | grep @SQ. ```. You should see something like this for the same naming convention for the `SN` field:. ```. @SQ SN:chr1 LN:249250621. @SQ SN:chr2 LN:243199373. @SQ SN:chr3 LN:198022430. @SQ SN:chr4 LN:191154276. @SQ SN:chr5 LN:180915260. @SQ SN:chr6 LN:171115067. @SQ SN:chr7 LN:159138663. @SQ SN:chr8 LN:146364022. ```. Also I'm assuming you used the same reference you aligned your reads against when generating your BAM files, as the reference used for DeepVariant. . Let me know if there is something I should expand on. Thanks,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:661,availability,state,stated,661,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:168,deployability,contain,contain,168,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:286,deployability,pipelin,pipeline,286,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:434,deployability,pipelin,pipeline,434,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:650,deployability,pipelin,pipeline,650,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:909,deployability,observ,observe,909,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:366,energy efficiency,model,model,366,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:286,integrability,pipelin,pipeline,286,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:434,integrability,pipelin,pipeline,434,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:650,integrability,pipelin,pipeline,650,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:661,integrability,state,stated,661,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:804,integrability,FILTER,FILTER,804,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1038,integrability,FILTER,FILTER,1038,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:539,interoperability,specif,specific,539,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1050,interoperability,FORMAT,FORMAT,1050,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:359,modifiability,PAC,PACBIO,359,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:492,modifiability,PAC,PACBIO,492,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:548,modifiability,paramet,parameter,548,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:849,modifiability,PAC,PACBIO,849,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:366,security,model,model,366,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:909,testability,observ,observe,909,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:836,usability,behavi,behavior,836,"Hey @pgrosu thanks for your quick reply. Looks like I used the wrong reference file for the original bam file indeed. Sadly the header of the original bam file did not contain any information which reference file they aligned the reads to. So I found the correct reference file and the pipeline finished successfully. Two uncertainties are left regarding the PACBIO model :. 1) The called variants are not phased. If I understood the pipeline correctly phasing should be part of deepvariants PACBIO mode in since v.1.1 right? Is there any specific parameter one need to supply to activate this? Is it good enough to just phase the vcf output from my pipeline I stated in the thread opener with WhatsHap? 2) Some of the calls seem to be HomREF and some of them are missing despite marked as HomREF in the FILTER column. Is this a normal behavior for PACBIO data? From my expertise I would have expected to not observe HomREF or missings in a VCF but maybe in a g.vcf. Am I missing something important here? ```. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT GFX. 1: 1 10031 . T C 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:29:7:4,3:0.428571:0,30,36. 2: 1 10037 . T C 0.8 RefCall . GT:GQ:DP:AD:VAF:PL ./.:8:7:3,4:0.571429:0,7,18. 3: 1 10043 . T C 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:17:7:5,2:0.285714:0,16,30. 4: 1 10055 . T C 2.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:4:7:3,4:0.571429:0,2,16. 5: 1 10067 . TA T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:20:7:3,2:0.285714:0,19,32. --- . 3912752: MT 7331 . C A 68.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:43:0,43:1:68,67,0. 3912753: MT 8843 . T C 68.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:68,68,0. 3912754: MT 8860 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:63:40:0,40:1:67,64,0. 3912755: MT 12005 . TG T 0.0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:65:43:37,6:0.139535:0,65,74. 3912756: MT 15326 . A G 67.9 PASS . GT:GQ:DP:AD:VAF:PL 1/1:65:37:0,37:1:67,68,0. ```. Best. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:957,availability,operat,operates,957,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:631,deployability,updat,update,631,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:125,energy efficiency,model,model,125,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:853,energy efficiency,model,model,853,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1149,energy efficiency,predict,predicted,1149," as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1317,energy efficiency,predict,predictions,1317,"5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/google/nucl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:700,integrability,filter,filter,700,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1681,integrability,filter,filter,1681,"t show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/variants.proto#L110-L119) ProtoBuf definition in Nucleus. Let me know if this helps clarify things. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1796,integrability,Filter,Filter,1796,"t show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/variants.proto#L110-L119) ProtoBuf definition in Nucleus. Let me know if this helps clarify things. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:118,modifiability,PAC,PACBIO,118,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:846,modifiability,PAC,PACBIO,846,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:631,safety,updat,update,631,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1149,safety,predict,predicted,1149," as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1317,safety,predict,predictions,1317,"5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/google/nucl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:125,security,model,model,125,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:631,security,updat,update,631,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:853,security,model,model,853,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:551,usability,tool,tool,551,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:616,usability,guid,guide,616,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2432,usability,help,helps,2432,"t show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/variants.proto#L110-L119) ProtoBuf definition in Nucleus. Let me know if this helps clarify things. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:588,deployability,contain,containing,588,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:671,deployability,contain,contains,671,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:272,integrability,topic,topic,272,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:334,modifiability,Pac,PacBio,334,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:570,modifiability,interm,intermediate,570,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:88,reliability,doe,does,88,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:341,testability,context,context,341,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:358,testability,understand,understand,358,"Hi @pgrosu;. thanks for the clarification. So when you deepvariant uses `--phase-reads` does this mean that during processing of the (long) reads those get phased instead of variants to improve the calling? If so could you go a little bit more into details regarding this topic? How do you guys at deepvariant define a _variant_ in a PacBio context? From my understand HomRef _variants_ do nat actually vary compared to the reference panel and if one calls for example a WGS file generated with Ilumina so that one ends up with short reads, I always obtain a gvcf as an intermediate file containing haplotype blocks of (non-informative) HomRef calls while the actual vcf contains only informative calls so non-HomRef variants. . Best,. Nils",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:237,availability,cluster,cluster,237,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1183,availability,reliab,reliable,1183,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:237,deployability,cluster,cluster,237,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1651,deployability,log,logic,1651,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1594,energy efficiency,current,currently,1594,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:171,performance,network,network,171,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1089,performance,network,network,1089,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1122,performance,network,network,1122,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1238,performance,network,network,1238,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1452,performance,network,network,1452,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1183,reliability,reliab,reliable,1183,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:837,safety,input,inputs,837,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1343,safety,input,input,1343,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1651,safety,log,logic,1651,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:171,security,network,network,171,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:373,security,ident,identify,373,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1089,security,network,network,1089,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1122,security,network,network,1122,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1238,security,network,network,1238,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1452,security,network,network,1452,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1651,security,log,logic,1651,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1208,testability,context,context,1208,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1217,testability,coverag,coverage,1217,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1651,testability,log,logic,1651,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1693,testability,understand,understand,1693,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:837,usability,input,inputs,837,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1146,usability,learn,learn,1146,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1343,usability,input,input,1343,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1688,usability,help,help,1688,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:317,deployability,observ,observed,317,"Thanks for the explanation @AndrewCarroll. Gave me a good insight about the applied phasing menchanic! Still I have one point I don't fully get from your answer:. > position with an ALT allele that receives a non-reference (0/0 or ./.) call. I'm not sure if I understand this part correctly. From my understanding an observed ALT allele that recieves a non-reference call is heterozygous or homozygous ALT so either (0/1,1/0 or 1/1). What do you mean by _non-reference (0/0 or ./.)_ ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:260,testability,understand,understand,260,"Thanks for the explanation @AndrewCarroll. Gave me a good insight about the applied phasing menchanic! Still I have one point I don't fully get from your answer:. > position with an ALT allele that receives a non-reference (0/0 or ./.) call. I'm not sure if I understand this part correctly. From my understanding an observed ALT allele that recieves a non-reference call is heterozygous or homozygous ALT so either (0/1,1/0 or 1/1). What do you mean by _non-reference (0/0 or ./.)_ ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:300,testability,understand,understanding,300,"Thanks for the explanation @AndrewCarroll. Gave me a good insight about the applied phasing menchanic! Still I have one point I don't fully get from your answer:. > position with an ALT allele that receives a non-reference (0/0 or ./.) call. I'm not sure if I understand this part correctly. From my understanding an observed ALT allele that recieves a non-reference call is heterozygous or homozygous ALT so either (0/1,1/0 or 1/1). What do you mean by _non-reference (0/0 or ./.)_ ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:317,testability,observ,observed,317,"Thanks for the explanation @AndrewCarroll. Gave me a good insight about the applied phasing menchanic! Still I have one point I don't fully get from your answer:. > position with an ALT allele that receives a non-reference (0/0 or ./.) call. I'm not sure if I understand this part correctly. From my understanding an observed ALT allele that recieves a non-reference call is heterozygous or homozygous ALT so either (0/1,1/0 or 1/1). What do you mean by _non-reference (0/0 or ./.)_ ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:10,deployability,observ,observes,10,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:63,deployability,observ,observed,63,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:220,deployability,observ,observed,220,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:10,testability,observ,observes,10,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:63,testability,observ,observed,63,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:220,testability,observ,observed,220,So if one observes a reference call at some loci the sequencer observed an actual ALT allele in some of the reads but the majority of the reads led to some higer GP for a reference call instead of a variant call? If one observed a missing the same reference call as described before had to be made with too much uncertainty?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2275,availability,sli,slices,2275," number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but tha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2413,availability,sli,slicing,2413," each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3126,availability,sli,slices,3126,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:482,deployability,stack,stacked,482,"Hi @Npaffen,. Let me give a little background first before answering each question, and some of this you can find in the following two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:595,deployability,contain,contain,595,"Hi @Npaffen,. Let me give a little background first before answering each question, and some of this you can find in the following two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1441,deployability,contain,contains,1441," ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2746,deployability,contain,contains,2746,"variant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3068,deployability,scale,scale,3068,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1928,energy efficiency,model,model,1928,". A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2018,energy efficiency,model,model,2018,"mber means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of geno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2143,energy efficiency,model,model-architecture,2143,"mber means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2740,energy efficiency,model,model,2740,"le/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2832,energy efficiency,model,model,2832,"alculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3068,energy efficiency,scale,scale,3068,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3422,energy efficiency,model,model,3422,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:643,integrability,transform,transformed,643,"Hi @Npaffen,. Let me give a little background first before answering each question, and some of this you can find in the following two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1910,integrability,topic,topics,1910,"sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2297,integrability,sub,sub-matrices,2297," different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2454,integrability,event,eventual,2454," first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign proba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2613,integrability,sub,sub-matrices,2613,"tiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2941,integrability,transform,transformation,2941,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3122,integrability,sub,sub-slices,3122,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:643,interoperability,transform,transformed,643,"Hi @Npaffen,. Let me give a little background first before answering each question, and some of this you can find in the following two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2091,interoperability,architectur,architecture,2091,"for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2149,interoperability,architectur,architecture,2149,"r means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2503,interoperability,specif,specific,2503,"essary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2941,interoperability,transform,transformation,2941,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2375,modifiability,layer,layers,2375,"the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2733,modifiability,Pac,PacBio,2733,"om/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2825,modifiability,Pac,PacBio,2825,"pe is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` corre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3068,modifiability,scal,scale,3068,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2050,performance,network,network,2050,"nd of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2592,performance,network,networks,2592,"tation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3068,performance,scale,scale,3068,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1235,reliability,doe,does,1235,"s/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2275,reliability,sli,slices,2275," number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but tha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2413,reliability,sli,slicing,2413," each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3126,reliability,sli,slices,3126,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3578,reliability,doe,does,3578,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2675,safety,valid,validated,2675,"following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3107,safety,compl,complex,3107,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1928,security,model,model,1928,". A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2018,security,model,model,2018,"mber means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of geno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2050,security,network,network,2050,"nd of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2143,security,model,model-architecture,2143,"mber means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2592,security,network,networks,2592,"tation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2675,security,validat,validated,2675,"following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2740,security,model,model,2740,"le/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2832,security,model,model,2832,"alculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3107,security,compl,complex,3107,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3422,security,model,model,3422,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:315,usability,custom,custom-channels,315,"Hi @Npaffen,. Let me give a little background first before answering each question, and some of this you can find in the following two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1127,usability,support,supports,1127,"two blogs: [Blog 1](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1172,usability,support,supports,1172,"deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) and [Blog 2](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). For each analysis, `make_examples` generates 6 matrices (usually with a height: 100 and width: 199) with values ranging from 0-255, piling them up (stacked) to form an (image) tensor of (100 x 199 x 6). DeepVariant refers to these as channels. These 6 channels contain the following representation (which get transformed to those value ranges):. - Read base: different intensities represent A, C, G, and T, which is detailed in the [`BaseColor()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image_native.cc#L171-L188). - Base quality: set by the sequencing machine. A higher number means higher quality. - Mapping quality: set by the aligner. A higher number means higher quality. - Strand of alignment: For forward it is 70, and for reverse it is 240. - Read supports variant: High number means the read supports the given alternate allele, and lower number means it does not. - Base differs from ref: A higher number means the base is different from the reference, while a lower number means the base matches the reference. There is one more thing, each of these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3843,usability,help,helps,3843,"best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high. Which is why there is the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function to adjust for that afterwards, giving the `./.` correction. Hope it helps,. ~p.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:55,performance,time,time,55,I chewed on this the last days but I still have a hard time to understand why the homref variants and the missings are added to the vcf in the first place. Could you try to repeat this for me in case you already answered this but I was unable to emphasize this in your answer?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:63,testability,understand,understand,63,I chewed on this the last days but I still have a hard time to understand why the homref variants and the missings are added to the vcf in the first place. Could you try to repeat this for me in case you already answered this but I was unable to emphasize this in your answer?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1343,availability,down,downstream,1343,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1523,integrability,FILTER,FILTER,1523,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1532,integrability,filter,filter,1532,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1584,integrability,filter,filters,1584,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:990,interoperability,distribut,distribution,990,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1119,interoperability,distribut,distribution,1119,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1483,safety,compl,complies,1483,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1483,security,compl,complies,1483,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:59,testability,context,context,59,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:607,usability,support,supporting,607,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:647,usability,support,supporting,647,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:771,usability,learn,learn,771,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1354,usability,tool,tools,1354,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1419,usability,tool,tools,1419,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1539,usability,statu,status,1539,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1756,usability,help,helps,1756,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants. You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194. which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. . This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:418,availability,down,downstream-analysis,418,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:27,energy efficiency,model,model,27,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:27,security,model,model,27,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:10,usability,help,helps,10,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:556,usability,help,help,556,Yes. This helps and from a model perspective this makes sense to add every site to the VCF that is a potential candidate for a variant. The last question I might raise is: What is your intention to have those positions added to the VCF by default and not as a opt-in option. To me it sound rather counterintuitive because I would argue that the majority of use-cases do not need those positions and/or removed them in downstream-analysis. But ofc this is you decision and just my view on the overall concept and I might be wrong here! . Thanks for all you help and patience so far!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:933,integrability,FILTER,FILTER,933,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:260,modifiability,variab,variable,260,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:144,performance,network,network,144,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:374,performance,network,network,374,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:736,reliability,doe,does,736,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:91,safety,compl,complete,91,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:91,security,compl,complete,91,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:144,security,network,network,144,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:374,security,network,network,374,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:770,testability,simpl,simpler,770,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:227,usability,user,user-presented,227,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:532,usability,user,users,532,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:756,usability,user,users,756,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:770,usability,simpl,simpler,770,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:857,usability,user,users,857,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:949,usability,indicat,indicate,949,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:302,deployability,version,version,302,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:507,deployability,version,version,507,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1670,deployability,contain,contains,1670,"s of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3998,deployability,stage,stages,3998,"cific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2208,energy efficiency,heat,heatmap,2208,". ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3761,energy efficiency,power,power,3761,"dentify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5196,energy efficiency,power,power,5196," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5564,energy efficiency,optim,optimize,5564," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5718,energy efficiency,power,power,5718," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:302,integrability,version,version,302,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:507,integrability,version,version,507,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:796,integrability,transform,transformation,796,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:976,integrability,transform,transformation,976,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1077,integrability,transform,transformed,1077," view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1346,integrability,transform,transformations,1346," following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1493,integrability,transform,transformations,1493,"ified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to deno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3132,integrability,transform,transformations,3132,"feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3291,integrability,transform,transformations,3291,"middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3380,integrability,transform,transformation,3380,"db-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3449,integrability,transform,transformations,3449,"hing like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5259,integrability,transform,transformations,5259," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:796,interoperability,transform,transformation,796,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:976,interoperability,transform,transformation,976,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1077,interoperability,transform,transformed,1077," view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1346,interoperability,transform,transformations,1346," following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1390,interoperability,specif,specific,1390,"thub.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1493,interoperability,transform,transformations,1493,"ified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to deno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2998,interoperability,specif,specific,2998,"age](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3019,interoperability,specif,specific,3019,"om/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3132,interoperability,transform,transformations,3132,"feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3291,interoperability,transform,transformations,3291,"middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3380,interoperability,transform,transformation,3380,"db-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3449,interoperability,transform,transformations,3449,"hing like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3651,interoperability,specif,specific,3651,"the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4589,interoperability,specif,specific,4589,"3d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5259,interoperability,transform,transformations,5259," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:302,modifiability,version,version,302,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:507,modifiability,version,version,507,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3370,modifiability,layer,layers,3370,"8f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3781,modifiability,layer,layers,3781,"features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:120,performance,network,network,120,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3432,performance,network,network,3432," of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5231,performance,network,network,5231," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5564,performance,optimiz,optimize,5564," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:854,safety,detect,detect,854,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1112,safety,detect,detecting,1112,"al network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me unde",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3798,safety,input,input,3798,"ht expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4291,safety,test,testing,4291,"sformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4628,safety,test,test,4628,"of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4777,safety,test,test,4777,"r layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4917,safety,test,test,4917," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:120,security,network,network,120,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:719,security,ident,identifying,719,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:854,security,detect,detect,854,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1112,security,detect,detecting,1112,"al network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me unde",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2763,security,ident,identify,2763,"ifferences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2804,security,expos,expose,2804,"[t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3432,security,network,network,3432," of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5084,security,sign,significant,5084," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5231,security,network,network,5231," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5294,security,ident,identify,5294," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5405,security,ident,identification,5405," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:294,testability,simpl,simpler,294,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:496,testability,simpl,simplified,496,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2113,testability,understand,understand,2113,"ing unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the orig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4291,testability,test,testing,4291,"sformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4628,testability,test,test,4628,"of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4777,testability,test,test,4777,"r layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4917,testability,test,test,4917," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:294,usability,simpl,simpler,294,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:496,usability,simpl,simplified,496,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:706,usability,help,help,706,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1101,usability,help,helps,1101,"g from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1899,usability,visual,visualize,1899," be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2104,usability,help,helps,2104,"with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2941,usability,help,helps,2941,"I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3798,usability,input,input,3798,"ht expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:3809,usability,help,helping,3809,"e type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:4552,usability,indicat,indicate,4552,"5937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5555,usability,help,help,5555," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5676,usability,help,helps,5676," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:5774,usability,help,helps,5774," to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,. Paul.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:119,energy efficiency,model,model,119,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:119,security,model,model,119,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:730,security,team,team,730,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:771,testability,simpl,simply,771,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:771,usability,simpl,simply,771,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:805,usability,behavi,behavior,805,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model! From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:6,availability,down,downstream,6,"In my downstream analysis with WhatsHap today I found that the VCF contains only 3,889,968. The data comes from PacBio CCS 10x sequencing and I wonder if this can be the correct number of SNPs for this method. E.g. I analyzed several WGS 30x datasets from Nebula so far and spotted amounts of SNPs in the range of 4.6-4.8 million. Is there anything that I missed here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:67,deployability,contain,contains,67,"In my downstream analysis with WhatsHap today I found that the VCF contains only 3,889,968. The data comes from PacBio CCS 10x sequencing and I wonder if this can be the correct number of SNPs for this method. E.g. I analyzed several WGS 30x datasets from Nebula so far and spotted amounts of SNPs in the range of 4.6-4.8 million. Is there anything that I missed here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:112,modifiability,Pac,PacBio,112,"In my downstream analysis with WhatsHap today I found that the VCF contains only 3,889,968. The data comes from PacBio CCS 10x sequencing and I wonder if this can be the correct number of SNPs for this method. E.g. I analyzed several WGS 30x datasets from Nebula so far and spotted amounts of SNPs in the range of 4.6-4.8 million. Is there anything that I missed here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1251,availability,sli,slightly,1251,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1311,deployability,pipelin,pipeline,1311,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1311,integrability,pipelin,pipeline,1311,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1214,modifiability,Pac,PacBio,1214,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1251,reliability,sli,slightly,1251,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:248,safety,compl,complexity,248,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:248,security,compl,complexity,248,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:192,usability,help,help,192,"@Npaffen Thank you for the kind words. Maybe trying the following two approaches might show additional benefits:. 1) [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) could help rescue some of these SNPs that might reside in low-complexity regions like homopolymer, dimer and trimer repeat regions. You can read more details in [this post](https://github.com/kishwarshafin/pepper/blob/r0.8/docs/misc/pepper_methods.md) or in [the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8571015/pdf/nihms-1738709.pdf). . 2) Another novel approach could be with this [new paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9882142/pdf/nihpp-2023.01.12.523790v2.pdf) which incorporates [Flye](https://github.com/fenderglass/Flye)/[Shasta](https://github.com/paoloshasta/shasta)/[Hapdup](https://github.com/KolmogorovLab/hapdup) for de novo assembly, and [HapDiff](https://github.com/KolmogorovLab/hapdiff) to call structural variants with methylation tagging (via Guppy via [Remora](https://github.com/nanoporetech/remora)) into a combined variant call set in [Margin](https://github.com/UCSC-nanopore-cgl/margin) with also using the PEPPER-Margin-DeepVariant for variant calling. Since you are using PacBio this would need to be tweaked slightly. I just read that paper, so I have not tried their pipeline yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:207,modifiability,Pac,PacBio,207,"Hi @Npaffen . For the lower number of variants, I would suspect that the main driver would be coverage here. 10x is a relatively lower coverage. I will try to find some time to see if we have a 10x coverage PacBio run from a sample handy to let you know how many variants we typically see at that range. Another possibility is that samples do naturally differ in their number of variants, especially based on the ancestry of the individuals analyzed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:169,performance,time,time,169,"Hi @Npaffen . For the lower number of variants, I would suspect that the main driver would be coverage here. 10x is a relatively lower coverage. I will try to find some time to see if we have a 10x coverage PacBio run from a sample handy to let you know how many variants we typically see at that range. Another possibility is that samples do naturally differ in their number of variants, especially based on the ancestry of the individuals analyzed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:94,testability,coverag,coverage,94,"Hi @Npaffen . For the lower number of variants, I would suspect that the main driver would be coverage here. 10x is a relatively lower coverage. I will try to find some time to see if we have a 10x coverage PacBio run from a sample handy to let you know how many variants we typically see at that range. Another possibility is that samples do naturally differ in their number of variants, especially based on the ancestry of the individuals analyzed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:135,testability,coverag,coverage,135,"Hi @Npaffen . For the lower number of variants, I would suspect that the main driver would be coverage here. 10x is a relatively lower coverage. I will try to find some time to see if we have a 10x coverage PacBio run from a sample handy to let you know how many variants we typically see at that range. Another possibility is that samples do naturally differ in their number of variants, especially based on the ancestry of the individuals analyzed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:198,testability,coverag,coverage,198,"Hi @Npaffen . For the lower number of variants, I would suspect that the main driver would be coverage here. 10x is a relatively lower coverage. I will try to find some time to see if we have a 10x coverage PacBio run from a sample handy to let you know how many variants we typically see at that range. Another possibility is that samples do naturally differ in their number of variants, especially based on the ancestry of the individuals analyzed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:77,modifiability,Pac,PacBio,77,Thanks for all the input again! Is it possible to use DeepTrio with combined PacBio (parents) and low-coverage Illumina data right now?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:19,safety,input,input,19,Thanks for all the input again! Is it possible to use DeepTrio with combined PacBio (parents) and low-coverage Illumina data right now?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:102,testability,coverag,coverage,102,Thanks for all the input again! Is it possible to use DeepTrio with combined PacBio (parents) and low-coverage Illumina data right now?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:19,usability,input,input,19,Thanks for all the input again! Is it possible to use DeepTrio with combined PacBio (parents) and low-coverage Illumina data right now?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:915,deployability,stack,stacked,915,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:113,energy efficiency,model,model,113,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:459,energy efficiency,model,model,459,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1031,energy efficiency,model,model,1031,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:245,modifiability,scenario,scenarios,245,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:63,performance,content,content,63,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:896,safety,input,input,896,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1012,safety,detect,detection,1012,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:113,security,model,model,113,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:459,security,model,model,459,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1012,security,detect,detection,1012,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1031,security,model,model,1031,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:188,testability,coverag,coverage,188,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:236,testability,coverag,coverage,236,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:896,usability,input,input,896,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:959,usability,hint,hint,959,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:94,energy efficiency,model,model,94,I'm sorry I missed a crucial part in my question. What I meant is : Is it possible to use any model if the parents come from CCS PacBio and the child from Illumina WGS. Thanks anyway for your brief explanation of the overall model!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:225,energy efficiency,model,model,225,I'm sorry I missed a crucial part in my question. What I meant is : Is it possible to use any model if the parents come from CCS PacBio and the child from Illumina WGS. Thanks anyway for your brief explanation of the overall model!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:129,modifiability,Pac,PacBio,129,I'm sorry I missed a crucial part in my question. What I meant is : Is it possible to use any model if the parents come from CCS PacBio and the child from Illumina WGS. Thanks anyway for your brief explanation of the overall model!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:94,security,model,model,94,I'm sorry I missed a crucial part in my question. What I meant is : Is it possible to use any model if the parents come from CCS PacBio and the child from Illumina WGS. Thanks anyway for your brief explanation of the overall model!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:225,security,model,model,225,I'm sorry I missed a crucial part in my question. What I meant is : Is it possible to use any model if the parents come from CCS PacBio and the child from Illumina WGS. Thanks anyway for your brief explanation of the overall model!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:14,energy efficiency,Current,Currently,14,Hi @Npaffen . Currently we don't have DeepTrio models that are trained with the setting you described (parents come from CCS PacBio and the child from Illumina WGS).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:47,energy efficiency,model,models,47,Hi @Npaffen . Currently we don't have DeepTrio models that are trained with the setting you described (parents come from CCS PacBio and the child from Illumina WGS).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:125,modifiability,Pac,PacBio,125,Hi @Npaffen . Currently we don't have DeepTrio models that are trained with the setting you described (parents come from CCS PacBio and the child from Illumina WGS).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:47,security,model,models,47,Hi @Npaffen . Currently we don't have DeepTrio models that are trained with the setting you described (parents come from CCS PacBio and the child from Illumina WGS).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:527,availability,error,errors,527,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:89,energy efficiency,model,models,89,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:548,energy efficiency,model,models,548,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:138,modifiability,paramet,parameters,138,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:490,modifiability,paramet,parameters,490,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:527,performance,error,errors,527,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:527,safety,error,errors,527,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:89,security,model,models,89,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:548,security,model,models,548,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:518,usability,minim,minimize,518,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:527,usability,error,errors,527,"@Npaffen As @pichuan mentioned, I would be hesitant as well to combine them as these two models seem to have been designed with different parameters in mind [as shown here](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L243-L261) (especially the image width, haplotype sorting and alternate aligned pileups). You can get more details in the [following blog](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html) as to how some of these parameters are utilized and minimize errors for different models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:28,energy efficiency,model,model,28,"@Npaffen I noticed a hybrid model `HYBRID_PACBIO_ILLUMINA` where one can combine PacBio with Illumina reads. You can read more about it in the [following case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-hybrid-case-study.md). This is new to me as well, so I need to read a bit more about it first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:81,modifiability,Pac,PacBio,81,"@Npaffen I noticed a hybrid model `HYBRID_PACBIO_ILLUMINA` where one can combine PacBio with Illumina reads. You can read more about it in the [following case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-hybrid-case-study.md). This is new to me as well, so I need to read a bit more about it first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:28,security,model,model,28,"@Npaffen I noticed a hybrid model `HYBRID_PACBIO_ILLUMINA` where one can combine PacBio with Illumina reads. You can read more about it in the [following case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-hybrid-case-study.md). This is new to me as well, so I need to read a bit more about it first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:25,energy efficiency,model,model,25,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:128,energy efficiency,model,model,128,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,energy efficiency,model,model,150,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:173,energy efficiency,model,model,173,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:218,energy efficiency,model,model,218,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:354,energy efficiency,model,model,354,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:259,integrability,sub,submission,259,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:58,modifiability,Pac,PacBio,58,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:364,reliability,doe,does,364,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:25,security,model,model,25,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:128,security,model,model,128,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,security,model,model,150,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:173,security,model,model,173,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:218,security,model,model,218,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:354,security,model,model,354,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:25,energy efficiency,model,model,25,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:128,energy efficiency,model,model,128,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,energy efficiency,model,model,150,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:173,energy efficiency,model,model,173,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:218,energy efficiency,model,model,218,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:354,energy efficiency,model,model,354,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:259,integrability,sub,submission,259,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:58,modifiability,Pac,PacBio,58,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:364,reliability,doe,does,364,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:25,security,model,model,25,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:128,security,model,model,128,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,security,model,model,150,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:173,security,model,model,173,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:218,security,model,model,218,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:354,security,model,model,354,"Hi @pgrosu ,. The hybrid model you mentioned is combining PacBio and Illumina reads of the same individual. It is a DeepVariant model, not a DeepTrio model. With the hybrid model, you can achieve better accuracy. This model was first developed as part of our submission to https://precision.fda.gov/challenges/10/results. However, this is not a DeepTrio model and does not fit the use case that @Npaffen described unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:33,deployability,continu,continues,33,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,energy efficiency,model,model,150,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:249,energy efficiency,model,model,249,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:150,security,model,model,150,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:249,security,model,model,249,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:231,testability,simpl,simply,231,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:43,usability,help,help,43,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:101,usability,help,helpful,101,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:231,usability,simpl,simply,231,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:395,availability,operat,operate,395,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:82,energy efficiency,model,model,82,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:166,energy efficiency,model,model,166,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:356,energy efficiency,model,model,356,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:159,modifiability,PAC,PACBIO,159,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:213,modifiability,Pac,PacBio,213,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:82,security,model,model,82,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:166,security,model,model,166,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:356,security,model,model,356,"Hi @Npaffen . Our typical recommendation for this would be to use DeepVariant WGS model to call variants with gVCF output in the Illumina samples, DeepVariant PACBIO model to call variants with gVCF output in the PacBio sample, and then use glnexus to jointly genotype the three samples together. The variant call probabilities are well calibrated in each model individually, and so glnexus can operate on them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:279,deployability,log,logits,279,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1766,deployability,updat,update,1766,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2115,energy efficiency,measur,measure,2115,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:195,interoperability,architectur,architecture,195,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:217,modifiability,Pac,PacBio,217,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:833,modifiability,Pac,PacBio,833,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:187,performance,network,network,187,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:824,performance,network,network,824,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:901,performance,network,network-resiliency,901,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1528,performance,network,network,1528,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2021,performance,perform,performed,2021,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2126,performance,network,network,2126,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2311,performance,network,network,2311,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:135,reliability,resilien,resiliency,135,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:909,reliability,resilien,resiliency,909,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2134,reliability,resilien,resiliency,2134,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:135,safety,resilien,resiliency,135,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:279,safety,log,logits,279,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:357,safety,test,test,357,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:451,safety,test,test,451,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:545,safety,test,test,545,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:889,safety,test,testing,889,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:909,safety,resilien,resiliency,909,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1241,safety,test,tested,1241,"natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. Thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1438,safety,test,test,1438,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1608,safety,test,test,1608,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1698,safety,test,testing,1698,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1766,safety,updat,update,1766,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2134,safety,resilien,resiliency,2134,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:187,security,network,network,187,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:279,security,log,logits,279,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:824,security,network,network,824,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:901,security,network,network-resiliency,901,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1528,security,network,network,1528,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1766,security,updat,update,1766,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2126,security,network,network,2126,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2311,security,network,network,2311,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:279,testability,log,logits,279,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:357,testability,test,test,357,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:451,testability,test,test,451,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:545,testability,test,test,545,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:889,testability,test,testing,889,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1241,testability,test,tested,1241,"natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. Thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1438,testability,test,test,1438,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1608,testability,test,test,1608,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1698,testability,test,testing,1698,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:610,usability,visual,visual,610,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:640,usability,confirm,confirmed,640,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:729,usability,indicat,indicates,729,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:866,usability,confirm,confirm,866,". Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1047,usability,interact,interactions,1047,"fication. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1367,usability,interact,interaction,1367,"937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1539,usability,interact,interactions,1539,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1711,usability,interact,interacting,1711,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:1958,usability,minim,minimum,1958,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:2021,usability,perform,performed,2021,"s://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:221,security,team,team,221,"Thanks @pgrosu for the analysis! If i understand correctly, the original question in the thread has been resolved. Thanks all for the following discussions in this thread. I'll close this issue now so it's easier for our team to track.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:38,testability,understand,understand,38,"Thanks @pgrosu for the analysis! If i understand correctly, the original question in the thread has been resolved. Thanks all for the following discussions in this thread. I'll close this issue now so it's easier for our team to track.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:177,usability,close,close,177,"Thanks @pgrosu for the analysis! If i understand correctly, the original question in the thread has been resolved. Thanks all for the following discussions in this thread. I'll close this issue now so it's easier for our team to track.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:336,availability,state,state,336,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:252,energy efficiency,model,model,252,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:319,energy efficiency,model,model,319,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:336,integrability,state,state,336,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:362,integrability,sub,substructures,362,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:252,security,model,model,252,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:319,security,model,model,319,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:130,usability,person,person,130,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:140,usability,confirm,confirm,140,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:260,usability,interact,interactions,260,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:346,usability,interact,interactions,346,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:490,usability,confirm,confirm,490,"Thank you for the kind words @pichuan. I think the original question is probably well-answered, but @Npaffen is probably a better person to confirm with. As a nice side note, this discussion has provided me more insight into the $`data \leftrightarrow model`$ interactions, revealing some interesting properties of the model's internal state and interactions of substructures $`-`$ which might have the potential for a paper $`-`$ but I would need to do some more preliminary work first to confirm some ideas. Thank you,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:95,testability,understand,understand,95,Yeah thanks too all here in this thread for the productive discussion. This helped me a lot to understand how deepvariant works! Good luck with the project!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:76,usability,help,helped,76,Yeah thanks too all here in this thread for the productive discussion. This helped me a lot to understand how deepvariant works! Good luck with the project!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:145,interoperability,specif,specific,145,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:130,modifiability,layer,layers,130,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:137,performance,perform,perform,137,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:30,usability,help,helpful,30,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/666:137,usability,perform,perform,137,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well! Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/666
https://github.com/google/deepvariant/issues/667:31,usability,help,helpful,31,@wlhCNU Can you see if this is helpful for you? https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/667
https://github.com/google/deepvariant/issues/668:80,availability,operat,operates,80,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:195,performance,time,timeout,195,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:195,safety,timeout,timeout,195,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/668:219,usability,help,helps,219,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/668
https://github.com/google/deepvariant/issues/669:97,deployability,instal,install,97,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:41,security,team,team,41,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:79,security,control,control,79,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:168,security,assess,assess,168,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:79,testability,control,control,79,"Hi @gambalab . We'll discuss this in the team today. We don't make or directly control the conda install for DeepVariant, that is done by external people. We'll try to assess our ability to debug or offer suggestions for conda issues like this today, but it might be a bit before we have a suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:11,deployability,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:72,deployability,instal,install,72,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:182,deployability,fail,fails,182,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:228,deployability,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:338,deployability,version,versions,338,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:11,integrability,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:228,integrability,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:338,integrability,version,versions,338,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:22,interoperability,conflict,conflict,22,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:11,modifiability,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:42,modifiability,pac,package,42,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:228,modifiability,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:338,modifiability,version,versions,338,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:182,reliability,fail,fails,182,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:11,safety,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:228,safety,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:11,testability,depend,dependency,11,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:228,testability,depend,dependencies,228,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:111,usability,command,command,111,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use. .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:87,availability,operat,operating,87,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:108,deployability,version,version,108,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:108,integrability,version,version,108,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:108,modifiability,version,version,108,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:177,security,access,access,177,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:10,availability,cluster,cluster,10,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:10,deployability,cluster,cluster,10,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:43,deployability,instal,install,43,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:197,deployability,version,version,197,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:243,deployability,releas,release,243,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:344,deployability,VERSION,VERSION,344,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:197,integrability,version,version,197,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:344,integrability,VERSION,VERSION,344,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:197,modifiability,version,version,197,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:344,modifiability,VERSION,VERSION,344,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:123,safety,permiss,permission,123,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:483,usability,support,support,483,"I am on a cluster. I will ask the admin to install docker engine. Conda was the first solution because you don t need sudo permission, and everything is under your home directory. Regarding the OS version, below you can find it. > cat /etc/os-release. >PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"". >NAME=""Debian GNU/Linux"". >VERSION_ID=""11"". >VERSION=""11 (bullseye)"". >VERSION_CODENAME=bullseye. >ID=debian. >HOME_URL=""https://www.debian.org/"". >SUPPORT_URL=""https://www.debian.org/support"". >BUG_REPORT_URL=""https://bugs.debian.org/""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:545,availability,down,download,545,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:734,availability,down,downloaded,734,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:90,deployability,instal,installed,90,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:254,deployability,instal,install,254,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:415,deployability,instal,install,415,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:536,deployability,releas,releases,536,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:158,interoperability,conflict,conflicts,158,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:229,performance,perform,performs,229,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:229,usability,perform,performs,229,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:285,usability,user,user-space,285,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:810,usability,help,helps,810,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```. wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz. tar xzvf udocker-1.3.9.tar.gz. cd udocker-1.3.9/udocker/. ./udocker pull google/deepvariant:1.5.0. ./udocker run google/deepvariant:1.5.0. ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:98,deployability,instal,installed,98,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:268,deployability,instal,install,268,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:458,deployability,instal,installation,458,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:166,interoperability,conflict,conflicts,166,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:243,performance,perform,performs,243,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:243,usability,perform,performs,243,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:299,usability,user,user-space,299,"> Hi @gambalab,. > . > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. > . > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. > . thank you! this is a great solution for me. you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:250,deployability,releas,release,250,"Thanks @pgrosu for the suggestion, and @gambalab for confirming it works. I don't think our team has tried `udocker` before. So it's good to know about this. I'll add an internal task to try this out and consider adding a pointer to it in the future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:92,security,team,team,92,"Thanks @pgrosu for the suggestion, and @gambalab for confirming it works. I don't think our team has tried `udocker` before. So it's good to know about this. I'll add an internal task to try this out and consider adding a pointer to it in the future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/669:53,usability,confirm,confirming,53,"Thanks @pgrosu for the suggestion, and @gambalab for confirming it works. I don't think our team has tried `udocker` before. So it's good to know about this. I'll add an internal task to try this out and consider adding a pointer to it in the future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/669
https://github.com/google/deepvariant/issues/670:52,usability,command,command,52,@zxy1555847 What happens when you run the following command on the CRAM files that show the suffix (just replace `CRAM_FILE` with your file names):. ```. samtools samples -h CRAM_FILE. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:524,availability,error,error,524,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:524,performance,error,error,524,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:216,reliability,pra,practices,216,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:524,safety,error,error,524,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:524,usability,error,error,524,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:865,usability,user,user,865,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
https://github.com/google/deepvariant/issues/670:947,usability,command,command,947,"Hi @zxy1555847 . It looks like this output is the result of running GLnexus on gVCFs. I believe you must be running DeepVariant on the 40 single samples and then GLnexus on the gVCF output as recommended in our best practices. The sample name that occurs in the gVCF file comes from the sample name in the BAM file tag. (as @pgrosu suggests, that should be what follows the `SM:` tag from the output of `samtools view -H ${CRAM} | grep SM`. DeepVariant should take that value as its sample name. DeepVariant should raise an error if a single BAM file has multiple sample names, suggesting that if these files were able to run through and produce output, they should be using the reads appropriately. . In either case, you can tell DeepVariant to over-ride any sample names present in a file and produce output from all reads with a sample name provided by you as a user. To do so, you can add the flag `--sample_name` with your sample name to the command.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/670
